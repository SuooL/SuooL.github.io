title: '机器学习中的数学基础--线性代数'
date: 2018-06-18 00:39:23
tags: [线性代数,机器学习]
category: [机器学习]
---

>换了个新的博客主题,开启新的学习主题.

# 线性代数

## 标量、向量、矩阵和张量

- 标量(scalar): 一个标量就是一个单独的数，它不同于线性代数中研
究的其他大部分对象(通常是多个数的数组)。
<!--more-->

- 向量(vector): 一个向量是一列数。这些数是有序排列的。通过次序中的索引，我们可以确定每个单独的数。
- 矩阵(matrix): 矩阵是一个二维数组，其中的每一个元素被两个索引(而非一个)所确定。
- 张量(tensor): 在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。

**转置(transpose)**是矩阵的重要操作之一。矩阵的转置是以对角线为轴的镜像， 这条从左上角到右下角的对角线被称为 主对角线(main diagonal)。我们将矩阵 A 的转置表示为 $A^T$，定义如下

$$(A^T)_{i,j} = A_{j,i} \tag{1}$$

向量可以看作只有一列的矩阵。对应地，向量的转置可以看作是只有一行的矩阵。标量可以看作是只有一个元素的矩阵。

只要矩阵的形状一样，我们可以把两个矩阵相加。两个矩阵相加是指对应位置的元素相加。

标量和矩阵相乘，或是和矩阵相加时，我们只需将其与矩阵的每个元素相乘或相加。

## 矩阵和向量相乘
两个矩阵 $\bf A$ 和 $ \bf B$ 的矩阵乘积 (matrix product) 是第三个矩阵 $\bf C$。为了使乘法定义良好，矩阵 $\bf A$ 的列数必须和矩阵 $\bf B$ 的行数相等。如果矩阵 $\bf A$的形状是 $m \times n$，矩阵 B 的形状是 $ n \times p$，那么矩阵$\bf C$的形状是 $ m \times p $ ，具体地，该乘法操作定义为 

$$
C _{i,j} = \sum_{k} {A_{i,k}}B_{k,j}
\tag{2}
$$

两个相同维数的向量 x 和 y 的 点积 (dot product) 可看作是矩阵乘积 $x^⊤y$。我 们可以把矩阵乘积 $C=AB$ 中计算 $C_{i,j}$ 的步骤看作是 $A$ 的第 $i$ 行和 $B$ 的第 $j$ 列之间的点积。

两个向量点积的结果是标量

## 单位矩阵和逆矩阵
 单位矩阵 (identity matrix)
 矩阵逆 (matrix inversion)
  
 任意向量和单位矩阵相乘，都不会改变。
 
单位矩阵的结构很简单:所有沿主对角线的元素都是 1，而所有其他位置的元素都是 0。如下所示。
 
$$
\begin{bmatrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\
0 & 0 & 1 \\  
\end{bmatrix} 
\tag{3}
$$

矩阵 $A$ 的矩阵逆(matrix inversion)记作 $A^{−1}$，其定义的矩阵满足如下条件
$$A^{−1}A = I_n.\tag{4}$$

一组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即:
$$
\sum_{i} {c_iv^i}.\tag{5}
$$

一组向量的**生成子空间(span)**是原始向量线性组合后所能抵达的点的集合

如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为**线性无关 (linearly independent**)。

一个列向量线性相关的方阵被称为**奇异的(singular)**

## 范数
在机器学习中，我们经常使用被称为**范数 (norm)**的函数衡量向量大小。形式上，$L^p$ 范数定义如下

$$
||x||_p=
\left(
\sum_{i}{|x_i|^p}
\right)^{\frac{1}{p}}
\tag{6}
$$

其中 $p \in R , p \geq 1 $.

范数(包括 $L^p$ 范数)是将向量映射到非负值的函数。直观上来说，向量 $x$ 的范数衡量从原点到点 $x$ 的距离。

当 $p = 2$ 时，$L^2$ 范数被称为**欧几里得范数(Euclidean norm)**。它表示从原点出发到向量 $x$ 确定的点的欧几里得距离。$L^2$ 范数在机器学习中出现地十分频繁，经常简化表示为 $∥x∥$，略去了下标 2。平方 $L^2$ 范数也经常用来衡量向量的大小，可以简单地通过点积 $x^⊤x$ 计算。

平方 $L^2$ 范数在数学和计算上都比 $L^2$ 范数本身更方便。例如，平方 $L^2$ 范数对 $x$ 中每个元素的导数只取决于对应的元素，而 $L^2$ 范数对每个元素的导数却和整个向量相关。但是在很多情况下，平方 $L^2$ 范数也可能不受欢迎，因为它在原点附近增长 得十分缓慢。在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素 是很重要的。在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数:$L^1$ 范数. $L^1$ 范数可以简化如下:

$$
||x||_1 = \sum_i {|x_i|}
\tag{7}
$$

当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 $L^1$ 范数。每当
$x$ 中某个元素从 0 增加 $\epsilon$，对应的 L1 范数也会增加 $\epsilon$ 。

另外一个经常在机器学习中出现的范数是 $L^\infty$ 范数，也被称为**最大范数(max norm)**。这个范数表示向量中具有最大幅值的元素的绝对值:

$$
||x||_\infty = \underset{i}{max}|x_i|
\tag{8}
$$

有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用 **Frobenius 范数(Frobenius norm)**，

$$
||A||_F = \sqrt {\sum_{i,j}A^2_{i,j}}
\tag{9}
$$

其类似于向量的 $L^2$ 范数。
两个向量的**点积(dot product)**可以用范数来表示。具体地，

$$
x^Ty=||x||_2||y||_2\cos\theta  \tag{10}
$$

其中 $θ$ 表示 $x$ 和 $y$ 之间的夹角。

如果 $x^⊤y = 0$，那么向量 $x$ 和向量 $y$ 互相**正交(orthogonal)**。如果两个向量都有非零范数，那么这两个向量之间的夹角是 90 度。在 $R^n$ 中，至多有 $n$ 个范数非零向量互相正交。如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们 是**标准正交(orthonormal)**。

**正交矩阵(orthogonal matrix)**是指行向量和列向量是分别标准正交的方阵,$A^{−1} = A^⊤$

## 特征分解
**特征分解(eigendecomposition)**是使用最广的矩阵分解之一，即我们将矩阵分解成一组**特征向量**和**特征值**。
方阵 A 的**特征向量(eigenvector)**是指与 A 相乘后相当于对该向量进行缩放的非零向量 v:
$$
Av = \lambda v
\tag{11}
$$
标量 $λ$ 被称为这个特征向量对应的**特征值(eigenvalue)**

A 的**特征分解(eigendecomposition)**可以记作

$$
A=Vdiag(\lambda)V^{-1}
\tag{12}
$$

所有特征值都是正数的矩阵被称为**正定(positive definite)**;所有特征值都是非负数的矩阵被称为**半正定(positive semidefinite)**。同样地，所有特征值都是负数的 矩阵被称为**负定(negative definite)**;所有特征值都是非正数的矩阵被称为**半负定(negative semidefinite)**。半正定矩阵受到关注是因为它们保证 $∀x, x^⊤Ax ≥ 0$。此外，正定矩阵还保证 $x^⊤Ax = 0 ⇒ x = 0$。

## 奇异值分解
**奇异值分解(singular value decomposition, SVD)**，将矩阵分解为**奇异向量(singular vector)**和**奇异值(singular value)**。通过奇异值分解，我们会得到一些与特征分解相同类型的信息。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。

分解表达式:
$$
A = UDV^⊤. \tag{13}
$$

假设 $A$ 是一个 $m×n$ 的矩阵，那么 $U$ 是一个 $m×m$ 的矩阵，$D$ 是一个 $m×n$
的矩阵，$V$ 是一个 $n × n$ 矩阵。 这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵 $U$ 和 $V$ 都定义为正交矩阵，而矩阵 $D$ 定义为对角矩阵。注意，矩阵 $D$ 不一定是方阵。
对角矩阵 $D$ 对角线上的元素被称为矩阵 $A$ 的**奇异值(singular value)**。矩阵 $U$ 的列向量被称为**左奇异向量(left singular vector)**，矩阵 $V$ 的列向量被称**右奇异向量(right singular vector)**。
事实上，我们可以用与 $A$ 相关的特征分解去解释 $A$ 的奇异值分解。$A$的**左奇异向量(left singular vector)**是 $AA^⊤$ 的特征向量。$A$ 的**右奇异向量(right singular vector)**是 $A^⊤A$ 的特征向量。A 的非零奇异值是 $A^⊤A$ 特征值的平方根，同时也是 $AA^⊤$ 特征值的平方根。

## 迹运算
迹运算返回的是矩阵对角元素的和:
$$
Tr(A)=\sum_i {A_{i,i}}
$$


