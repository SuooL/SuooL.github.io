<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SuooL&#39;s Blog</title>
  
  <subtitle>蛰伏于盛夏 藏华于当春</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://suool.net/"/>
  <updated>2018-11-17T03:26:34.297Z</updated>
  <id>http://suool.net/</id>
  
  <author>
    <name>SuooL</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MCMC采样及M-H采样</title>
    <link href="http://suool.net/2018/11/17/Markov-Chain-Monte-Carlo-3/"/>
    <id>http://suool.net/2018/11/17/Markov-Chain-Monte-Carlo-3/</id>
    <published>2018-11-17T02:12:15.000Z</published>
    <updated>2018-11-17T03:26:34.297Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="mcmc采样及m-h采样">MCMC采样及M-H采样</h1><h2 id="前言">前言</h2><p><a href="https://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-2/">前面的文章</a>已经说到给定一个概率平稳分布π, 只要能够其马尔科夫链状态转移矩阵P，我们就可以找到一种通用的概率分布采样方法，进而用于蒙特卡罗模拟。现在就学习下解决这个问题的办法：MCMC采样和它的易用版M-H采样。</p><p>如何能做到这一点呢？我们主要使用如下的定理。 <a id="more"></a></p><h3 id="马尔科夫链的细致平稳条件">马尔科夫链的细致平稳条件</h3><p>细致平稳条件: 如果非周期马尔科夫链的状态转移矩阵<span class="math inline">\(P\)</span>和概率分布<span class="math inline">\(π(x)\)</span>对于所有的<span class="math inline">\(i,j\)</span>满足： <span class="math display">\[\pi(i)P(i,j) = \pi(j)P(j,i)\]</span></p><p>则称概率分布<span class="math inline">\(π(x)\)</span>是状态转移矩阵<span class="math inline">\(P\)</span>的平稳分布。上式被称为细致平稳条件(<code>detailed balance condition</code>)。</p><p>其实这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态<span class="math inline">\(i,j\)</span>, 从<span class="math inline">\(i\)</span>转移出去到<span class="math inline">\(j\)</span>而丢失的概率质量，恰好会被从<span class="math inline">\(j\)</span>转移回<span class="math inline">\(i\)</span>的概率质量补充回来，所以状态上的概率质量<span class="math inline">\(π(i\)</span>)是稳定的，从而<span class="math inline">\(π(x)\)</span>是马尔科夫链的平稳分布。</p><p>证明很简单,由细致平稳条件可知: <span class="math display">\[\sum\limits_{i=1}^{\infty}\pi(i)P(i,j)  = \sum\limits_{i=1}^{\infty} \pi(j)P(j,i) =  \pi(j)\sum\limits_{i=1}^{\infty} P(j,i) =  \pi(j)\]</span></p><p>将上式用矩阵表示即为： <span class="math display">\[\pi P = \pi\]</span></p><p>即满足马尔可夫链的收敛性质。也就是说，只要我们找到了可以使概率分布<span class="math inline">\(π(x)\)</span>满足细致平稳分布的矩阵<span class="math inline">\(P\)</span>即可。这给了我们寻找从平稳分布<span class="math inline">\(π\)</span>, 找到对应的马尔科夫链状态转移矩阵<span class="math inline">\(P\)</span>的新思路。</p><p>不过不幸的是，仅仅从细致平稳条件还是很难找到合适的矩阵P。比如我们的目标平稳分布是<span class="math inline">\(π(x)\)</span>,随机找一个马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>,它是很难满足细致平稳条件的，即: <span class="math display">\[\pi(i)Q(i,j) \neq \pi(j)Q(j,i)\]</span></p><p>那么如何使这个等式满足呢？</p><h2 id="mcmc采样">MCMC采样</h2><p>由于一般情况下，目标平稳分布<span class="math inline">\(π(x)\)</span>和某一个马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>不满足细致平稳条件，即: <span class="math display">\[\pi(i)Q(i,j) \neq \pi(j)Q(j,i)\]</span></p><p>我们可以对上式做一个改造，使细致平稳条件成立。方法是引入一个<span class="math inline">\(α(i,j)\)</span>,使上式可以取等号，即：</p><p><span class="math display">\[\pi(i)Q(i,j)\alpha(i,j) = \pi(j)Q(j,i)\alpha(j,i)\]</span></p><p>此时的问题转化为什么样的<span class="math inline">\(α(i,j)\)</span>可以使等式成立, 其实很简单，只要满足下两式即可： <span class="math display">\[\alpha(i,j) = \pi(j)Q(j,i) \\\alpha(j,i) = \pi(i)Q(i,j)\]</span></p><p>这样，我们就得到了我们的分布<span class="math inline">\(π(x)\)</span>对应的马尔科夫链状态转移矩阵<span class="math inline">\(P\)</span>，满足： <span class="math display">\[P(i,j) = Q(i,j)\alpha(i,j)\]</span></p><p>也就是说，我们的目标矩阵<span class="math inline">\(P\)</span>可以通过任意一个马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>乘以<span class="math inline">\(α(i,j)\)</span>得到。<span class="math inline">\(α(i,j)\)</span>我们有一般称之为接受率。取值在[0,1]之间，可以理解为一个概率值。即目标矩阵<span class="math inline">\(P\)</span>可以通过任意一个马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>以一定的接受率获得。这个很像我们在<a href="https://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-1/">蒙特卡罗方法</a>讲到的接受-拒绝采样，那里是以一个常用分布通过一定的接受-拒绝概率得到一个非常见分布，这里是以一个常见的马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>通过一定的接受-拒绝概率得到目标转移矩阵<span class="math inline">\(P\)</span>,两者的解决问题思路是类似的。</p><p><strong>MCMC的采样过程</strong>如下</p><ol type="1"><li>输入我们任意选定的马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>，平稳分布<span class="math inline">\(π(x)\)</span>，设定状态转移次数阈值<span class="math inline">\(n_1\)</span>，需要的样本个数<span class="math inline">\(n_2\)</span></li><li>从任意简单概率分布采样得到初始状态值<span class="math inline">\(x_0\)</span></li><li><span class="math inline">\(for\; t=0 \; to \; n_1+n_2−1:\)</span><ol type="1"><li>从条件概率分布<span class="math inline">\(Q(x|x_t)\)</span>中采样得到样本<span class="math inline">\(x_∗\)</span></li><li>从均匀分布采样<span class="math inline">\(u∼uniform[0,1]\)</span></li><li>如果<span class="math inline">\(u &lt; \alpha(x_t,x_{*}) = \pi(x_{*})Q(x_{*},x_t)\)</span>, 则接受转移<span class="math inline">\(x_t \to x_{*}\)</span>，即<span class="math inline">\(x_{t+1}= x_{*}\)</span></li><li>否则不接受转移，<span class="math inline">\(t=max(t−1,0)\)</span></li></ol></li><li>样本集<span class="math inline">\((x_{n_1}, x_{n_1+1},..., x_{n_1+n_2-1})\)</span>即为我们需要的平稳分布对应的样本集。</li></ol><p>以上的MCMC采样算法已经能很漂亮的工作了，不过它有一个小的问题：马尔科夫链<span class="math inline">\(Q\)</span>在转移的过程中的接受率<span class="math inline">\(α(i,j)\)</span>可能偏小，这样采样过程中容易原地踏步，拒绝大量的跳转，使得马尔科夫链收敛到平稳分布<span class="math inline">\(p(x)\)</span>的速度太慢。有可能我们采样了上百万次马尔可夫链还没有收敛，也就是上面这个<span class="math inline">\(n_1\)</span>要非常非常的大，这让人难以接受，怎么办呢？</p><p>看下节.</p><h2 id="m-h采样">M-H采样</h2><p>M-H采样是Metropolis-Hastings采样的简称，这个算法首先由Metropolis提出，被Hastings改进，因此被称之为Metropolis-Hastings采样或M-H采样.</p><p>M-H采样解决了我们上一节MCMC采样接受率过低的问题。</p><p>假设<span class="math inline">\(α(i,j)=0.1,α(j,i)=0.2\)</span>此时满足细致平稳条件，于是: <span class="math display">\[p(i)q(i,j) \cdot 0.1=p(j)q(j,i) \cdot 0.2\]</span></p><p>上式两边同时扩大5倍，等式变为: <span class="math display">\[p(i)q(i,j) \cdot 0.5=p(j)q(j,i) \cdot 1\]</span></p><p>我们提高了接受率，而细致平稳条件并没有打破。这启发我们可以把细致平稳条件中<span class="math inline">\(α(i,j),α(j,i)\)</span>等比例放大，使得两数中较大的一个放大到1，如此提高了采样中的跳转接受率。 这样我们的接受率可以做如下改进，即： <span class="math display">\[\alpha(i,j) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}\]</span></p><p>通过这个微小的改造，我们就得到了可以在实际应用中使用的M-H采样算法过程如下：</p><ol type="1"><li>输入我们任意选定的马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>，平稳分布<span class="math inline">\(π(x)\)</span>，设定状态转移次数阈值<span class="math inline">\(n_1\)</span>，需要的样本个数<span class="math inline">\(n_2\)</span></li><li>从任意简单概率分布采样得到初始状态值<span class="math inline">\(x_0\)</span></li><li><span class="math inline">\(for\; t=0 \;to\; n_1+n_2−1:\)</span><ol type="1"><li>从条件概率分布<span class="math inline">\(Q(x|x_t)\)</span>中采样得到样本<span class="math inline">\(x_∗\)</span></li><li>从均匀分布采样<span class="math inline">\(u∼uniform[0,1]\)</span></li><li>如果<span class="math inline">\(u &lt; \alpha(x_t,x_{*}) = min\{ \frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)},1\}\)</span>, 则接受转移<span class="math inline">\(x_t \to x_{*}\)</span>，即<span class="math inline">\(x_{t+1}=x_∗\)</span></li><li>否则不接受转移，<span class="math inline">\(t=max(t-1,0)\)</span></li></ol></li><li>样本集<span class="math inline">\((x_{n_1}, x_{n_1+1},..., x_{n_1+n_2-1})\)</span>即为我们需要的平稳分布对应的样本集。</li></ol><p>很多时候，我们选择的马尔科夫链状态转移矩阵<span class="math inline">\(Q\)</span>如果是对称的，即满足<span class="math inline">\(Q(i,j)=Q(j,i)\)</span>,这时我们的接受率可以进一步简化为： <span class="math display">\[\alpha(i,j) = min\{ \frac{\pi(j)}{\pi(i)},1\}\]</span></p><h2 id="note注意点">Note注意点</h2><ul><li>具体的关于上述两个采样的方法的转移矩阵<span class="math inline">\(Q\)</span>的选择，如果是离散的分布，那么只要这个矩阵满足每行概率和为1即可，没有特殊要求。如果是连续分布，则一般会选择正态分布作为<span class="math inline">\(Q\)</span>，因为正态分布的联合分布，条件分布，边缘分布都是正态分布，所以后面计算条件概率比较的容易。对采样的好坏一般影响都不大，只能说不同的<span class="math inline">\(Q\)</span>对算法的收敛速度会快有慢，只要收敛了，最后的采样结果都是可以的。</li><li>这里<span class="math inline">\(α(i,j)\)</span>是某一个位置的<span class="math inline">\((i,j)\)</span>的接受率，所以α实际上也是一个矩阵，如果按矩阵的写法，就是:<span class="math inline">\(P=Qα^T\)</span>, 不要理解为所有的位置都乘以的是同一个α(i,j),这是不对的。</li></ul><h3 id="m-h采样实例">M-H采样实例</h3><p>我们的目标平稳分布是一个均值3，标准差2的正态分布，而选择的马尔可夫链状态转移矩阵<span class="math inline">\(Q(i,j)\)</span>的条件转移概率是以<span class="math inline">\(i\)</span>为均值,方差<span class="math inline">\(1\)</span>的正态分布在位置<span class="math inline">\(j\)</span>的值。这个例子仅仅用来加深对M-H采样过程的理解。毕竟一个普通的一维正态分布用不着去用M-H采样来获得样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm_dist_prob</span><span class="params">(theta)</span>:</span></span><br><span class="line">    y = norm.pdf(theta, loc=<span class="number">3</span>, scale=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">T = <span class="number">100000</span></span><br><span class="line">pi = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(T)]</span><br><span class="line">sigma = <span class="number">1</span></span><br><span class="line">t = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> t &lt; T<span class="number">-1</span>:</span><br><span class="line">    t = t + <span class="number">1</span></span><br><span class="line">    pi_star = norm.rvs(loc=pi[t - <span class="number">1</span>], scale=sigma, size=<span class="number">1</span>, random_state=<span class="keyword">None</span>)</span><br><span class="line">    alpha = min(<span class="number">1</span>, (norm_dist_prob(pi_star[<span class="number">0</span>]) / norm_dist_prob(pi[t - <span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    u = random.uniform(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> u &lt; alpha:</span><br><span class="line">        pi[t] = pi_star[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pi[t] = pi[t - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(pi, norm.pdf(pi, loc=<span class="number">3</span>, scale=<span class="number">2</span>))</span><br><span class="line">num_bins = <span class="number">1000</span></span><br><span class="line">plt.hist(pi, num_bins, density=<span class="number">1</span>, facecolor=<span class="string">'red'</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下： <img src="media/977C93F7-DC2F-4E60-BC0F-820B055DF4C4.png" alt="采样结果"></p><h2 id="m-h采样总结">M-H采样总结</h2><p>M-H采样完整解决了使用蒙特卡罗方法需要的任意概率分布样本集的问题，因此在实际生产环境得到了广泛的应用。</p><p>但是在大数据时代，M-H采样面临着两大难题：</p><ol type="1"><li><p>我们的数据特征非常的多，M-H采样由于接受率计算式<span class="math inline">\(\frac{\pi(j)Q(j,i)}{\pi(i)Q(i,j)}\)</span>的存在，在高维时需要的计算时间非常的可观，算法效率很低。同时<span class="math inline">\(α(i,j)\)</span>一般小于1，有时候辛苦计算出来却被拒绝了。能不能做到不拒绝转移呢？</p></li><li><p>由于特征维度大，很多时候我们甚至很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布。这时候我们能不能只有各维度之间条件概率分布的情况下方便的采样呢？</p></li></ol><p>Gibbs采样解决了上面两个问题，因此在大数据时代，MCMC采样基本是Gibbs采样的天下，下一篇我们就来学习Gibbs采样。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;mcmc采样及m-h采样&quot;&gt;MCMC采样及M-H采样&lt;/h1&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-2/&quot;&gt;前面的文章&lt;/a&gt;已经说到给定一个概率平稳分布π, 只要能够其马尔科夫链状态转移矩阵P，我们就可以找到一种通用的概率分布采样方法，进而用于蒙特卡罗模拟。现在就学习下解决这个问题的办法：MCMC采样和它的易用版M-H采样。&lt;/p&gt;
&lt;p&gt;如何能做到这一点呢？我们主要使用如下的定理。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>隐马尔科夫模型（HMM）与维特比（Veterbi）算法及其在分词上的应用</title>
    <link href="http://suool.net/2018/11/16/HMM-Viterbi/"/>
    <id>http://suool.net/2018/11/16/HMM-Viterbi/</id>
    <published>2018-11-16T14:32:35.000Z</published>
    <updated>2018-11-16T07:11:12.847Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css">]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;

      
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="NLP" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/NLP/"/>
    
    
      <category term="NLP" scheme="http://suool.net/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NLP 之分词技术概述</title>
    <link href="http://suool.net/2018/11/16/Text-Segmentation-1/"/>
    <id>http://suool.net/2018/11/16/Text-Segmentation-1/</id>
    <published>2018-11-16T12:01:00.000Z</published>
    <updated>2018-11-16T07:11:05.376Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="前言">前言</h2><p>NLP 的问题领域中，首先的要解决核心问题就是分词。在英文以空格来作为天然的词语间隔的语言中，分词是非常容易的；但是在中文领域，词以汉字为单位组成，词语与词语之间并无天然的界限，句子或短语之间以标点符号作为间隔。这就导致在中文领域做 NLP 的前提是做好中文分词技术。 <a id="more"></a></p><h3 id="基于规则的分词">基于规则的分词</h3><p>中文分词技术是中文 NLP 领域一个非常基础而核心的问题。在此问题被提出来之后，主要有三种思路出现，分别是：基于规则的分词、基于统计的分词、混合方式分词（规则+统计）。</p><p>基于规则的分词是一种机械的分词方法，主要通过维护词典，在切分语句的时候，通过将语句中的每个字符串与词表中的词语逐一匹配，找到即切分，否则不切分。按照匹配切分的方法，主要有：正向最大匹配法（MM），逆向最大匹配法（RMM）以及双向最大匹配法（BM）。</p><p>基于规则的方式其优点和缺点都显而易见，当今的环境下，这种方式已不再是主流，不再细说。</p><p>###基于统计的分词</p><p>基于统计的分词其主要思想是把每个词看作是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现的次数越多，就说明这个相连的字很可能就是一个词。因此可以利用字与字相邻出现的频率来反映其组成词的可靠度，统计语料库中相邻共现的各个字的组合的频度，当这个频度高于某一个临界值的时候，便可以认为这个字的组合是一个词语。</p><p>基于统计的分词，基本操作有两个： 1. 建立语言模型 2. 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式，这里面用到的统计算法主要有隐马尔科夫模型（HMM）、条件随机场（CRF）等。</p><h2 id="语言模型">语言模型</h2><p>语言模型主要有统计语言模型（n-gram语言模型）和神经网络语言模型等，这里主要详细说明统计语言模型。</p><h3 id="n-gram语言模型">n-gram语言模型</h3><p>统计语言模型简单来讲，就是计算一个句子的概率，更确切的说是计算组成这个句子一系列词语的概率。</p><p>比如给定一个已知词语序列的句子： <span class="math display">\[S = W_1,W_2,\cdots,W_n\]</span></p><p>一个好的语言模型应该会给这个句子比较高的概率。因为这个句子在语义语法上都没有任何问题。我们可以用如下的公式来计算概率： <span class="math display">\[\begin{align}P(S)&amp;=P(W_1,W_2,...,W_n) \\&amp;=P(W_1)P(W_2|W_1)\cdots P(W_n|W_1,W_2,...W_{n−1})\end{align}\]</span></p><p>可是这样的方法存在两个致命的缺陷：</p><ul><li><strong>參数空间过大</strong>：条件概率 <span class="math inline">\(P(W_n|W_1,W_2,...W_{n−1})\)</span> 的可能性太多，无法估算，不可能有用；</li><li><strong>数据稀疏严重</strong>：对于非常多词对的组合，在语料库中都没有出现，依据最大似然估计得到的概率将会是0。</li></ul><p>为了解决參数空间过大的问题。引入了<strong>马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关</strong>。</p><p>如果 n=1，即假设每个单词之间相互独立，那么就会得到 unigram 也就是一元语言模型： <strong>Unigram model</strong> <span class="math display">\[ P(W_1,W_2,W_3,...,W_n)=∏^n_{i=1}P(W_i)\]</span></p><p>如果 n=2 则为 bi-gram 模型表示，因为，下一个单词和之前一个单词是有很强的关联性 <strong>Bigram model</strong> <span class="math display">\[P(W_1,W_2,W_3,...,W_n)=∏^n_{i=2}P(W_i|W_{i−1})\]</span></p><p>假设一个词的出现仅依赖于它前面出现的两个词，即是 n=3，那么我们就称之为trigram： <strong>Trigram model</strong> <span class="math display">\[P(W_1,W_2,W_3,...,W_n)=∏^n_{i=3}P(W_i|W_{i−1},W_{i−2})\]</span></p><p>一般来说，N 元模型就是假设当前词的出现概率只与它前面的 N-1 个词有关。而这些概率参数都是可以通过大规模语料库来计算.</p><p>同时，n一般不会超过3, 否则参数空间过大，数据稀疏严重，时间复杂度高，精度却提高的不多。对于非常对多词对的组合，在语料库中没有出现，依据最大似然估计得到的概率将会是0。</p><p>当然，对于n-gram模型来说，有很多的平滑方法。即对频率为0的n元对进行估计，典型的平滑算法有加法平滑、Good-Turing平滑、Katz平滑、插值平滑，等等</p><h3 id="隐马尔科夫模型">隐马尔科夫模型</h3><p>隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;NLP 的问题领域中，首先的要解决核心问题就是分词。在英文以空格来作为天然的词语间隔的语言中，分词是非常容易的；但是在中文领域，词以汉字为单位组成，词语与词语之间并无天然的界限，句子或短语之间以标点符号作为间隔。这就导致在中文领域做 NLP 的前提是做好中文分词技术。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>马尔科夫链及其采样方法</title>
    <link href="http://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-2/"/>
    <id>http://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-2/</id>
    <published>2018-11-16T02:36:19.000Z</published>
    <updated>2018-11-17T02:15:23.325Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="前言">前言</h2><p>了解了什么是蒙特卡罗方法之后，自然引出了马尔可夫链这个概念，其在 WIKI 的解释如下：</p><blockquote><p>马尔可夫链（英语：Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性质。 在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。</p></blockquote><a id="more"></a><h2 id="定义及性质">定义及性质</h2><p>依据上述 wiki 的定义可以知道，马尔可夫链能在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等，当然MCMC也需要它。</p><p>如果用精确的数学定义来描述，则假设我们的序列状态是<span class="math inline">\(...X_{t-2}, X_{t-1}, X_{t}, X_{t+1},...\)</span>那么我们的在时刻 <span class="math inline">\(X_{t+1}\)</span> 的状态的条件概率仅仅依赖于时刻 <span class="math inline">\(X_t\)</span>，即： <span class="math display">\[P(X_{t+1} |...X_{t-2}, X_{t-1}, X_{t} ) = P(X_{t+1} | X_{t})\]</span></p><p>既然某一时刻状态转移的概率只依赖于它的前一个状态，那么我们只要能求出系统中任意两个状态之间的转换概率，这个马尔科夫链的模型就定了。我们来看看下图这个马尔科夫链模型的具体的例子(来源于维基百科)。</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/11/16/15423528014563.jpg" alt="股市转移概率"><figcaption>股市转移概率</figcaption></figure><p>这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。如果我们定义矩阵P某一位置<span class="math inline">\(P(i,j)\)</span>的值为<span class="math inline">\(P(j|i)\)</span>,即从状态i转化到状态j的概率，并定义牛市为状态0， 熊市为状态1, 横盘为状态2. 这样我们得到了马尔科夫链模型的状态转移矩阵为： <span class="math display">\[P=\left( \begin{array}{ccc} 0.9&amp;0.075&amp;0.025 \\ 0.15&amp;0.80&amp; 0.05 \\ 0.25&amp;0.25&amp;0.5 \end{array} \right)\]</span></p><p>假设已知当前处在熊市、牛市、横盘的人的比例是概率分布向量<span class="math inline">\(π_0=[π_0(1),π_0(2),π_0(3)]\)</span>,那么第一天的分布比例将是<span class="math inline">\(π_1=π_0P\)</span>, 第二天的分布比例将是<span class="math inline">\(π_2=π_1P=π_0P^2\)</span> ，第n天分布比例将是<span class="math inline">\(π_n=π_{n−1}P=π_0P^n\)</span>。</p><p>假设我们当前已知股市的概率分布为：[0.8,0.1,0.1],即80%概率的牛市，10%概率的熊盘与10%的横盘。然后这个状态作为序列概率分布的初始状态<span class="math inline">\(t_0\)</span>，将其带入这个状态转移矩阵计算<span class="math inline">\(t_1,t_2,t_3...\)</span>的状态。代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">matrix = np.matrix([[<span class="number">0.9</span>,<span class="number">0.075</span>,<span class="number">0.025</span>],[<span class="number">0.15</span>,<span class="number">0.8</span>,<span class="number">0.05</span>],[<span class="number">0.25</span>,<span class="number">0.25</span>,<span class="number">0.5</span>]], dtype=float)</span><br><span class="line">vector1 = np.matrix([[<span class="number">0.8</span>,<span class="number">0.1</span>,<span class="number">0.1</span>]], dtype=float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    vector1 = vector1*matrix</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Current round:"</span> , i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> vector1</span><br></pre></td></tr></table></figure></p><p>通过观察输出结果,可以看出在迭代一定次数之后,数据结果出现了如下的稳定情况 <figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Current round: <span class="number">48</span></span><br><span class="line"><span class="string">[[0.62500011 0.3124999  0.06249999]]</span></span><br><span class="line">Current round: <span class="number">49</span></span><br><span class="line"><span class="string">[[0.62500008 0.31249992 0.06249999]]</span></span><br><span class="line">Current round: <span class="number">50</span></span><br><span class="line"><span class="string">[[0.62500006 0.31249994 0.06249999]]</span></span><br><span class="line">.............</span><br><span class="line">Current round: <span class="number">59</span></span><br><span class="line"><span class="string">[[0.625  0.3125 0.0625]]</span></span><br><span class="line">Current round: <span class="number">60</span></span><br><span class="line"><span class="string">[[0.625  0.3125 0.0625]]</span></span><br><span class="line">Current round: <span class="number">61</span></span><br><span class="line"><span class="string">[[0.625  0.3125 0.0625]]</span></span><br><span class="line">Current round: <span class="number">62</span></span><br><span class="line"><span class="string">[[0.625  0.3125 0.0625]]</span></span><br><span class="line">........</span><br></pre></td></tr></table></figure></p><p>可以发现，从第60轮开始，我们的状态概率分布就不变了，一直保持在[0.625 0.3125 0.0625]，即62.5%的牛市，31.25%的熊市与6.25%的横盘。</p><p>我们现在换一个初始概率分布试一试，现在我们用[0.7,0.1,0.2]作为初始概率分布, 利用上述的代码计算,得出的结果一样出现的最终状态的概率分布趋于同一个稳定的概率分布[0.625 0.3125 0.0625], 也就是说我们的<strong>马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关</strong>。这是一个非常好的性质, 也就是说，如果我们得到了这个稳定概率分布对应的马尔科夫链模型的状态转移矩阵，则我们可以用任意的概率分布样本开始，代入马尔科夫链模型的状态转移矩阵，这样经过一些序列的转换，最终就可以得到符合对应稳定概率分布的样本。</p><p>这个性质不光对我们上面的状态转移矩阵有效，对于绝大多数的其他的马尔科夫链模型的状态转移矩阵也有效。同时不光是离散状态，连续状态时也成立。</p><p>同时这个性质也意味着，对于一个确定的状态转移矩阵 P，它的 n 次幂<span class="math inline">\(P^n\)</span>在当 n 大于一定的值的时候也可以发现是确定的.</p><h3 id="性质的数学语言描述">性质的数学语言描述</h3><p>如果一个非周期的马尔科夫链有状态转移矩阵P, 并且它的任何两个状态是连通的，那么<span class="math inline">\(\lim_{n \to \infty}P_{ij}^n\)</span>与 i 无关，我们有： <span class="math display">\[\lim_{n \to \infty}P_{ij}^n = \pi(j) \tag{1}\]</span></p><p><span class="math display">\[\lim_{n \to \infty}P^n = \left( \begin{array}{ccc} \pi(1)&amp;\pi(2)&amp;\ldots&amp;\pi(j)&amp;\ldots \\ \pi(1)&amp;\pi(2)&amp;\ldots&amp;\pi(j)&amp;\ldots \\ \ldots&amp;\ldots&amp;\ldots&amp;\ldots&amp;\ldots \\ \pi(1)&amp;\pi(2)&amp;\ldots&amp;\pi(j)&amp;\ldots \\ \ldots&amp;\ldots&amp;\ldots&amp;\ldots&amp;\ldots \end{array} \right) \tag{2}\]</span></p><p><span class="math display">\[\pi(j) = \sum\limits_{i=0}^{\infty}\pi(i)P_{ij} \tag{3}\]</span></p><p>π 是方程<span class="math inline">\(πP=π\)</span>的唯一非负解，其中 <span class="math display">\[\pi = [\pi(1),\pi(2),...,\pi(j),...]\;\; , \sum\limits_{i=0}^{\infty}\pi(i) = 1 \tag{4}\]</span></p><p>上面的性质中需要解释的有：</p><ol type="1"><li>非周期的马尔科夫链：定理中的“非周期“这个概念不解释，因为我们遇到的绝大多数马氏链都是非周期的；这个主要是指马尔科夫链的状态转化不是循环的，如果是循环的则永远不会收敛。幸运的是我们遇到的马尔科夫链一般都是非周期性的。用数学方式表述则是：对于任意某一状态i，d为集合<span class="math inline">\(\{n \mid n \geq 1,P_{ii}^n&gt;0 \}\)</span>的最大公约数，如果 d=1 ，则该状态为非周期的</li><li>任何两个状态是连通的：这个指的是从任意一个状态可以通过有限步到达其他的任意一个状态，不会出现条件概率一直为0导致不可达的情况。</li><li>马尔科夫链的状态数可以是有限的，也可以是无限的。因此可以用于连续概率分布和离散概率分布。</li><li>π 通常称为马尔科夫链的平稳分布。</li><li>我们用<span class="math inline">\(X_i\)</span>表示在马氏链上跳转第 i 步所处的状态，如果<span class="math inline">\(\lim_{n \rightarrow \infty}P_{ij}^{n} = \pi(j)\)</span>存在,很容易证明定理第二个结论 <span class="math display">\[P(X_{n+1}=j) = \sum_{i=1}^{\infty}P(X_{n}=i)P(X_{n+1}=j\mid x_{n}=i)=\sum_{i=1}^{\infty}P(X_{n}=i) p_{ij}\]</span></li></ol><h2 id="基于马尔科夫链采样">基于马尔科夫链采样</h2><p>对于给定的概率分布<span class="math inline">\(p(x)\)</span> ，我们希望能有便捷的方式生成它对应的样本。由于马尔科夫链能收敛到平稳分布，于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为 P 的马尔科夫链，使得该马尔科夫链的平稳分布恰好是<span class="math inline">\(p(x)\)</span>，那么我们从任何一个初始状态<span class="math inline">\(x_0\)</span>出发沿着马尔科夫链转移，得到一个转移序列<span class="math inline">\(x_0,x_1,x_2,\ldots,x_n,x_{n+1},\ldots\)</span>, 如果马尔科夫在第n步已经收敛了，于是我们就得到了<span class="math inline">\(p(x)\)</span>的样本<span class="math inline">\(x_n,x_{n+1},\ldots\)</span></p><p>总结下基于马尔科夫链的采样过程：</p><ol type="1"><li>输入马尔科夫链状态转移矩阵P，设定状态转移次数阈值<span class="math inline">\(n_1\)</span>，需要的样本个数<span class="math inline">\(n_2\)</span></li><li>从任意简单概率分布采样得到初始状态值<span class="math inline">\(x_0\)</span></li><li><span class="math inline">\(for t=0 to n_1+n_2−1:\)</span> 从条件概率分布<span class="math inline">\(P(x|x_t)\)</span>中采样得到样本<span class="math inline">\(x_t+1\)</span></li></ol><p>样本集<span class="math inline">\((x_{n1},x_{n_1+1},\ldots,...,x_{n1+n2−1})\)</span>即为我们需要的平稳分布对应的样本集。</p><h2 id="小结">小结</h2><p>马尔科夫链的收敛性质主要由转移矩阵P决定， 所以基于马尔科夫链做采样的关键问题是如何构造转移矩阵P，使得平稳分布恰好是我们要的分布<span class="math inline">\(p(x)\)</span>。</p><p>如何能做到这一点呢？</p><p>幸运的是，MCMC采样通过迂回的方式解决了上面这个大问题，我们在下一篇来讨论MCMC的采样，以及它的使用改进版采样: M-H(Metropolis-Hastings)采样和Gibbs采样.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;了解了什么是蒙特卡罗方法之后，自然引出了马尔可夫链这个概念，其在 WIKI 的解释如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;马尔可夫链（英语：Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性质。 在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>蒙特卡罗方法概述</title>
    <link href="http://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-1/"/>
    <id>http://suool.net/2018/11/16/Markov-Chain-Monte-Carlo-1/</id>
    <published>2018-11-16T02:06:19.000Z</published>
    <updated>2018-11-16T07:02:59.422Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="前言">前言</h2><p>作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习,深度学习以及自然语言处理等领域都有广泛的应用，是很多复杂算法求解的基础。</p><p>从名字我们可以看出，MCMC由两个MC组成，即蒙特卡罗方法（Monte Carlo Simulation，简称MC）和马尔科夫链（Markov Chain ，也简称MC）。要弄懂MCMC的原理我们首先得搞清楚蒙特卡罗方法和马尔科夫链的原理。 <a id="more"></a></p><h2 id="蒙特卡罗方法">蒙特卡罗方法</h2><p>蒙特卡罗原来是一个赌场的名称，用它作为名字大概是因为蒙特卡罗方法是一种随机模拟的方法，这很像赌博场里面的扔骰子的过程。原理是通过大量随机样本，去了解一个系统，进而得到所要计算的值。</p><p>它非常强大和灵活，又相当简单易懂，很容易实现。对于许多问题来说，它往往是最简单的计算方法，有时甚至是唯一可行的方法。</p><p>具体的适合理解的例子参考<a href="http://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html" target="_blank" rel="noopener">这篇文章</a></p><p>给定统计样本集，如何估计产生这个样本集的随机变量概率密度函数,是我们比较熟悉的概率密度估计问题。 求解概率密度估计问题的常用方法是最大似然估计、最大后验估计等。但是，我们思考概率密度估计问题的逆问题:给定一个概率分布p(x)，如何让计算机生成满足这个概率分布的样本。 这个问题就是统计模拟中研究的重要问题–采样(Sampling)。</p><p>如果有一个我们很难求解出<span class="math inline">\(f(x)\)</span>的原函数的函数, 现要求其在定义域 [a, b] 上的积分, 如果这个函数是均匀分布, 那么我们可以采样 [a,b] 区间的 n 个值：<span class="math inline">\({x_0,x_1,...x_{n-1}}\)</span>,用它们的均值来代表 [a,b] 区间上所有的 f(x) 的值。这样我们上面的定积分的近似求解为:</p><p><span class="math display">\[\theta = \int_a^b f(x)dx = \frac{b-a}{n}\sum\limits_{i=0}^{n-1}f(x_i)\]</span></p><p>如果不是均匀分布, 并<strong>假设我们可以得到 <span class="math inline">\(x\)</span> 在<span class="math inline">\([a,b]\)</span>的概率分布函数<span class="math inline">\(p(x)\)</span></strong>，那么我们的定积分求和可以这样进行： <span class="math display">\[\theta = \int_a^b f(x)dx =  \int_a^b \frac{f(x)}{p(x)}p(x)dx \approx \frac{1}{n}\sum\limits_{i=0}^{n-1}\frac{f(x_i)}{p(x_i)}\]</span></p><p>上式最右边的这个形式就是蒙特卡罗方法的一般形式。当然这里是连续函数形式的蒙特卡罗方法，但是在离散时一样成立。</p><p>可以看出，最上面我们假设x在[a,b]之间是均匀分布的时候，p(xi)=1/(b−a)，带入我们有概率分布的蒙特卡罗积分的上式，可以得到： <span class="math display">\[\frac{1}{n}\sum\limits_{i=0}^{n-1}\frac{f(x_i)}{1/(b-a)} = \frac{b-a}{n}\sum\limits_{i=0}^{n-1}f(x_i)\]</span></p><p>也就是说，我们最上面的均匀分布也可以作为一般概率分布函数<span class="math inline">\(p(x)\)</span>在均匀分布时候的特例。那么我们现在的问题转到了如何在已知分布求出 x 的分布<span class="math inline">\(p(x)\)</span>对应的若干个样本上来。</p><h2 id="采样方法">采样方法</h2><p>主要有概率分布采样及接受-拒绝采样方法. ###概率分布采样 如果求出了<span class="math inline">\(x\)</span>的概率分布，我们可以基于概率分布去采样基于这个概率分布的 n 个<span class="math inline">\(x\)</span>的样本集，带入蒙特卡罗求和的式子即可求解。但是还有一个关键的问题需要解决，即如何基于概率分布去采样基于这个概率分布的 n 个<span class="math inline">\(x\)</span>的样本集。</p><p>一般而言均匀分布<code>Uniform(0,1)</code>的样本是相对容易生成的。 通过线性同余发生器可以生成伪随机数，我们用确定性算法生成[0,1]之间的伪随机数序列后， 这些序列的各种统计指标和均匀分布<code>Uniform(0,1)</code>的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。线性同余随机数生成器如下: <span class="math display">\[x_{n+1}=(ax_n+c)~\textrm{mod}~m\]</span> 式中<span class="math inline">\(a，c，m\)</span>是数学推导出的合适的常数。这种算法产生的下一个随机数完全依赖当前的随机数，当随机数序列足够大的时候，随机数会出现重复子序列的情况。 当然，也有很多更加先进的随机数产生算法出现，比如<code>numpy</code>用的是 <code>Mersenne Twister</code> 等.</p><p>而其他常见的概率分布，无论是离散的分布还是连续的分布，它们的样本都可以通过<code>Uniform(0,1)</code>的样本转换而得, 但是如何产生满足其他分布下的随机数呢？</p><p>比如二维正态分布的样本<span class="math inline">\((Z1,Z2)\)</span>可以通过通过独立采样得到的<code>uniform(0,1)</code>样本对<span class="math inline">\((X1,X2)\)</span>通过如下的式子转换而得： <span class="math display">\[Z_1 = \sqrt{-2 ln X_1}cos(2\pi X_2) \\Z_2 = \sqrt{-2 ln X_1}sin(2\pi X_2)\]</span></p><p>其他一些常见的连续分布，比如<code>t分布</code>，<code>F分布</code>，<code>Beta分布</code>，<code>Gamma分布</code>等，都可以通过类似的方式从<code>uniform(0,1)</code>得到的采样样本转化得到。在python的<code>numpy</code>，<code>scikit-learn</code>等类库中，都有生成这些常用分布样本的函数可以使用。</p><p>不过很多时候，我们的x的概率分布不是常见的分布，这意味着我们没法方便的得到这些非常见的概率分布的样本集。那这个问题怎么解决呢？</p><h3 id="接受-拒绝采样">接受-拒绝采样</h3><p>对于概率分布不是常见的分布，一个可行的办法是采用接受-拒绝采样来得到该分布的样本。既然 <span class="math inline">\(p(x)\)</span> 太复杂在程序中没法直接采样，那么我设定一个程序可采样的分布 <span class="math inline">\(q(x\)</span>) 比如高斯分布，然后按照一定的方法拒绝某些样本，以达到接近 <span class="math inline">\(p(x)\)</span> 分布的目的，其中<span class="math inline">\(q(x)\)</span>叫做 <code>proposal distribution</code>。</p><p>具体采用过程如下，设定一个方便采样的常用概率分布函数 q(x)，以及一个常量 k，使得 p(x) 总在 kq(x) 的下方。</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/11/16/15423393056582.jpg" alt="接受-拒绝采样-w283"><figcaption>接受-拒绝采样-w283</figcaption></figure><p>首先，采样得到<code>q(x)</code>的一个样本<span class="math inline">\(z_0\)</span>，采样方法如上，使用<code>uniform(0,1)</code>转换得到。然后，从均匀分布<span class="math inline">\((0,kq(z_0))\)</span>中采样得到一个值<span class="math inline">\(u\)</span>。如果<span class="math inline">\(u\)</span>落在了上图中的灰色区域，则拒绝这次抽样，否则接受这个样本<span class="math inline">\(z_0\)</span>。重复以上过程得到 n 个接受的样本 <span class="math inline">\(z_0,z_1,...z_{n−1}\)</span>,则最后的蒙特卡罗方法求解结果为： <span class="math display">\[\frac{1}{n}\sum\limits_{i=0}^{n-1}\frac{f(z_i)}{p(z_i)}\]</span> 整个过程中，我们通过一系列的接受拒绝决策来达到用q(x)模拟p(x)概率分布的目的。</p><h2 id="小结">小结</h2><p>使用接受-拒绝采样，我们可以解决一些概率分布不是常见的分布的时候，得到其采样集并用蒙特卡罗方法求和的目的。但是接受-拒绝采样也只能部分满足我们的需求，在很多时候我们还是很难得到我们的概率分布的样本集。比如：</p><ol type="1"><li>对于一些二维分布<span class="math inline">\(p(x,y)\)</span>，有时候我们只能得到条件分布<span class="math inline">\(p(x|y)\)</span>和<span class="math inline">\(p(y|x)\)</span>,却很难得到二维分布<span class="math inline">\(p(x,y)\)</span>一般形式，这时我们无法用接受-拒绝采样得到其样本集。</li><li>对于一些高维的复杂非常见分布<span class="math inline">\(p(x_1,x_2,...,x_n)\)</span>，我们要找到一个合适的<span class="math inline">\(q(x)和\)</span>k$非常困难。</li></ol><p>从上面可以看出，要想将蒙特卡罗方法作为一个通用的采样模拟求和的方法，必须解决如何方便得到各种复杂概率分布的对应的采样样本集的问题。</p><p>此时就需要使用一些更加复杂的随机模拟的方法来生成样本。比如马尔科夫链蒙特卡罗方法，了解这个算法我们首先要对马尔科夫链的平稳分布的性质有基本的认识。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习,深度学习以及自然语言处理等领域都有广泛的应用，是很多复杂算法求解的基础。&lt;/p&gt;
&lt;p&gt;从名字我们可以看出，MCMC由两个MC组成，即蒙特卡罗方法（Monte Carlo Simulation，简称MC）和马尔科夫链（Markov Chain ，也简称MC）。要弄懂MCMC的原理我们首先得搞清楚蒙特卡罗方法和马尔科夫链的原理。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>自剖</title>
    <link href="http://suool.net/2018/11/01/self-analysis/"/>
    <id>http://suool.net/2018/11/01/self-analysis/</id>
    <published>2018-10-31T23:15:32.000Z</published>
    <updated>2018-10-31T23:29:09.775Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="说明前面">说明前面</h2><p>这是一篇去年差不多这个时候写的东西，现在读起来依然适用现在的自己。</p><p>最近这几天是真的有点不好受，昨晚几乎失眠了一整晚，缺少很多的东西，对未来看不清楚，对自己没有信心，心里面就充满了怀疑。 <a id="more"></a> ## 正文</p><p>我终究还是成为了现在的我，我迷失了自我。</p><p>如今的我变成了什么样子呢？ 直观的感受来说，我变成了一个越来越封闭，一个离内在自我越来越远的自己了。这个过程像是我自己亲手给我自己筑了一堵墙，或者说我亲手抛弃了我自己，任由他在那里腐化、堕落、锈蚀，然后，至于此地，我越来越觉得难辨真正的自己是什么样了。</p><p>现在看上一次写这些东西，那时的我的确发现一些问题所在，但是也仅是止步于此，而后至今，我非但未能解决问题，反而使问题更加复杂和难以解决了。</p><p>现在的我越来越认同“最难也最重要的是经营好自己”这句话了。不仅是我现在近乎彻底失败于经营自己，也是在自察我的生活，直观的感觉就是众多问题都是由我而起。 自身都经营不好，谈何经营一段良好的亲密关系，谈何拥有一个好的未来。与人关系及美好的未来，如若缺少一个足够好的自己存在，怎么想都觉得不可能。</p><p>自我的经营，包括内在自我的经营和外在自我的经营。 近些日子，读了一些书，对于内在的自我和意识有了更多更深刻清晰的认识，所以，多谈一下这方面。</p><p>内在的自我不仅仅是指那个理性、逻辑的意识自我，也包括在潜意识中的自我。 内在的意识的自我是我的脑的活动的直接映射，而我的脑的活动则更多的是先验自我的映射，也即是昨日的我造就了今日的我，今日我蕴含着明日的我。 因此，经营内在自我的关键明显就在于经营当下的自我，因为当下所做的每个选择，每个情绪反应都是为以后的自我埋下伏笔，现时的每次情绪反馈和选择都不可避免的成为我的脑的先验自我的经验的一部分，从而成为其下一次重复当下的一个可能选择，重复这个选择即是再次强化意识中的自我，意识又会继续强化我的感受和情绪，进而形成了一个正向的反馈循环。 至此，改变内在自己我的唯一可能就是打破这个过去自我与当下自我的循环，从而造就新的当下自我和未来自我的新的循环的可能。说的在直接点，就是改变当下自我不如过去自我一样的选择和情绪反应，才能为产生一个新的明日自我提供可能。</p><p>纵观这些日子，我则在一遍又一遍的强化那个懒惰、消极、贪婪、放纵的自我，一次又一次的不断重复过去自我所做的错误选择，成为了今日的我，算是“修得正果”。</p><p>外在的自我的经营首先是对自己身体的经营，强身健体，其意义显然无需赘言。其次是外界关系的经营，人际沟通，心智交流。</p><p>这段时间里，在自己的亲密关系里面，因为自己不自觉的要去选择封闭自己，隐藏自己的感受，导致很多问题不仅没有被解决而且变得更加复杂和不堪。对于自己的不满意也使得自己缺少很多的耐心。</p><p>然后，现在就又回到了那个亘古难题：明明知道自己该怎么做，该做什么，但是就是不去做，做不好。</p><p>这里，我先自我剖析我的实际情况： 首先，很多情况下只有在当下这种自剖的时候，我才清醒的意识到自己该做什么，真正在日常生活中，往往是<em>由着自己的大脑做着潜意识下的决定</em>而走了。 其次，即是偶尔能够在面对选择和决定的时候（比如早上起床）意识到自己真正应该做的决定，但是要么是自己创造了一个理由没有去选择，要么是选择了但是具体执行的时候效果很不好。 最后，在自己做了不该做的选择之后，偶尔会意识到自己不应该如此，但是却无法去改变自己的方向，无法停止之前错误的选择造成的行为，而是继续任由自己错误下去。</p><p>根据上面的自己的实际情况，改变自己的可以尝试的方式是，每当自己面临比较关键的选择的时候，主动的去注意观察自己大脑的反应，观察自己潜意识的意图是什么，然后停下来，试着和自己的情绪和感受共处，不要急于做不同的决定，先试着跟它共处。 试着学会觉察自己的潜意识做决定时候的意图，站在一个第三方的角度去看自己的情绪和感受，与这个时候的自己共处，与自我的懒惰、拖延、放纵的意图保持觉察的链接。会对自己更深刻的认识和了解，多去练习这个过程。</p><p>比如早上闹钟响起，要做起不起床的决定的时候，去观察下此时自己内心的意图，不用急着说服自己起床，就纯粹是与此刻自己这个无论是要起床还是继续睡觉的意图保持链接，停下来和它共处，观察，然后再去做决定。</p><p>类似的还有在要不要锻炼，要不要赶紧关上灯关上手机睡觉等等这些决定的时候都一样，试着去观察自己当时的意图，并停下来和它共处。</p><p>这样子必然存在一定的困难，因为这是有意识的觉察无意识决定的过程，但是只要用心去做，还是做得到的，不用急着改变想法，只是尝试着要去观察并且和它共处，把它当作客观存在的东西，把自己当成一个观察的第三者。</p><p>改变，特别是向好的改变，意味着打破大脑同意识的旧有循环，而旧有循环之所以能够成为一个固化的循环，就是因为他能给大脑带来舒适，给意识带来愉悦。因此，改变，便意味这个打破这个正向反馈的“舒适愉悦”的循环，必然伴随着大脑的不适应及情绪上的痛苦体验，也必然是不易的。 但是，好的改变，都值得去尝试和牺牲。</p><p>学会和糟糕的自己和解，同做糟糕决定的大脑和意识共处，去觉察自己的懒惰、放纵等意图闪现的瞬间，然后停下来和他们共处。这是有意识觉察无意识的意图的练习。努力将有意识觉察养成无意识觉察的习惯，会发现更多关于深层次自我的东西。</p><p>这次找出了一些问题，也想了一些解决的方法，试试看吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;说明前面&quot;&gt;说明前面&lt;/h2&gt;
&lt;p&gt;这是一篇去年差不多这个时候写的东西，现在读起来依然适用现在的自己。&lt;/p&gt;
&lt;p&gt;最近这几天是真的有点不好受，昨晚几乎失眠了一整晚，缺少很多的东西，对未来看不清楚，对自己没有信心，心里面就充满了怀疑。
    
    </summary>
    
      <category term="心理学" scheme="http://suool.net/categories/%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
    
      <category term="反思" scheme="http://suool.net/tags/%E5%8F%8D%E6%80%9D/"/>
    
  </entry>
  
  <entry>
    <title>pytorch 自定义数据集及 Kaggle 101 数字识别</title>
    <link href="http://suool.net/2018/10/29/custom-dataset-use-pytorch/"/>
    <id>http://suool.net/2018/10/29/custom-dataset-use-pytorch/</id>
    <published>2018-10-29T12:54:46.000Z</published>
    <updated>2018-10-30T02:39:59.258Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>前面的文章中使用 feed forward neural network 实现了简单的手写数字识别，但是这不能直接照搬到 kaggle上面，因为 kaggle 使用的数据集是 CSV 文件，因此需要自定义一个 pytorch 的数据类型，来完成这个入门题目。</p><p>本文的提纲如下: 1. 自定义 Dataset 2. 模型搭建保存与读取 <a id="more"></a></p><h2 id="自定义-dataset">自定义 Dataset</h2><p>Pytorch的数据读取主要包含三个类: 1. Dataset 2. DataLoader 3. DataLoaderIter</p><p>这三者大致是一个依次封装的关系: Dataset 被封装进DataLoader, DataLoader 被装进 DataLoaderIter。</p><h3 id="torch.utils.data.dataset">torch.utils.data.Dataset</h3><p>这个类的源码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""An abstract class representing a Dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    All other datasets should subclass it. All subclasses should override</span></span><br><span class="line"><span class="string">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class="line"><span class="string">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure><p>其类的说明在上述源码的注释中一目了然，这是一个抽象类, 自定义的Dataset需要继承它并且实现下面两个成员方法:</p><ol type="1"><li><code>__getitem__()</code> 方法</li><li><code>__len__()</code> 方法</li></ol><p>自定义类的框架如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Initialize file path or list of file names.</span></span><br><span class="line">        <span class="comment"># 做一些初始化的工作</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span></span><br><span class="line">        <span class="comment"># 2. Preprocess the data (e.g. torchvision.Transform).</span></span><br><span class="line">        <span class="comment"># 3. Return a data pair (e.g. image and label).</span></span><br><span class="line">        <span class="comment"># 这里需要注意的是，第一步：read one data，是一个data</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># You should change 0 to the total size of your dataset.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>以下是官方的 <code>MNIST</code> 的一个参考例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""`MNIST &lt;http://yann.lecun.com/exdb/mnist/&gt;`_ Dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        root (string): Root directory of dataset where ``processed/training.pt``</span></span><br><span class="line"><span class="string">            and  ``processed/test.pt`` exist.</span></span><br><span class="line"><span class="string">        train (bool, optional): If True, creates dataset from ``training.pt``,</span></span><br><span class="line"><span class="string">            otherwise from ``test.pt``.</span></span><br><span class="line"><span class="string">        download (bool, optional): If true, downloads the dataset from the internet and</span></span><br><span class="line"><span class="string">            puts it in root directory. If dataset is already downloaded, it is not</span></span><br><span class="line"><span class="string">            downloaded again.</span></span><br><span class="line"><span class="string">        transform (callable, optional): A function/transform that  takes in an PIL image</span></span><br><span class="line"><span class="string">            and returns a transformed version. E.g, ``transforms.RandomCrop``</span></span><br><span class="line"><span class="string">        target_transform (callable, optional): A function/transform that takes in the</span></span><br><span class="line"><span class="string">            target and transforms it.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, train=True, transform=None, target_transform=None, download=False)</span>:</span></span><br><span class="line">        self.root = os.path.expanduser(root)</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.train = train  <span class="comment"># training set or test set</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> download:</span><br><span class="line">            self.download()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._check_exists():</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">'Dataset not found.'</span> +</span><br><span class="line">                               <span class="string">' You can use download=True to download it'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            self.train_data, self.train_labels = torch.load(</span><br><span class="line">                os.path.join(self.root, self.processed_folder, self.training_file))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.test_data, self.test_labels = torch.load(</span><br><span class="line">                os.path.join(self.root, self.processed_folder, self.test_file))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            index (int): Index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            tuple: (image, target) where target is index of the target class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            img, target = self.train_data[index], self.train_labels[index]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img, target = self.test_data[index], self.test_labels[index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># doing this so that it is consistent with all other datasets</span></span><br><span class="line">        <span class="comment"># to return a PIL Image</span></span><br><span class="line">        img = Image.fromarray(img.numpy(), mode=<span class="string">'L'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="keyword">return</span> len(self.train_data)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> len(self.test_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_exists</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) <span class="keyword">and</span> \</span><br><span class="line">            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Download the MNIST data if it doesn't exist in processed_folder already."""</span></span><br><span class="line">        <span class="comment"># PASS</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># PASS</span></span><br><span class="line">        <span class="keyword">return</span> fmt_str</span><br></pre></td></tr></table></figure><p>第一个最为重要, 即每次怎么读数据；上面的读取图片的例子中可以看出如何实现这个方法，值得一提的是, <code>pytorch</code> 还提供了很多常用的 <code>transform</code>, 在 <code>torchvision.transforms</code> 里面, 本文中不多介绍, 我常用的有<code>Resize</code> , <code>RandomCrop</code> , <code>Normalize</code> , <code>ToTensor</code> (这个<strong>极为重要</strong>, 可以把一个 <code>PIL或numpy</code> 图片转为 <code>torch.Tensor</code>, 但是好像对 <code>numpy</code> 数组的转换比较受限, 所以这里建议在 <code>__getitem__()</code> 里面用 <code>PIL</code> 来读图片, 而不是用 <code>skimage.io</code>)。</p><p>第二个比较简单, 就是返回整个数据集的长度。</p><p>按照这个思路，我写的Version 1.0 版的如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DigitDataSet</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_path, train=True, transform=None, target_transform=None)</span>:</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.train = train</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.data = pd.read_csv(data_path, header=<span class="number">0</span>, sep=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            img, target = self.data.iloc[index:index+<span class="number">1</span>, <span class="number">1</span>:].values.reshape(<span class="number">28</span>, <span class="number">28</span>).astype(np.uint8), self.data.iloc[index:index+<span class="number">1</span>, :<span class="number">1</span>].values[<span class="number">0</span>].tolist()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img, target = self.data.iloc[index:index + <span class="number">1</span>, :].values.reshape(<span class="number">28</span>, <span class="number">28</span>).astype(np.uint8), \</span><br><span class="line">                          <span class="number">-1</span></span><br><span class="line">        <span class="comment"># doing this so that it is consistent with all other datasets</span></span><br><span class="line">        <span class="comment"># to return a PIL Image</span></span><br><span class="line">        <span class="comment"># print(target, type(target))</span></span><br><span class="line">        img = Image.fromarray(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line">        <span class="keyword">return</span> img, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br></pre></td></tr></table></figure><p>这是一个显然不够好看的实现方式，存在一些问题，比如没办法划分训练集和验证集，只能读取训练集和测试集。有点模型训练完了，一切看天的意思。</p><p>改进版会在下次再说，这次纯粹是为了解决问题。</p><h3 id="torch.utils.data.dataloader">torch.utils.data.DataLoader</h3><p>这个类中的源代码中的注释非常详细，在此直接贴下来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">r"""</span></span><br><span class="line"><span class="string">    Data loader. Combines a dataset and a sampler, and provides</span></span><br><span class="line"><span class="string">    single- or multi-process iterators over the dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        dataset (Dataset): dataset from which to load the data.</span></span><br><span class="line"><span class="string">        batch_size (int, optional): how many samples per batch to load</span></span><br><span class="line"><span class="string">            (default: ``1``).</span></span><br><span class="line"><span class="string">        shuffle (bool, optional): set to ``True`` to have the data reshuffled</span></span><br><span class="line"><span class="string">            at every epoch (default: ``False``).</span></span><br><span class="line"><span class="string">        sampler (Sampler, optional): defines the strategy to draw samples from</span></span><br><span class="line"><span class="string">            the dataset. If specified, ``shuffle`` must be False.</span></span><br><span class="line"><span class="string">        batch_sampler (Sampler, optional): like sampler, but returns a batch of</span></span><br><span class="line"><span class="string">            indices at a time. Mutually exclusive with :attr:`batch_size`,</span></span><br><span class="line"><span class="string">            :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.</span></span><br><span class="line"><span class="string">        num_workers (int, optional): how many subprocesses to use for data</span></span><br><span class="line"><span class="string">            loading. 0 means that the data will be loaded in the main process.</span></span><br><span class="line"><span class="string">            (default: ``0``)</span></span><br><span class="line"><span class="string">        collate_fn (callable, optional): merges a list of samples to form a mini-batch.</span></span><br><span class="line"><span class="string">        pin_memory (bool, optional): If ``True``, the data loader will copy tensors</span></span><br><span class="line"><span class="string">            into CUDA pinned memory before returning them.</span></span><br><span class="line"><span class="string">        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,</span></span><br><span class="line"><span class="string">            if the dataset size is not divisible by the batch size. If ``False`` and</span></span><br><span class="line"><span class="string">            the size of dataset is not divisible by the batch size, then the last batch</span></span><br><span class="line"><span class="string">            will be smaller. (default: ``False``)</span></span><br><span class="line"><span class="string">        timeout (numeric, optional): if positive, the timeout value for collecting a batch</span></span><br><span class="line"><span class="string">            from workers. Should always be non-negative. (default: ``0``)</span></span><br><span class="line"><span class="string">        worker_init_fn (callable, optional): If not ``None``, this will be called on each</span></span><br><span class="line"><span class="string">            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as</span></span><br><span class="line"><span class="string">            input, after seeding and before data loading. (default: ``None``)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note:: By default, each worker will have its PyTorch seed set to</span></span><br><span class="line"><span class="string">              ``base_seed + worker_id``, where ``base_seed`` is a long generated</span></span><br><span class="line"><span class="string">              by main process using its RNG. However, seeds for other libraies</span></span><br><span class="line"><span class="string">              may be duplicated upon initializing workers (w.g., NumPy), causing</span></span><br><span class="line"><span class="string">              each worker to return identical random numbers. (See</span></span><br><span class="line"><span class="string">              :ref:`dataloader-workers-random-seed` section in FAQ.) You may</span></span><br><span class="line"><span class="string">              use :func:`torch.initial_seed()` to access the PyTorch seed for</span></span><br><span class="line"><span class="string">              each worker in :attr:`worker_init_fn`, and use it to set other</span></span><br><span class="line"><span class="string">              seeds before data loading.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. warning:: If ``spawn`` start method is used, :attr:`worker_init_fn` cannot be an</span></span><br><span class="line"><span class="string">                 unpicklable object, e.g., a lambda function.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    __initialized = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dataset, batch_size=<span class="number">1</span>, shuffle=False, sampler=None, batch_sampler=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_workers=<span class="number">0</span>, collate_fn=default_collate, pin_memory=False, drop_last=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 timeout=<span class="number">0</span>, worker_init_fn=None)</span>:</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line">        self.collate_fn = collate_fn</span><br><span class="line">        self.pin_memory = pin_memory</span><br><span class="line">        self.drop_last = drop_last</span><br><span class="line">        self.timeout = timeout</span><br><span class="line">        self.worker_init_fn = worker_init_fn</span><br><span class="line">        <span class="comment"># PASS</span></span><br><span class="line">        self.sampler = sampler</span><br><span class="line">        self.batch_sampler = batch_sampler</span><br><span class="line">        self.__initialized = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span><span class="params">(self, attr, val)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.__initialized <span class="keyword">and</span> attr <span class="keyword">in</span> (<span class="string">'batch_size'</span>, <span class="string">'sampler'</span>, <span class="string">'drop_last'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'&#123;&#125; attribute should not be set after &#123;&#125; is '</span></span><br><span class="line">                             <span class="string">'initialized'</span>.format(attr, self.__class__.__name__))</span><br><span class="line"></span><br><span class="line">        super(DataLoader, self).__setattr__(attr, val)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> _DataLoaderIter(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.batch_sampler)</span><br></pre></td></tr></table></figure><p>从他的 <code>__init__</code> 方法中可以看出可以看到, 主要参数有这么几个:</p><ul><li><code>dataset</code> : 即上面自定义的 dataset.</li><li><code>collate_fn</code>: 这个函数用来打包 batch.</li><li><code>num_worker</code>: 非常简单的多线程方法, 只要设置为&gt;=1, 就可以多线程预读数据</li></ul><p>这个类其实就是下面将要讲的 <code>DataLoaderIter</code> 的一个框架, 一共干了两件事: 1. 定义了一堆成员变量, 到时候赋给 <code>DataLoaderIter</code>, 2. 然后有一个 <code>__iter__()</code> 函数, 把自己 &quot;装进&quot; DataLoaderIter 里面.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> DataLoaderIter(self)</span><br></pre></td></tr></table></figure><h3 id="torch.utils.data.dataloader.dataloaderiter">torch.utils.data.dataloader.DataLoaderIter</h3><p>上面提到, <code>DataLoader</code> 就是<code>DataLoaderIter</code>的一个框架, 用来传给<code>DataLoaderIter</code> 一堆参数, 并把自己装进<code>DataLoaderIter</code> 里.</p><p>比如下面一个框架:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">   <span class="comment"># 自定义自己的dataset</span></span><br><span class="line"></span><br><span class="line">dataset = CustomDataset()</span><br><span class="line">dataloader = Dataloader(dataset, ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">   <span class="comment"># training...</span></span><br></pre></td></tr></table></figure><p>在for 循环里, 总共有三点操作:</p><ul><li>调用了 <code>dataloader</code> 的 <code>__iter__()</code> 方法, 产生了一个<code>DataLoaderIter</code></li><li>反复调用 <code>DataLoaderIter</code> 的 <code>__next__()</code> 来得到 batch, 具体操作就是, 多次调用 dataset 的 <code>__getitem__()</code> 方法 (如果<code>num_worker&gt;0</code>就多线程调用), 然后用<code>collate_fn</code>来把它们打包成batch. 中间还会涉及到<code>shuffle</code> , 以及 <code>sample</code> 的方法等, 这里就不多说了.</li><li>数据读完后, <code>__next__()</code> 抛出一个 <code>StopIteration</code> 异常, for循环结束, <code>dataloader</code> 失效.</li></ul><h2 id="模型搭建保存与读取">模型搭建保存与读取</h2><p>模型的搭建及保存部分和<a href="https://suool.net/2018/10/07/feed-forward-neural-network-on-tf-and-pytorch/">上篇文章</a>的一模一样，这里就不多说了。</p><p>关于模型的读取预测部分代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Created on 2018-10-29</span></span><br><span class="line"><span class="string">Update  on 2018-10-29</span></span><br><span class="line"><span class="string">Author: SuooL</span></span><br><span class="line"><span class="string">Github: https://github.com/SuooL</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多核 cpu 设置</span></span><br><span class="line">os.environ[<span class="string">"OMP_NUM_THREADS"</span>] = <span class="string">"8"</span></span><br><span class="line">os.environ[<span class="string">"MKL_NUM_THREADS"</span>] = <span class="string">"8"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置使用 CPU</span></span><br><span class="line">device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数配置</span></span><br><span class="line">input_size = <span class="number">784</span></span><br><span class="line">hidden_size = <span class="number">512</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 Fully connected neural network with one hidden layer 定义网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_classes)</span>:</span></span><br><span class="line">        super(NeuralNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.fc1(x)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = NeuralNet(input_size, hidden_size, num_classes).to(device)</span><br><span class="line">model_dict = model.load_state_dict(torch.load(<span class="string">'model.ckpt'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the model 预测</span></span><br><span class="line"><span class="comment"># In test phase, we don't need to compute gradients (for memory efficiency)</span></span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(<span class="string">'test.csv'</span>, header=<span class="number">0</span>, sep=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'submission.csv'</span>, <span class="string">'w'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> csv_file:</span><br><span class="line">        writer = csv.writer(csv_file, dialect=<span class="string">'excel'</span>)</span><br><span class="line">        writer.writerow([<span class="string">"ImageId"</span>, <span class="string">"Label"</span>])</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">28000</span>):</span><br><span class="line">            img = Image.fromarray(test_data.iloc[index:index+<span class="number">1</span>, :].values.reshape(<span class="number">28</span>, <span class="number">28</span>).astype(np.uint8))</span><br><span class="line">            transform1 = transforms.Compose([</span><br><span class="line">                transforms.ToTensor(),  <span class="comment"># range [0, 255] -&gt; [0.0,1.0]</span></span><br><span class="line">            ])</span><br><span class="line">            img_tensor = transform1(img)</span><br><span class="line">            images = img_tensor.reshape(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>).to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># writer.writerow([])</span></span><br><span class="line">            print(<span class="string">"picture %d, predicted number is %s"</span> % (index+<span class="number">1</span>, predicted[<span class="number">0</span>].item()))</span><br><span class="line">            writer.writerow([index+<span class="number">1</span>, predicted[<span class="number">0</span>].item()])</span><br></pre></td></tr></table></figure></p><h2 id="总结">总结</h2><p>这篇文章基本上上熟悉 pytorch 自定义数据集相关的知识，代码实现并没有过多去关注，算是完成了 kaggle 第一题的尝试。</p><p>以后会更新下这个代码，实现的更优雅一些。</p><p>下篇文章会写一些关于 NLP 处理的基础经典的算法使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;前面的文章中使用 feed forward neural network 实现了简单的手写数字识别，但是这不能直接照搬到 kaggle上面，因为 kaggle 使用的数据集是 CSV 文件，因此需要自定义一个 pytorch 的数据类型，来完成这个入门题目。&lt;/p&gt;
&lt;p&gt;本文的提纲如下: 1. 自定义 Dataset 2. 模型搭建保存与读取
    
    </summary>
    
      <category term="深度学习" scheme="http://suool.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>全连接前向神经网络与手写数字的实践</title>
    <link href="http://suool.net/2018/10/07/feed-forward-neural-network-on-tf-and-pytorch/"/>
    <id>http://suool.net/2018/10/07/feed-forward-neural-network-on-tf-and-pytorch/</id>
    <published>2018-10-07T10:15:42.000Z</published>
    <updated>2018-10-29T12:53:08.399Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>上一篇文章提到了 logistics regression 、多分类的 softmax 算法及梯度等概念，其实就可以很自然的引出深度学习了。</p><p>引用WiKi的定义： &gt;深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。</p><p>早在1958年就提出了 perceptron 的模型，即最简单的线性感知机模型，在当时引起了很大的轰动，甚至提出了机器可以取代人的说法，然而后来就被人质疑，现在看来线性感知机的限制显而易见。</p><p>然后在20世纪80年代，根据之前 perceptron 提出了 multi-layer perceptron（又叫 Neural Network）， 这个模型和当今的深度神经网络是没有显著区别的。1986年提出了反向传播的概念，但是通常大于三层的 hidden layer 就没有效果了，神经网络学习出现了梯度消失的问题。</p><p>后来在 2006年，在上述神经网络的算法模型上，取得了一些改进（RBM initialization），将之前 multi-layer perceptron 改了个名字 —— Deep Learning 重新提了出来，2009年的时候 DL 的运算开始利用 GPU，后面其在各个领域取得了一些突破性的应用进展，就火起来了。</p><p>所以，深度学习并不是什么新鲜事物，只是换了个名字的稍微改进的旧模型。 <a id="more"></a></p><h2 id="全连接前向神经网络">全连接前向神经网络</h2><p>一个全连接的前向神经网络示例如下所示，其激活函数是之前提到的 sigmod 函数，经过这个全连接的神经网络，其 weight 和 bias 都知道的情况下，输入的向量就会不断的变化，最后输出一个向量。</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/10/07/3AE7177E-8176-41EF-B7AE-54874C0E6DE8.png" alt="3AE7177E-8176-41EF-B7AE-54874C0E6DE8"><figcaption>3AE7177E-8176-41EF-B7AE-54874C0E6DE8</figcaption></figure><p>一般来说，Fully Connect Feedforward Network 的架构如下图所示，前一层每个输入都连接到下一层的所有神经元中： <img src="https://suool-bolg.b0.upaiyun.com/2018/10/07/819348CB-888B-4214-8A58-3E9E8EE9C721.png" alt="819348CB-888B-4214-8A58-3E9E8EE9"></p><p>其输入层和输出层都是一个 vector，但是其 dimension 不一定相同，其中的 hidden layer 一般有多层，这也是 Deep Learning 的 Deep 所在。</p><p>而神经网络的运算实质是矩阵运算，这也是为什么 GPU 能加速神经网络的原因所在。</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/10/07/076D7461-22D5-4378-BED1-DA8F23AFB6B0.png" alt="076D7461-22D5-4378-BED1-DA8F23AFB6B0"><figcaption>076D7461-22D5-4378-BED1-DA8F23AFB6B0</figcaption></figure><p>##实例 以之前一直在用的手写数字识别为例，分别使用 keras 和 pytorch 搭建两个 fully connect feedforward network 模型，使用 Mnist 数据集进行训练。</p><p>首先是 keras （Using TensorFlow backend.）的代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/local/bin/python3.6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多核 CPU 使用设置</span></span><br><span class="line">K.set_session(K.tf.Session(config=K.tf.ConfigProto(device_count=&#123;<span class="string">"CPU"</span>: <span class="number">8</span>&#125;,</span><br><span class="line">                inter_op_parallelism_threads=<span class="number">8</span>,</span><br><span class="line">                intra_op_parallelism_threads=<span class="number">8</span>,</span><br><span class="line">                log_device_placement=<span class="keyword">True</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensorboard 可视化        </span></span><br><span class="line">tbCallBack = keras.callbacks.TensorBoard(log_dir=<span class="string">'./Graph'</span>,</span><br><span class="line">                                         histogram_freq=<span class="number">1</span>,</span><br><span class="line">                                         write_graph=<span class="keyword">True</span>,</span><br><span class="line">                                         write_images=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    f = np.load(file_path)</span><br><span class="line">    x_train, y_train = f[<span class="string">'x_train'</span>], f[<span class="string">'y_train'</span>]</span><br><span class="line">    x_test, y_test = f[<span class="string">'x_test'</span>], f[<span class="string">'y_test'</span>]</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> (x_train, y_train), (x_test, y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化数据</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = load_data(<span class="string">'./mnist.npz'</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">X_test = X_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line">X_train = X_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">X_test = X_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">X_train /= <span class="number">255</span></span><br><span class="line">X_test /= <span class="number">255</span></span><br><span class="line"></span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将 label 数据转化为 one-hot，因为模型训练 loss 参数为 categorical_crossentropy</span></span><br><span class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</span><br><span class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个 model</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 添加第一层，输入是784维，第一层节点为 500，激活函数为 relu</span></span><br><span class="line">model.add(Dense(<span class="number">500</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"><span class="comment"># model.add(Dropout(0.2))</span></span><br><span class="line"><span class="comment"># 添加第二层，节点为 500，激活函数为 relu</span></span><br><span class="line">model.add(Dense(<span class="number">500</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line"><span class="comment"># model.add(Dropout(0.2))</span></span><br><span class="line"><span class="comment"># 添加输出层，输出 10 维，激活函数为 softmax</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置模型训练参数，loss 使用多类的对数损失函数，optimizer 优化器使用 adam，模型性能评估使用 accuracy</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练，batch_size为100， 10 个 epoch，callbacks调用 tensorboard</span></span><br><span class="line">model.fit(X_train, Y_train,</span><br><span class="line">          batch_size=<span class="number">100</span>, epochs=<span class="number">10</span>,</span><br><span class="line">          validation_data=(X_test, Y_test),</span><br><span class="line">          callbacks=[tbCallBack]</span><br><span class="line">          )</span><br><span class="line"></span><br><span class="line">score = model.evaluate(X_test, Y_test, verbose=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'Test score:'</span>, score[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>这是一个两层的全连接前向神经网络，训练了 10 epochs，准确率如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/10/07/1F78B36F-9654-4FF3-9166-F827DD83B6B9.png" alt="1F78B36F-9654-4FF3-9166-F827DD83B6B9"></p><p>没有 GPU，纯 CPU 跑起来的不算慢，准确率达到 97.7%，其神经网络结构图如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/10/07/96202340-46DD-40B4-97DB-7642D377A42D.png" alt="96202340-46DD-40B4-97DB-7642D377A42D"></p><p>pytorch 使用起来就没 keras 那么简单了，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多核 cpu 设置</span></span><br><span class="line">os.environ[<span class="string">"OMP_NUM_THREADS"</span>] = <span class="string">"8"</span></span><br><span class="line">os.environ[<span class="string">"MKL_NUM_THREADS"</span>] = <span class="string">"8"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置使用 CPU</span></span><br><span class="line">device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数配置</span></span><br><span class="line">input_size = <span class="number">784</span></span><br><span class="line">hidden_size = <span class="number">500</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># 1 MNIST dataset 加载图像数据</span></span><br><span class="line">train_dataset = torchvision.datasets.MNIST(root=<span class="string">'.'</span>,</span><br><span class="line">                                           train=<span class="keyword">True</span>,</span><br><span class="line">                                           transform=transforms.ToTensor(),</span><br><span class="line">                                           download=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">test_dataset = torchvision.datasets.MNIST(root=<span class="string">'.'</span>,</span><br><span class="line">                                          train=<span class="keyword">False</span>,</span><br><span class="line">                                          transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 Data loader pytorch的数据加载方式，tensorflow是没有的</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                           batch_size=batch_size,</span><br><span class="line">                                           shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                          batch_size=batch_size,</span><br><span class="line">                                          shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 Fully connected neural network with one hidden layer 定义网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_classes)</span>:</span></span><br><span class="line">        super(NeuralNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.fc1(x)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = NeuralNet(input_size, hidden_size, num_classes).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 Loss and optimizer 定义损失和优化函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(),</span><br><span class="line">                             lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 Train the model 训练模型</span></span><br><span class="line">total_step = len(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> enumerate(train_loader):  <span class="comment"># batch size的大小</span></span><br><span class="line">        <span class="comment"># Move tensors to the configured device</span></span><br><span class="line">        images = images.reshape(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>).to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass 前向传播</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward and optimize 后向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span></span><br><span class="line">                   .format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test the model 预测</span></span><br><span class="line"><span class="comment"># In test phase, we don't need to compute gradients (for memory efficiency)</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.reshape(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>).to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Accuracy of the network on the 10000 test images: &#123;&#125; %'</span></span><br><span class="line">            .format(<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the model checkpoint</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">'model.ckpt'</span>)</span><br></pre></td></tr></table></figure><p>准确率如下: <img src="https://suool-bolg.b0.upaiyun.com/2018/10/07/4E84C574-A959-4E97-9C9F-3BB5057F954F.png" alt="4E84C574-A959-4E97-9C9F-3BB5057F954F"></p><p>总体时间上，要比 TF 的慢，从源码编译了一遍安装还是慢。。</p><p>下一篇练习 CNN 。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;上一篇文章提到了 logistics regression 、多分类的 softmax 算法及梯度等概念，其实就可以很自然的引出深度学习了。&lt;/p&gt;
&lt;p&gt;引用WiKi的定义： &amp;gt;深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。&lt;/p&gt;
&lt;p&gt;早在1958年就提出了 perceptron 的模型，即最简单的线性感知机模型，在当时引起了很大的轰动，甚至提出了机器可以取代人的说法，然而后来就被人质疑，现在看来线性感知机的限制显而易见。&lt;/p&gt;
&lt;p&gt;然后在20世纪80年代，根据之前 perceptron 提出了 multi-layer perceptron（又叫 Neural Network）， 这个模型和当今的深度神经网络是没有显著区别的。1986年提出了反向传播的概念，但是通常大于三层的 hidden layer 就没有效果了，神经网络学习出现了梯度消失的问题。&lt;/p&gt;
&lt;p&gt;后来在 2006年，在上述神经网络的算法模型上，取得了一些改进（RBM initialization），将之前 multi-layer perceptron 改了个名字 —— Deep Learning 重新提了出来，2009年的时候 DL 的运算开始利用 GPU，后面其在各个领域取得了一些突破性的应用进展，就火起来了。&lt;/p&gt;
&lt;p&gt;所以，深度学习并不是什么新鲜事物，只是换了个名字的稍微改进的旧模型。
    
    </summary>
    
      <category term="深度学习" scheme="http://suool.net/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>logistic 算法及其在手写数字识别的实践</title>
    <link href="http://suool.net/2018/09/05/logistic-mnist/"/>
    <id>http://suool.net/2018/09/05/logistic-mnist/</id>
    <published>2018-09-05T00:47:57.000Z</published>
    <updated>2018-10-29T12:53:08.400Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>逻辑斯谛回归（logistic regression）是统计学习中的经典分类算法。最大熵模型是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model). 逻辑斯谛回归模型与最大熵模型都是对数线性模型. <a id="more"></a></p><h2 id="逻辑斯谛回归模型">逻辑斯谛回归模型</h2><p>###sigmoid 函数 首先我们知道，线性回归的公式如下： <span class="math display">\[z = \omega_0x_0+\omega_1x_1+\omega_2x_1+\cdots+\omega_nx_n = \omega^Tx \tag{1}\]</span></p><p>其次在介绍逻辑斯谛回归模型之前，我们先引入sigmoid函数，其数学形式是： <span class="math display">\[g(x) = \frac {1}{1+e^{-x}} \tag{2}\]</span></p><p>函数曲线如下: <img src="https://suool-bolg.b0.upaiyun.com/2018/09/18/15362860576396.jpg"></p><p>从上图可以看到 sigmoid 函数是一个 s 形的曲线，它的取值在 [0, 1] 之间，在远离 0 的地方函数的值会很快接近 0/1 。在横坐标的大尺度下看，sigmoid 函数很类似阶跃函数。</p><h2 id="二项逻辑斯谛回归模型">二项逻辑斯谛回归模型</h2><p>一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是：</p><p><span class="math display">\[h_\omega(x)=\frac {1}{1+e^{-z}} = \frac {1}{1+e^{-\omega^T\cdot x}} \tag{3}\]</span></p><p>其中， <span class="math display">\[y = \frac {1}{1+e^{-x}}\]</span></p><p>我们可以看到，Logistic Regression 算法是将线性函数的结果映射到了sigmoid函数中。</p><p>因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数（如公式1所示），然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以，Logistic 回归也是一种概率估计，比如这里Sigmoid 函数得出的值为0.5，可以理解为给定数据和参数，数据被分入 1 类的概率为0.5。</p><p>模型的条件概率分布函数的一般写法如下 <span class="math display">\[P(Y=1\mid x;\omega) = h_\omega(x) \tag{4}\]</span> 及 <span class="math display">\[P(Y=0 \mid x;\omega) = 1-h_\omega(x) \tag{5}\]</span></p><p>其中 <span class="math inline">\(w = (w^1,w^2, \cdots, w^n, b)^T\)</span> 为权值向量, <span class="math inline">\(x=(x^1, x^2, \cdots, x^n, 1)^T\)</span> 为输入向量.</p><p>因此逻辑斯谛回归的对数几率为 <span class="math display">\[log \frac {P(Y=1\mid x)}{1-P(Y=1\mid x)} = w \cdot x \tag{6}\]</span></p><p>由此可知, 在二项逻辑斯谛回归模型中, 输出 Y=1 的对数几率是输入 x 的线性函数. 又由式(3)可知,线性函数的值越接近正无穷, 概率值就越接近1, 反之越接近0, 这样子的模型就是逻辑斯谛回归模型.</p><p>###参数求解之最大似然估计法 模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然估计，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）越大.</p><p>首先通过式(4)及(5)确定的概率函数为： <span class="math display">\[P(y\mid x;\omega) = (h_\omega(x))^y\cdot (1-h_\omega(x))^{1-y}\]</span></p><p>因为样本数据相互独立，所以他们的联合分布可以表示为各边际分布的乘积，取似然函数为： <span class="math display">\[\begin{align}L(\omega) &amp;= \prod_{i=1}^n P(y^i \mid x^i; \omega) \\&amp; = \prod_{i=1}^n (h_\omega(x^i))^{y^i}\cdot (1-h_\omega(x^i))^{1-y^i}\end{align}\]</span></p><p>取对数似然函数： <span class="math display">\[l(\omega)= \log(L(\omega)) = \sum_{i=1}^m\log((h_\omega(x^i))^{y^i})+\log((1-h_\omega(x^i))^{1-y^i})\]</span></p><p>最大似然估计就是要求使得<span class="math inline">\(l(\omega)\)</span>取最大值时候的<span class="math inline">\(\omega\)</span>，这里可以用显然可以用梯度上升法求解，同时，如果稍微做下变换： <span class="math display">\[J(\omega) = - \frac{1}{n}l(\omega)\]</span></p><p>因为乘了一个负的系数,然后就可以使用梯度下降算法进行求解了.</p><p>###梯度上升算法 梯度上升算法的基本思想是:要找到某个函数的最大值, 最好的方法是沿着该函数的梯度方向探寻. 如果梯度记为 <span class="math inline">\(\nabla\)</span>, 那么函数 <span class="math inline">\(f(x,y)\)</span>的梯度表示为: <span class="math display">\[\nabla f(x,y) = \begin{pmatrix}  \frac{\partial f(x,y)}{\partial x} \\\frac{\partial f(x,y)}{\partial y} \end{pmatrix} \]</span></p><p>梯度上升算法的描述公式为: <span class="math display">\[w:=w+\alpha \nabla f(w)\]</span></p><p>梯度下降算法的描述公式: <span class="math display">\[w:=w-\alpha \nabla f(w)\]</span></p><p>这个梯度意味着要沿 x 的方向移动 <span class="math inline">\(\frac{\partial f(x,y)}{\partial x}\)</span> ，沿 y 的方向移动 <span class="math inline">\(\frac{\partial f(x,y)}{\partial y}\)</span> 。其中，函数<span class="math inline">\(f(x, y)\)</span>必须要在待计算的点上有定义并且可微。</p><p>梯度上升的一个示意图 <img src="https://suool-bolg.b0.upaiyun.com/2018/09/18/LR_8.png" alt="LR_8"></p><p>步长，记作 α , <span class="math inline">\(\nabla f(w)\)</span> 表示沿着梯度变化的方向</p><p>这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。</p><h3 id="简单的线性分类器">简单的线性分类器</h3><p>代码示例-暂略</p><h2 id="多项逻辑斯谛回归模型">多项逻辑斯谛回归模型</h2><p>上述的都是二分类问题, 那么如何改造 logistics 回归解决多分类问题呢?</p><p>第一种方式是直接根据每个类别，都建立一个二分类器，带有这个类别的样本标记为1，带有其他类别的样本标记为0。假如我们有 K 个类别，最后我们就得到了 K 个针对不同标记的普通的logistic分类器。</p><p>第二种方式是修改 logistic 回归的损失函数，让其适应多分类问题。这个损失函数不再笼统地只考虑二分类非1就0的损失，而是具体考虑每个样本标记的损失。这种方法叫做 softmax 回归，即 logistic 回归的多分类版本。</p><p>首先对于第一种方式, 假如给定的的数据集 <span class="math inline">\(X \in R^{m\times n}\)</span>, 他们的标记为 <span class="math inline">\(Y \in R^k\)</span>, 即是政协样本有 K 个不同的类别。</p><p>现在我们挑选出数据集中标记为 <span class="math inline">\(c(c\leq k)\)</span> 的样本，将挑选出来的样本标记为1， 其余的所有不为 c 的样本标记为0，由此训练出一个 logistics回归二分分类器， 即得到 <span class="math inline">\(h_c(x)\)</span> （表示针对标记为 c 的 logistics 分类函数）</p><p>按照上面的步骤， 我们可以得到k个不同的分类器。针对一个分类样本，我们需要找到这个k个分类函数中输出值最大的那个， 即认为是测试样本的输出标记： <span class="math display">\[\mathop{\arg\;\max}\limits_{c} h_c(x), c=1,2,\cdots,k\]</span></p><p>第二种方式是 softmax 回归</p><p><span class="math display">\[h_\theta(x^i) = \begin{bmatrix} p(y^i = 1 \mid x^i; \theta) \\ p(y^i = 2 \mid x^i; \theta) \\ \vdots \\p(y^i = k \mid x^i; \theta)\end{bmatrix} = \frac{1}{\sum_{c=1}^k e^{\theta_c^Tx^i}} \begin{bmatrix}e^{\theta_1^Tx^i} \\e^{\theta_2^Tx^i} \\\vdots \\e^{\theta_k^Tx^i}\end{bmatrix}\]</span></p><p>然后使用矩阵<span class="math inline">\(\theta\)</span>表示上式中的<span class="math inline">\(\theta_1,\cdots,\theta_k\)</span>,即是: <span class="math display">\[\theta = \begin{bmatrix}\theta_1^T \\\theta_2^T \\\vdots \\\theta_k^T \\\end{bmatrix}\]</span></p><p>Softmax回归中将 <span class="math inline">\(\textstyle x\)</span> 分类为类别 <span class="math inline">\(\textstyle j\)</span> 的概率: <span class="math display">\[p(y^i=j\mid x^i;\theta) = \frac{e^{\theta_j^T}x^i}{\sum_{l=1}^k e_l^Tx^i}\]</span></p><p>一个图解: <img src="https://suool-bolg.b0.upaiyun.com/2018/09/18/15366230049078.jpg"></p><p><img src="https://suool-bolg.b0.upaiyun.com/2018/09/18/15366242666868.jpg"></p><h3 id="损失函数">损失函数</h3><p>为了训练模型，我们需要定义一个 loss function 来描述模型对问题的分类精度。loss 越小，代表模型的分类结果与真实值的偏差越小。在多分类问题中常使用交差熵作为损失函数，交叉熵定义如下，其中y代表真实值， <span class="math inline">\(\tilde{y}\)</span> 代表预测值，n代表需要区分的类别数。</p><p><span class="math display">\[H_y(\tilde{y}) = - \sum_i^n y\log \tilde{y}\]</span></p><p>那么，为什么交叉熵可以用来判断模型对真实概率分布 估计的准确程度呢？首先补充几个知识点</p><h4 id="信息量"><strong>信息量</strong></h4><p>直觉上来说：一件事情发生的可能性越小（概率小），当这件事发生时，人们所获得信息量越大；相反，一件事极有可能发生（概率大），当这件事发生时，人们会感觉没有什么信息量，因为早知道会发生。因此信息量跟事件发生的概率有关。定义如下，其中 <span class="math inline">\(p(x_{0})\)</span> 为事件 <span class="math inline">\(x_{0}\)</span> 发生的概率，log 为自然对数。</p><p><span class="math display">\[I(x_0) = -log(p(x_0))\]</span></p><p>由于是概率所以 <span class="math inline">\(p(x_{0})\)</span> 的取值范围是 [0,1],绘制为图，可见该函数符合我们对信息量的直觉</p><p><img src="https://suool-bolg.b0.upaiyun.com/2018/09/18/15366229765258.jpg"></p><h4 id="熵">熵</h4><p>对于某个事件，有n种可能性，每种可能性有个概率值。熵用来表示所有信息量的期望, 也是随机变量不确定性的度量, 其定义如下: <span class="math display">\[H(X) = - \sum_i^n p(x_i)\log (p(x_i))\]</span></p><p>由此可见, 熵只依赖与 X 的分布,与其取值大小无关,因此熵也可以直接记做: <span class="math display">\[H(p) = - \sum_i^n p_i\log p_i\]</span></p><p>熵越大, 随机变量的不确定性越大.</p><h4 id="相对熵kl散度">相对熵（KL散度）</h4><p>对同一个问题，用P、Q同时描述，用 KL 散度来衡量两个分布的差异。 <span class="math display">\[D_{KL}(p||q)=\sum_{i}^{n}{}p(x_{i})log(\frac{p(x_{i})}{q(x_{i})})\\\]</span> 在机器学习中，P往往用来表示真实的样本分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]。当然Q越接近P越好，这样就说明预测值十分逼近真实值。也就是上式的 D_{KL}的值越小越好。</p><h4 id="交差熵">交差熵</h4><p>对 <span class="math inline">\(D_{KL}(p||q)\)</span>公式进行变形：</p><p><span class="math display">\[\begin{align}D_{KL}(p||q) &amp;= \sum_{i}^{n}{p(x_{i})log(p(x_{i}))} - \sum_{i}^{n}{p(x_{i})log(q(x_{i}))} \\&amp; = -H(p(x)) + [-\sum_{i}^{n}{p(x_{i})log(q(x_{i}))}]\end{align}\]</span></p><p>等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：</p><p><span class="math display">\[H(p,q) = - \sum_{i}^{n}{p(x_{i})log(q(x_{i}))}\]</span></p><p>在机器学习中，我们需要评估 labels 和 predicts 之间的差距，使用 KL 散度刚刚好，即 <span class="math inline">\(D_{KL}(y||\tilde{y})\)</span> ，由于KL散度中的前一部分−H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。</p><h3 id="识别数字实践">识别数字实践</h3><p>暂略</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;逻辑斯谛回归（logistic regression）是统计学习中的经典分类算法。最大熵模型是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model). 逻辑斯谛回归模型与最大熵模型都是对数线性模型.
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>问题解决记录</title>
    <link href="http://suool.net/2018/07/25/problems-solved-listed/"/>
    <id>http://suool.net/2018/07/25/problems-solved-listed/</id>
    <published>2018-07-25T14:57:12.000Z</published>
    <updated>2018-10-29T12:53:08.546Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="系统问题">系统问题</h2><h3 id="docker">Docker</h3><p><strong>1.Ubuntu 下安装 <code>docker</code> 使用非 <code>sudo</code> 命令的问题</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied</span><br></pre></td></tr></table></figure><p>解决方法： &gt;the error message tells you that your current user can’t access the docker engine, because you’re lacking permissions to access the unix socket to communicate with the engine. <strong>Temporary solution</strong> Use the <code>sudo</code> command to execute the commands with elevated permissions every time. <strong>Permanent (suggested) solution</strong> Add the current user to the docker group. This can be achieved by typing <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -<span class="selector-tag">a</span> -G docker <span class="variable">$USER</span></span><br></pre></td></tr></table></figure></p><p><strong>You have to log out and log in again</strong> for the group membership to take effect.</p><a id="more"></a><p><strong>2.Docker 换源</strong></p><p>新版的 <code>Docker</code> 使用 <code>/etc/docker/daemon.json（Linux）</code> 或者 <code>%programdata%\docker\config\daemon.json（Windows）</code> 来配置 Daemon 。</p><p>请在该配置文件中加入（没有该文件的话，请先建一个）：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"registry-mirrors"</span>: [<span class="string">"https://docker.mirrors.ustc.edu.cn"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完成上述配置后，执行如下命令即可：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="keyword"> system</span>ctl daemon-reload</span><br><span class="line">sudo<span class="keyword"> system</span>ctl restart docker</span><br></pre></td></tr></table></figure><h3 id="替换及重置homebrew默认源">替换及重置Homebrew默认源</h3><p>替换 <code>brew.git</code>:</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd </span><span class="string">"$(brew --repo)"</span></span><br><span class="line"><span class="string">git </span><span class="string">remote </span><span class="built_in">set-url</span> <span class="string">origin </span><span class="string">https:</span>//<span class="string">mirrors.</span><span class="string">ustc.</span><span class="string">edu.</span><span class="string">cn/</span><span class="string">brew.</span><span class="string">git</span></span><br></pre></td></tr></table></figure><p>替换 <code>homebrew-core.git</code>:</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd </span><span class="string">"$(brew --repo)/Library/Taps/homebrew/homebrew-core"</span></span><br><span class="line"><span class="string">git </span><span class="string">remote </span><span class="built_in">set-url</span> <span class="string">origin </span><span class="string">https:</span>//<span class="string">mirrors.</span><span class="string">ustc.</span><span class="string">edu.</span><span class="string">cn/</span><span class="string">homebrew-core.</span><span class="string">git</span></span><br></pre></td></tr></table></figure><p>替换 <code>Homebrew Bottles</code> 源: 参考:<a href="https://lug.ustc.edu.cn/wiki/mirrors/help/homebrew-bottles" target="_blank" rel="noopener">替换 Homebrew Bottles 源</a></p><p>在中科大源失效或宕机时可以： 1. <a href="https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/" target="_blank" rel="noopener">使用清华源设置参考</a>。 2. 切换回官方源：</p><p>重置 <code>brew.git</code>:</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd </span><span class="string">"$(brew --repo)"</span></span><br><span class="line"><span class="string">git </span><span class="string">remote </span><span class="built_in">set-url</span> <span class="string">origin </span><span class="string">https:</span>//<span class="string">github.</span><span class="string">com/</span><span class="string">Homebrew/</span><span class="string">brew.</span><span class="string">git</span></span><br></pre></td></tr></table></figure><p>重置 <code>homebrew-core.git</code>:</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd </span><span class="string">"$(brew --repo)/Library/Taps/homebrew/homebrew-core"</span></span><br><span class="line"><span class="string">git </span><span class="string">remote </span><span class="built_in">set-url</span> <span class="string">origin </span><span class="string">https:</span>//<span class="string">github.</span><span class="string">com/</span><span class="string">Homebrew/</span><span class="string">homebrew-core.</span><span class="string">git</span></span><br></pre></td></tr></table></figure><p>注释掉bash配置文件里的有关Homebrew Bottles即可恢复官方源。 重启bash或让bash重读配置文件。</p><h3 id="xcode-高亮和提示失效">Xcode 高亮和提示失效</h3><p>Xcode 在长时间运行之后，会出现一个问题，那就是 例如 return 不再高亮显示了。 重启Xcode 重启机器 也不能解决和个问题。 后来找了一些资料 解决了这个问题</p><ol type="1"><li>由于 DerivedData 问题<ul><li>关闭项目，Xcode 设置，选择 Localtions 点击 Derived Data 的的箭头进入 DerivedData 目录</li><li>直接进入 /Users/jingwenzheng/Library/Developer/Xcode/DerivedData 目录，删除这里面所有的文件</li><li>重启Xcode</li></ul></li><li>由于 pch 文件的问题<ul><li>把.pch里的内容全部注释掉，clean掉项目里的内容，把.pch里的注释去掉，编译。代码高亮，语法提示功能都回来了。</li></ul></li></ol><h2 id="vscode">VSCode</h2><p><a href="https://segmentfault.com/a/1190000012322533" target="_blank" rel="noopener">mac vscode Python配置</a> <a href="https://blog.csdn.net/ever_now_future/article/details/79075581" target="_blank" rel="noopener">使用vs(visual studio code)写python代码遇到的import requests失败问题</a> ## TensorFlow <a href="https://www.jianshu.com/p/1d305ac47d61" target="_blank" rel="noopener">源码编译安装TensorFlow</a> <a href="http://nooverfit.com/wp/tensorflow%E5%A6%82%E4%BD%95%E5%85%85%E5%88%86%E4%BD%BF%E7%94%A8%E6%89%80%E6%9C%89cpu%E6%A0%B8%E6%95%B0%EF%BC%8C%E6%8F%90%E9%AB%98tensorflow%E7%9A%84cpu%E4%BD%BF%E7%94%A8%E7%8E%87%EF%BC%8C%E4%BB%A5/" target="_blank" rel="noopener">TensorFlow如何充分使用所有CPU核数，提高TensorFlow的CPU使用率，以及Intel的MKL加速</a> <a href="https://blog.csdn.net/rockingdingo/article/details/55652662" target="_blank" rel="noopener">Tensorflow并行计算：多核(multicore)，多线程(multi-thread)，图分割(Graph Partition)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;系统问题&quot;&gt;系统问题&lt;/h2&gt;
&lt;h3 id=&quot;docker&quot;&gt;Docker&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1.Ubuntu 下安装 &lt;code&gt;docker&lt;/code&gt; 使用非 &lt;code&gt;sudo&lt;/code&gt; 命令的问题&lt;/strong&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;解决方法： &amp;gt;the error message tells you that your current user can’t access the docker engine, because you’re lacking permissions to access the unix socket to communicate with the engine. &lt;strong&gt;Temporary solution&lt;/strong&gt; Use the &lt;code&gt;sudo&lt;/code&gt; command to execute the commands with elevated permissions every time. &lt;strong&gt;Permanent (suggested) solution&lt;/strong&gt; Add the current user to the docker group. This can be achieved by typing &lt;figure class=&quot;highlight stylus&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo usermod -&lt;span class=&quot;selector-tag&quot;&gt;a&lt;/span&gt; -G docker &lt;span class=&quot;variable&quot;&gt;$USER&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You have to log out and log in again&lt;/strong&gt; for the group membership to take effect.&lt;/p&gt;
    
    </summary>
    
      <category term="技术" scheme="http://suool.net/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="记录" scheme="http://suool.net/tags/%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>决策树及其在手写数字识别的实践</title>
    <link href="http://suool.net/2018/07/19/Mnist-With-Decision-Tree-ID3/"/>
    <id>http://suool.net/2018/07/19/Mnist-With-Decision-Tree-ID3/</id>
    <published>2018-07-19T02:32:03.000Z</published>
    <updated>2018-10-29T12:53:08.398Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>##引言 决策时是一种基本的分类和回归方法，现在主要讨论分类决策树。决策树模型呈树形结构，在分类问题中，表示基本特征对实例进行分类的过程，你可以认为他是一个 <code>if-then</code> 的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。 <a id="more"></a></p><p>其优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测的时，对新的数据利用训练建立的决策树模型来分类。</p><p>决策树学习分为三个步骤：特征选择、决策树生成和决策树的剪枝。主要的决策树生成算法有 ID3 算法、C4.5 算法、 CART 算法。</p><p>本文的大纲如下： 1. 介绍决策树模型的基本概念 2. 决策树的特征选择和学习过程 3. 以 ID3 算法为例进行手写数字识别实践</p><h2 id="决策树模型的基本概念">决策树模型的基本概念</h2><h3 id="决策树模型">决策树模型</h3><p>分类决策树是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成，节点分为两类：内部结点和叶节点。内部结点表示一个特征或者属性，叶节点表示一个分类。</p><p>用决策树分类的过程类似于一系列的 <code>if-then</code> 判断，如下图的一个决策树，圆和方框分别表示内部节点和叶节点，决策分类过程是这样的：首先从顶端的根节点出发，每个内部结点都是一个特征判断，即是 <code>if-then</code> 判断，如果满足特征是一种路径，不满足特征是另一条路径。</p><p><img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/D295E3E5-FD3E-4509-BBA7-D80D26317A61.png" alt="D295E3E5-FD3E-4509-BBA7-D80D26317A61"> ### 决策树学习 决策树学习，假设给定训练数据集： <span class="math display">\[D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}\]</span></p><p>其中，<span class="math inline">\(x_i = (x_i^1,x_i^2,\cdots,x_i^n)\)</span> 为输入实力（特征向量），n为特征个数， <span class="math inline">\(y_i\in\{1,2,\cdots,K\}\)</span>为类标记，<span class="math inline">\(i=1,2,\cdots,N\)</span>，N 为样本容量。学习的目标是根据给定的训练数据集构建一个决策树模型，使他能够进行正确的分类。</p><p>而通过上面决策树的概念介绍，我们可以知道，决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即是能对训练数据进行正确分类的决策树）可能有多个，有可能一个都没有。我们需要的是一个与训练数据矛盾较小的决策树，同时有较好的泛化能力。</p><p>决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略也自然是以损失函数为目标函数的最小化。而这是一个 NP 问题，所以一般采用启发式的方法求得一个次最优解。</p><p>决策树学习的算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。</p><h2 id="特征选择">特征选择</h2><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率，而衡量特征分类效果的函数就是信息增益函数。</p><p>###信息增益 信息增益是信息论中的概念，了解信息增益，首先要了解熵和条件熵的定义。 <img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/02E0F04F-0D7E-463B-BBEC-92B7909204F8.png" alt="熵的定义"> <img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/E16F35E0-799D-4CD7-8927-2CCC0BFBBC17.png" alt="条件熵"></p><p><strong>信息增益的定义</strong>：特征 A 对训练集 D 对信息增益 g(D,A)，定义为集合 D 的经验熵 H(D) 与特征 A 给定的条件下 D 的经验条件熵 H(D|A) 之差，即是： <span class="math display">\[g(D,A) = H(D)-H(D|A)\]</span></p><p><strong>信息增益算法</strong></p><ul><li>基本假设 <img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/D145C82E-3CEC-40BA-BBA3-223CC39CE9AB.png" alt="基本元素定义"></li></ul><p>于是信息增益算法如下： <strong>输入</strong>：训练数据集 D 和特征 A； <strong>输出</strong>：特征 A 对训练数据集 D 的信息增益 g(D,A) <img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/18A795F2-9690-479B-B780-B893C562ED4D.png" alt="算法步骤"></p><h2 id="决策树生成">决策树生成</h2><p>ID3 算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树。具体点方法是：从根结点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归的调用以上方法，构建决策树；知道所有的特征的信心增益均很小或者没有特征可以选择为止。最后得到一个决策树，ID3 算法相当于用最大似然估计进行概率模型的选择。</p><p>###ID3 算法 <strong>输入：</strong>训练数据 D，特征集 A，阈值 <span class="math inline">\(\epsilon\)</span> <strong>输出：</strong>决策树 <img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/A2008824-0C64-4C99-AB08-27B6AD708FE0.png" alt="ID3-1"> <img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/5D4D317A-2635-47DC-81B5-CCF172A6BD72.png" alt="ID3-2"></p><p>ID3 算法只有树的生成，所以其生成的树很容易过拟合。</p><p>以下为该算法的代码在 Mnist 数据集上实现的准确率，86.7%，比不上 KNN 的准确度，但是速度比其快的多。</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/07/24/8959541E-8E3F-4094-997E-C839D795ACA9.png" alt="预测准确率"><figcaption>预测准确率</figcaption></figure><p>代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#encoding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> testLibrary <span class="keyword">as</span> tl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ALL_DATA = <span class="number">60000</span></span><br><span class="line"></span><br><span class="line">total_class = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        logging.debug(<span class="string">'start %s()'</span> % func.__name__)</span><br><span class="line">        ret = func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">        end_time = time.time()</span><br><span class="line">        logging.debug(<span class="string">'end %s(), cost %s seconds'</span> % (func.__name__,end_time-start_time))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二值化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binaryzation</span><span class="params">(img)</span>:</span></span><br><span class="line">    cv_img = img.astype(np.uint8)</span><br><span class="line">    cv2.threshold(cv_img,<span class="number">50</span>,<span class="number">1</span>,cv2.cv.CV_THRESH_BINARY_INV,cv_img)</span><br><span class="line">    <span class="keyword">return</span> cv_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binaryzation_features</span><span class="params">(trainset)</span>:</span></span><br><span class="line">    features = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> trainset:</span><br><span class="line">        img = np.reshape(img,(<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">        cv_img = img.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">        img_b = binaryzation(cv_img)</span><br><span class="line">        <span class="comment"># hog_feature = np.transpose(hog_feature)</span></span><br><span class="line">        features.append(img_b)</span><br><span class="line"></span><br><span class="line">    features = np.array(features)</span><br><span class="line">    features = np.reshape(features, (<span class="number">-1</span>, <span class="number">784</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, node_type, Class=None, feature=None)</span>:</span></span><br><span class="line">        self.node_type = node_type</span><br><span class="line">        self.dict = &#123;&#125;</span><br><span class="line">        self.Class = Class</span><br><span class="line">        self.feature = feature</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_tree</span><span class="params">(self, val, tree)</span>:</span></span><br><span class="line">        self.dict[val] = tree</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.node_type == <span class="string">'leaf'</span>:</span><br><span class="line">            <span class="keyword">return</span> self.Class</span><br><span class="line"></span><br><span class="line">        tree = self.dict[features[self.feature]]</span><br><span class="line">        <span class="keyword">return</span> tree.predict(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        calculate shanno ent of x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    x_value_list = set([x[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>])])</span><br><span class="line">    ent = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        p = float(x[x == x_value].shape[<span class="number">0</span>]) / x.shape[<span class="number">0</span>]</span><br><span class="line">        logp = np.log2(p)</span><br><span class="line">        ent -= p * logp</span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_condition_ent</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        calculate ent H(y|x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calc ent(y|x)</span></span><br><span class="line">    x_value_list = set([x[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>])])</span><br><span class="line">    ent = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        sub_y = y[x == x_value]</span><br><span class="line">        temp_ent = calc_ent(sub_y)</span><br><span class="line">        ent += (float(sub_y.shape[<span class="number">0</span>]) / y.shape[<span class="number">0</span>]) * temp_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># def calc_ent_grap(x,y):</span></span><br><span class="line"><span class="comment">#     """</span></span><br><span class="line"><span class="comment">#         calculate ent grap</span></span><br><span class="line"><span class="comment">#     """</span></span><br><span class="line"><span class="comment">#     base_ent = calc_ent(y)</span></span><br><span class="line"><span class="comment">#     condition_ent = calc_condition_ent(x, y)</span></span><br><span class="line"><span class="comment">#     ent_grap = base_ent - condition_ent</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     return ent_grap</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recurse_train</span><span class="params">(train_set, train_label, features, epsilon)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> total_class</span><br><span class="line"></span><br><span class="line">    LEAF = <span class="string">'leaf'</span></span><br><span class="line">    INTERNAL = <span class="string">'internal'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤1——如果train_set中的所有实例都属于同一类Ck</span></span><br><span class="line">    label_set = set(train_label)</span><br><span class="line">    <span class="keyword">if</span> len(label_set) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> Tree(LEAF, Class=label_set.pop())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤2——如果features为空</span></span><br><span class="line">    (max_class, max_len) = max([(i, len([x <span class="keyword">for</span> x <span class="keyword">in</span> train_label <span class="keyword">if</span> x == i])) <span class="keyword">for</span> i <span class="keyword">in</span> range(total_class)],key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(features) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> Tree(LEAF, Class=max_class)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤3——计算信息增益</span></span><br><span class="line">    max_feature = <span class="number">0</span></span><br><span class="line">    max_gda = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    d = train_label</span><br><span class="line">    hd = calc_ent(d)</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        A = np.array(train_set[:, feature].flat)</span><br><span class="line">        gda = hd - calc_condition_ent(A, d)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> gda &gt; max_gda:</span><br><span class="line">            max_gda, max_feature = gda, feature</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤4——小于阈值</span></span><br><span class="line">    <span class="keyword">if</span> max_gda &lt; epsilon:</span><br><span class="line">        <span class="keyword">return</span> Tree(LEAF, Class=max_class)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 步骤5——构建非空子集</span></span><br><span class="line">    sub_features = [x <span class="keyword">for</span> x <span class="keyword">in</span> features <span class="keyword">if</span> x != max_feature]</span><br><span class="line">    tree = Tree(INTERNAL, feature=max_feature)</span><br><span class="line"></span><br><span class="line">    feature_col = np.array(train_set[:, max_feature].flat)</span><br><span class="line">    feature_value_list = set([feature_col[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(feature_col.shape[<span class="number">0</span>])])</span><br><span class="line">    <span class="keyword">for</span> feature_value <span class="keyword">in</span> feature_value_list:</span><br><span class="line"></span><br><span class="line">        index = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_label)):</span><br><span class="line">            <span class="keyword">if</span> train_set[i][max_feature] == feature_value:</span><br><span class="line">                index.append(i)</span><br><span class="line"></span><br><span class="line">        sub_train_set = train_set[index]</span><br><span class="line">        sub_train_label = train_label[index]</span><br><span class="line"></span><br><span class="line">        sub_tree = recurse_train(sub_train_set,sub_train_label,sub_features,epsilon)</span><br><span class="line">        tree.add_tree(feature_value,sub_tree)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_set, train_label, features, epsilon)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> recurse_train(train_set, train_label, features, epsilon)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@log</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(test_set, tree)</span>:</span></span><br><span class="line"></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> features <span class="keyword">in</span> test_set:</span><br><span class="line">        tmp_predict = tree.predict(features)</span><br><span class="line">        result.append(tmp_predict)</span><br><span class="line">    <span class="keyword">return</span> np.array(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_accuracy</span><span class="params">(predict_ary, test)</span>:</span></span><br><span class="line">    right_count = <span class="number">0.0</span></span><br><span class="line">    accuracy_ary = []</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(len(predict_ary)):</span><br><span class="line">        <span class="keyword">if</span> predict_ary[index] == test[index]:</span><br><span class="line">            right_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">                accuracy_ary.append(float(right_count) / (index + <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">"预测值：%d 实际值： %d"</span> % (predict_ary[index], test[index]))</span><br><span class="line">    <span class="keyword">return</span> right_count/len(test), accuracy_ary</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    logger = logging.getLogger()</span><br><span class="line">    logger.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取训练数据集和测试数据集的方法和朴素贝叶斯方法一致</span></span><br><span class="line">    print(<span class="string">'Start read train data'</span>)</span><br><span class="line">    time_1 = time.time()</span><br><span class="line">    data_map, labels = tl.loadCSVfile(<span class="string">"data.csv"</span>)</span><br><span class="line">    print(data_map.shape, labels.shape)</span><br><span class="line">    time_2 = time.time()</span><br><span class="line">    print(<span class="string">'read data train cost '</span>, time_2 - time_1, <span class="string">' seconds'</span>, <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Start read predict data'</span>)</span><br><span class="line">    time_3 = time.time()</span><br><span class="line">    test_data_map, test_labels = tl.loadCSVfile(<span class="string">"dataTest.csv"</span>)</span><br><span class="line">    print(test_data_map.shape, test_data_map.shape)</span><br><span class="line">    time_4 = time.time()</span><br><span class="line">    print(<span class="string">'read predict data cost '</span>, time_4 - time_3, <span class="string">' seconds'</span>, <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    tree = train(data_map, labels, [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">784</span>)], <span class="number">0.1</span>)</span><br><span class="line">    test_predict = predict(test_data_map, tree)</span><br><span class="line"></span><br><span class="line">    rate, accuracy = calculate_accuracy(test_predict, test_labels)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"The accuracy score is "</span>, rate)</span><br><span class="line">    new_ticks = np.linspace(<span class="number">1</span>, <span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">    plt.xticks(new_ticks)</span><br><span class="line">    plt.ylim(ymin=<span class="number">0.7</span>, ymax=<span class="number">1</span>)</span><br><span class="line">    plt.plot(new_ticks, accuracy, <span class="string">'o-'</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"x -- 1:500"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">    plt.title(<span class="string">u"预测准确率"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##引言 决策时是一种基本的分类和回归方法，现在主要讨论分类决策树。决策树模型呈树形结构，在分类问题中，表示基本特征对实例进行分类的过程，你可以认为他是一个 &lt;code&gt;if-then&lt;/code&gt; 的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K 近邻法及其在手写数字识别的实践</title>
    <link href="http://suool.net/2018/07/10/Mnist-With-K-Nearest-Neighbor/"/>
    <id>http://suool.net/2018/07/10/Mnist-With-K-Nearest-Neighbor/</id>
    <published>2018-07-10T09:09:08.000Z</published>
    <updated>2018-10-29T12:53:08.398Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>k 近邻法（k-nearest-neighbor, KNN）是一种基本的分类和回归方法。现在只讨论其分类方面的应用，它不具备明显的学习过程，实际上是利用已知的训练数据集对输入特征向量空间进行划分，并作为其分类的“模型”。 <a id="more"></a></p><p>其中 k 值的选择、距离的度量及分类决策规则是 k 近邻模型的三个基本要素。</p><p>本文将按照以下提纲进行： - k 近邻法阐述 - k 近邻的模型 - k 近邻在手写数字识别上的实战</p><h2 id="k-近邻法阐述">k 近邻法阐述</h2><p>k 近邻算法非常容易理解，因为其本质上就是求距离，这是非常简单而直观的度量方法：对于给定的一个训练数据集，对新的输入实例 M，在训练数据集中找到与该新实例 M 最邻近的 k 个实例，由这 k 个实例按照一定的表决规则进行投票决策最合适的类别，那么实例 M 就属于这个类。下面是算法的描述： <img src="https://suool-bolg.b0.upaiyun.com/2018/07/10/CC3A072A-F73B-4A54-A6F9-94EF80316B66.png" alt="CC3A072A-F73B-4A54-A6F9-94EF80316B66"></p><h2 id="k-近邻模型">k 近邻模型</h2><p>k 近邻算法本质上是在超空间内划分区域空间分类的问题，在输入数据集的特征空间内，对于每个训练实例点 <span class="math inline">\(x_i\)</span> ，距离改点比其他点更近的所有点组成一个区域，叫做单元(cell)。上文说了 k 近邻模型的三个要素，k 值选择、距离度量、决策函数，下面一一说明。</p><h3 id="k-值选择">k 值选择</h3><p>k 值指的是选择近邻点的数目，如果 k = 1 则是最近邻，即是每次由距离新实例最近的训练点所属的类别决定待分类实例的类别。</p><p>k 值的选择对于 k 近邻法的结果可以产生重大影响。</p><p>当 k 值较小的时候，那么预测学习的近似误差会减少，因为此时只有距离待分类点较近的训练实例才会对于分类预测结果有影响作用，但是缺点是估计误差会增大，因为预测结果会对近邻的实例点非常敏感，如果近邻的实例多数都是噪声点，那么就很容易导致预测出错。即是说，k 值的减少意味着模型变得复杂，容易发生过拟合。</p><p>当 k 值较大的时候，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大，这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。k 值的增大就意味着整体的模型变得简单。</p><p>如果 k = N，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。</p><p>在应用中，k 值一般取一个比较小的数值。通常采用交叉验证法来选取最优的 k 值。</p><h3 id="距离度量">距离度量</h3><p>在实数域中，数的大小和两个数之间的距离是通过绝对值来度量的。在解析几何中，向量的大小和两个向量之差的大小是“长度”和“距离”的概念来度量的。为了对矩阵运算进行数值分析，我们需要对向量 和矩阵的“大小”引进某种度量。而范数是绝对值概念的自然推广。</p><p>特征空间中两个实例点的距离是其相似程度的反映，k 近邻空间选用欧式距离及更一般的 <span class="math inline">\(L_p\)</span> 距离。</p><p>设特征空间 X 是 n 维实数向量空间 <span class="math inline">\(R^n\)</span>，<span class="math inline">\(x_i,x_j \in \mathcal{X}, x_i = (x_i^1,x_i^2,\cdots,x_i^n), x_j = (x_j^1,x_j^2,\cdots,x_j^n)\)</span>，则 <span class="math inline">\(x_i, x_j\)</span> 的<span class="math inline">\(L_p\)</span>距离定义为: <span class="math display">\[L_p(x_i,x_j) =  \left(\sum_{l=1}^n |x_i^l-x_j^l|^p\right) ^{\frac{1}{p}}\]</span></p><p>这里 p 要不小于1，当 p = 2时，成为欧氏距离； 当 p = 1 时，称为曼哈顿距离； 当 p = <span class="math inline">\(\infty\)</span> 时，它是各个坐标距离的最大值。</p><h3 id="分类决策">分类决策</h3><p>k 近邻法中的分类决策规则往往是多数投票表决，即由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。</p><p>多数表决规则（majorityvotingrule）有如下解释：如果分类的损失函数为 0-1 损失函数，分类函数为： <span class="math display">\[f:R^n \to \{c_1,c_2,\cdots, c_K\}\]</span></p><p>那么对给定的实例 <span class="math inline">\(x\in X\)</span>，其最近邻的 k 个训练实例点构成集合 <span class="math inline">\(N_k(x\)</span>。如果涵盖<span class="math inline">\(N_k(x)\)</span>的区域的类别是，那么误分类率是： <span class="math display">\[\frac{1}{k} \sum_{x_i\in N_k(x)} I (y_i\not=c_j) = 1 - \frac{1}{k} \sum_{x_i\in N_k(x)} I (y_i = c_j)\]</span></p><p>要使误分类率最小即经验风险最小，就要使<span class="math inline">\(\sum_{x_i\in N_k(x)} I (y_i = c_j)\)</span>最大，所以多数表决规则等价于经验风险最小化。</p><p>同时多数表决可以加权表决，可以一定程度提高表决结果的准确性。</p><h2 id="k-近邻在手写数字识别上的实战">k 近邻在手写数字识别上的实战</h2><p>数据集的读取和解析和<a href="https://suool.net/2018/07/08/navie-bayes-handwriting-recognition/">朴素贝叶斯法识别手写数字</a>的原理一样，这里不再赘述。</p><p>代码实现算法上，这里先采用线性暴搜的方法，效率上明显是非常低的，耗时也比朴素贝叶斯慢的多，但是准确率却非常高，目前表决数为 k=3 的情况下且不加权的预测准确率可以达到 94% 以上。</p><p>训练预测结果如下: <img src="https://suool-bolg.b0.upaiyun.com/2018/07/10/597C98CC-BFA6-4998-9E03-A43C6BAF6620.png" alt="597C98CC-BFA6-4998-9E03-A43C6BAF6620"></p><p>可以看出，测试 2100 个图片，用了1218秒，20多分钟，效率非常慢。 <img src="https://suool-bolg.b0.upaiyun.com/2018/07/10/A535D08C-349E-46B2-8147-E59F88076FB1.png" alt="A535D08C-349E-46B2-8147-E59F88076FB1"></p><p>但是准确率异常高，且比较稳定。</p><h3 id="总结">总结</h3><ul><li><p>更高效率的 k 近邻寻找方法是 k-d树（k-dimensional树的简称），这是一种分割 k 维数据空间的数据结构，主要应用于多维空间关键数据的搜索。</p></li><li><p>可以对 k 近邻进行加权表决，对于预测准确率应该也会有所提升。</p></li></ul><h2 id="next">NEXT</h2><p>下一次将实践以上的两点总结，看看具体的表现如何吧。</p><hr><p>附KNN 算法的线性暴搜实现如下:</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># -*- coding: utf-8 -*</span></a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> time</a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> testLibrary <span class="im">as</span> tl</a><a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">import</span> collections</a><a class="sourceLine" id="cb1-6" data-line-number="6"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-7" data-line-number="7"></a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co"># 距离计算</span></a><a class="sourceLine" id="cb1-9" data-line-number="9"><span class="kw">def</span> calc_dis(train_image,test_image):</a><a class="sourceLine" id="cb1-10" data-line-number="10">    dist<span class="op">=</span>np.linalg.norm(train_image<span class="op">-</span>test_image)</a><a class="sourceLine" id="cb1-11" data-line-number="11">    <span class="cf">return</span> dist</a><a class="sourceLine" id="cb1-12" data-line-number="12"></a><a class="sourceLine" id="cb1-13" data-line-number="13"></a><a class="sourceLine" id="cb1-14" data-line-number="14"><span class="co"># 确定待分类实例的 k 近邻</span></a><a class="sourceLine" id="cb1-15" data-line-number="15"><span class="kw">def</span> find_labels(k,train_images,train_labels,test_image):</a><a class="sourceLine" id="cb1-16" data-line-number="16">    all_dis <span class="op">=</span> []</a><a class="sourceLine" id="cb1-17" data-line-number="17">    labels<span class="op">=</span>collections.defaultdict(<span class="bu">int</span>)</a><a class="sourceLine" id="cb1-18" data-line-number="18">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(train_images)):</a><a class="sourceLine" id="cb1-19" data-line-number="19">        dis <span class="op">=</span> np.linalg.norm(train_images[i]<span class="op">-</span>test_image)</a><a class="sourceLine" id="cb1-20" data-line-number="20">        all_dis.append(dis)</a><a class="sourceLine" id="cb1-21" data-line-number="21">    sorted_dis <span class="op">=</span> np.argsort(all_dis)</a><a class="sourceLine" id="cb1-22" data-line-number="22">    count <span class="op">=</span> <span class="dv">0</span></a><a class="sourceLine" id="cb1-23" data-line-number="23">    <span class="cf">while</span> count <span class="op">&lt;</span> k:</a><a class="sourceLine" id="cb1-24" data-line-number="24">        labels[train_labels[sorted_dis[count]]]<span class="op">+=</span><span class="dv">1</span></a><a class="sourceLine" id="cb1-25" data-line-number="25">        count <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb1-26" data-line-number="26">    <span class="cf">return</span> labels</a><a class="sourceLine" id="cb1-27" data-line-number="27"></a><a class="sourceLine" id="cb1-28" data-line-number="28"></a><a class="sourceLine" id="cb1-29" data-line-number="29"><span class="co"># 结合训练数据集，对所有待分类实例进行 k 近邻分类预测</span></a><a class="sourceLine" id="cb1-30" data-line-number="30"><span class="kw">def</span> knn_all(k,train_images,train_labels,test_images, test_labels):</a><a class="sourceLine" id="cb1-31" data-line-number="31">    <span class="bu">print</span>(<span class="st">&quot;start knn_all!&quot;</span>)</a><a class="sourceLine" id="cb1-32" data-line-number="32">    res<span class="op">=</span>[]</a><a class="sourceLine" id="cb1-33" data-line-number="33">    right <span class="op">=</span> <span class="dv">0</span></a><a class="sourceLine" id="cb1-34" data-line-number="34">    accuracy <span class="op">=</span> []</a><a class="sourceLine" id="cb1-35" data-line-number="35">    count<span class="op">=</span><span class="dv">0</span></a><a class="sourceLine" id="cb1-36" data-line-number="36">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2100</span>):</a><a class="sourceLine" id="cb1-37" data-line-number="37">        labels<span class="op">=</span>find_labels(k,train_images,train_labels,test_images[i])</a><a class="sourceLine" id="cb1-38" data-line-number="38">        res.append(<span class="bu">max</span>(labels))</a><a class="sourceLine" id="cb1-39" data-line-number="39">        <span class="bu">print</span>(<span class="st">&quot;Picture </span><span class="sc">%d</span><span class="st"> has been predicted! real is </span><span class="sc">%d</span><span class="st"> predicted is </span><span class="sc">%d</span><span class="st">&quot;</span><span class="op">%</span>(count, test_labels[i], <span class="bu">max</span>(labels)))</a><a class="sourceLine" id="cb1-40" data-line-number="40">        count<span class="op">+=</span><span class="dv">1</span></a><a class="sourceLine" id="cb1-41" data-line-number="41">        <span class="cf">if</span> <span class="bu">max</span>(labels) <span class="op">==</span> test_labels[i]:</a><a class="sourceLine" id="cb1-42" data-line-number="42">            right<span class="op">+=</span><span class="dv">1</span></a><a class="sourceLine" id="cb1-43" data-line-number="43">        <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">70</span> <span class="op">==</span> <span class="dv">0</span>:</a><a class="sourceLine" id="cb1-44" data-line-number="44">            accuracy.append(<span class="bu">float</span>(right)<span class="op">/</span>(i<span class="op">+</span><span class="dv">1</span>))</a><a class="sourceLine" id="cb1-45" data-line-number="45">    <span class="cf">return</span> res, accuracy</a><a class="sourceLine" id="cb1-46" data-line-number="46"></a><a class="sourceLine" id="cb1-47" data-line-number="47"></a><a class="sourceLine" id="cb1-48" data-line-number="48"><span class="co"># 总的预测准确率计算</span></a><a class="sourceLine" id="cb1-49" data-line-number="49"><span class="kw">def</span> calc_precision(res,test_labels):</a><a class="sourceLine" id="cb1-50" data-line-number="50">    f_res_open<span class="op">=</span><span class="bu">open</span>(<span class="st">&quot;res.txt&quot;</span>,<span class="st">&quot;a+&quot;</span>)</a><a class="sourceLine" id="cb1-51" data-line-number="51">    precision<span class="op">=</span><span class="dv">0</span></a><a class="sourceLine" id="cb1-52" data-line-number="52">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(res)):</a><a class="sourceLine" id="cb1-53" data-line-number="53">        f_res_open.write(<span class="st">&quot;res:&quot;</span><span class="op">+</span><span class="bu">str</span>(res[i])<span class="op">+</span><span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a><a class="sourceLine" id="cb1-54" data-line-number="54">        f_res_open.write(<span class="st">&quot;test:&quot;</span><span class="op">+</span><span class="bu">str</span>(test_labels[i])<span class="op">+</span><span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a><a class="sourceLine" id="cb1-55" data-line-number="55">        <span class="cf">if</span> res[i]<span class="op">==</span>test_labels[i]:</a><a class="sourceLine" id="cb1-56" data-line-number="56">            precision<span class="op">+=</span><span class="dv">1</span></a><a class="sourceLine" id="cb1-57" data-line-number="57">    <span class="cf">return</span> precision<span class="op">/</span><span class="bu">len</span>(res)</a><a class="sourceLine" id="cb1-58" data-line-number="58"></a><a class="sourceLine" id="cb1-59" data-line-number="59"></a><a class="sourceLine" id="cb1-60" data-line-number="60"><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</a><a class="sourceLine" id="cb1-61" data-line-number="61">    <span class="bu">print</span>(<span class="st">&#39;Start process train data&#39;</span>)</a><a class="sourceLine" id="cb1-62" data-line-number="62">    time_0 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-63" data-line-number="63">    <span class="co"># tl.get_train_set()</span></a><a class="sourceLine" id="cb1-64" data-line-number="64"></a><a class="sourceLine" id="cb1-65" data-line-number="65">    <span class="bu">print</span>(<span class="st">&#39;Start process test data&#39;</span>)</a><a class="sourceLine" id="cb1-66" data-line-number="66">    time_t <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-67" data-line-number="67">    <span class="co"># tl.get_test_set()</span></a><a class="sourceLine" id="cb1-68" data-line-number="68"></a><a class="sourceLine" id="cb1-69" data-line-number="69">    <span class="co"># 读取训练数据集和测试数据集的方法和朴素贝叶斯方法一致</span></a><a class="sourceLine" id="cb1-70" data-line-number="70">    <span class="bu">print</span> (<span class="st">&#39;Start read train data&#39;</span>)</a><a class="sourceLine" id="cb1-71" data-line-number="71">    time_1 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-72" data-line-number="72">    data_map, labels <span class="op">=</span> tl.loadCSVfile(<span class="st">&quot;data.csv&quot;</span>)</a><a class="sourceLine" id="cb1-73" data-line-number="73">    <span class="bu">print</span>(data_map.shape, labels.shape)</a><a class="sourceLine" id="cb1-74" data-line-number="74">    time_2 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-75" data-line-number="75">    <span class="bu">print</span>(<span class="st">&#39;read data train cost &#39;</span>, time_2 <span class="op">-</span> time_1, <span class="st">&#39; seconds&#39;</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</a><a class="sourceLine" id="cb1-76" data-line-number="76"></a><a class="sourceLine" id="cb1-77" data-line-number="77">    <span class="bu">print</span>(<span class="st">&#39;Start read predict data&#39;</span>)</a><a class="sourceLine" id="cb1-78" data-line-number="78">    time_3 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-79" data-line-number="79">    test_data_map, test_labels <span class="op">=</span> tl.loadCSVfile(<span class="st">&quot;dataTest.csv&quot;</span>)</a><a class="sourceLine" id="cb1-80" data-line-number="80">    <span class="bu">print</span>(test_data_map.shape, test_data_map.shape)</a><a class="sourceLine" id="cb1-81" data-line-number="81">    time_4 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-82" data-line-number="82">    <span class="bu">print</span>(<span class="st">&#39;read predict data cost &#39;</span>, time_4 <span class="op">-</span> time_3, <span class="st">&#39; seconds&#39;</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</a><a class="sourceLine" id="cb1-83" data-line-number="83"></a><a class="sourceLine" id="cb1-84" data-line-number="84">    <span class="bu">print</span>(<span class="st">&#39;Start predicting data&#39;</span>)</a><a class="sourceLine" id="cb1-85" data-line-number="85">    time_5 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-86" data-line-number="86">    res, accuracy <span class="op">=</span> knn_all(<span class="dv">3</span>, data_map, labels, test_data_map, test_labels)</a><a class="sourceLine" id="cb1-87" data-line-number="87">    score <span class="op">=</span> calc_precision(res, test_labels)</a><a class="sourceLine" id="cb1-88" data-line-number="88">    time_6 <span class="op">=</span> time.time()</a><a class="sourceLine" id="cb1-89" data-line-number="89">    <span class="bu">print</span>(<span class="st">&#39;read predict data cost &#39;</span>, time_6 <span class="op">-</span> time_5, <span class="st">&#39; seconds&#39;</span>, <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</a><a class="sourceLine" id="cb1-90" data-line-number="90"></a><a class="sourceLine" id="cb1-91" data-line-number="91">    new_ticks <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">30</span>, <span class="dv">30</span>)</a><a class="sourceLine" id="cb1-92" data-line-number="92">    plt.xticks(new_ticks)</a><a class="sourceLine" id="cb1-93" data-line-number="93">    plt.ylim(ymin<span class="op">=</span><span class="fl">0.5</span>, ymax <span class="op">=</span> <span class="dv">1</span>)</a><a class="sourceLine" id="cb1-94" data-line-number="94">    plt.plot(new_ticks, accuracy, <span class="st">&#39;o-&#39;</span>, color<span class="op">=</span><span class="st">&#39;g&#39;</span>)</a><a class="sourceLine" id="cb1-95" data-line-number="95">    plt.xlabel(<span class="st">&quot;x -- 1:70&quot;</span>)</a><a class="sourceLine" id="cb1-96" data-line-number="96">    plt.ylabel(<span class="st">&quot;y&quot;</span>)</a><a class="sourceLine" id="cb1-97" data-line-number="97">    plt.title(<span class="st">u&quot;预测准确率&quot;</span>)</a><a class="sourceLine" id="cb1-98" data-line-number="98">    plt.show()</a><a class="sourceLine" id="cb1-99" data-line-number="99"></a><a class="sourceLine" id="cb1-100" data-line-number="100">    <span class="bu">print</span>(<span class="st">&quot;The accuracy rate is &quot;</span>, score)</a><a class="sourceLine" id="cb1-101" data-line-number="101">    <span class="bu">print</span>(<span class="st">&quot;All data processing cost </span><span class="sc">%s</span><span class="st"> seconds&quot;</span> <span class="op">%</span> (time_6 <span class="op">-</span> time_0))</a></code></pre></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;k 近邻法（k-nearest-neighbor, KNN）是一种基本的分类和回归方法。现在只讨论其分类方面的应用，它不具备明显的学习过程，实际上是利用已知的训练数据集对输入特征向量空间进行划分，并作为其分类的“模型”。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>乡土中国</title>
    <link href="http://suool.net/2018/07/09/from-the-soil-first/"/>
    <id>http://suool.net/2018/07/09/from-the-soil-first/</id>
    <published>2018-07-09T08:28:07.000Z</published>
    <updated>2018-10-29T12:53:08.399Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>上个周看完了费孝通的《乡土中国》，现在摘抄一下其中的标记，后续再写相关的读后感吧。 <a id="more"></a></p><ol type="1"><li>从基层上看，中国社会是乡土性的。</li><li>农业和游牧或者工业不同，它是直接取资于土地的。种地的农民是搬不动的，因为长在土里的庄稼是他们生存的基础，伺候庄家的老农也像是半身插在土里，无法流动。</li><li>直接靠土地来谋生的人是站着在土地上的。以农为生的人，时代定居是常态，迁移是变态。除非天灾战火才有可能会使他们背井离乡，即使这样，死守在土里的可能还是较大的。</li><li>人口的不流动是从人和空间的关系上说的，从人和人在空间的排列关系上说就是孤立和隔膜。孤立和隔膜并不是以个人为单位的，而是以一处住在的集团为单位的，比如村落。</li><li>中国的农民一般是聚村而居，形成数百户人家的村落是积极普遍的；而美国等国家农民则一般是一户人家形成一个单位，因此而形成的精神文明也大不相同。</li><li>乡土社会在地方性的限制下成了生于斯、死于斯的社会，常态的生活是终老于乡。因此，中国的基层社会也就形成了这样一个特色，每个孩子都是在人家眼里看着长大的，在孩子眼里周围的人也是从小看惯的。这就形成了一个对每个人而言都充满了“熟悉”感的社会，没有陌生人的社会。</li><li>社会学里分出两种性质的社会：礼俗社会和法理社会。礼俗社会是没有具体目的，只是因为在一起生长而发生的社会，是有机的团结；法理社会是为了要完成一件任务而结合的社会，是机械的团结。</li><li>乡土社会中的规矩不是法律，规矩是“习”出来的礼俗。</li><li>乡土社会是熟人社会，因此乡土社会的人与人之间也从熟悉得到信任，但这种信任不是出于契约精神的信用，乡土社会的信用不是对契约的重视，而是发生于对一种行为的规矩熟悉到不假思索时的可靠性。</li><li>像是一株植物扎根于土地，在能在后续悠长的生存时间里，从容的摸清每个人都生活，甚至摸清“物”的生活，传统的二十四节气，以及各种农业谚语都是农人对于生存环境熟悉的经验总结。然而这些熟悉里面的认识都是个别的，不具有抽象的普遍原则，在熟悉环境中生长的人也不需要这些原则就能世代安稳的生存，因此，他们也会不去尝试追求更加抽象的笼罩万物的真理。</li><li>但是熟悉个别的事物，总结成生存经验这种生存哲学在一个陌生人的社会是无法应用的。在我们社会的急速变迁中，从乡土社会进入现代社会的过程中，我们在乡土社会中所养成的生活方式处处产生了流弊。陌生人所组成的现代社会是无法用乡土社会的习俗来应对的。于是，“土气”成了贬义词，“乡”也不再是衣锦荣归的去处了。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上个周看完了费孝通的《乡土中国》，现在摘抄一下其中的标记，后续再写相关的读后感吧。
    
    </summary>
    
      <category term="社会学" scheme="http://suool.net/categories/%E7%A4%BE%E4%BC%9A%E5%AD%A6/"/>
    
    
      <category term="书摘" scheme="http://suool.net/tags/%E4%B9%A6%E6%91%98/"/>
    
      <category term="思考" scheme="http://suool.net/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯应用之在手写数字识别的实践</title>
    <link href="http://suool.net/2018/07/08/navie-bayes-handwriting-recognition/"/>
    <id>http://suool.net/2018/07/08/navie-bayes-handwriting-recognition/</id>
    <published>2018-07-08T06:42:24.000Z</published>
    <updated>2018-10-29T12:53:08.545Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><hr><h2 id="引言">引言</h2><p>我们都见过或者用过的一个东西就是输入法的手写键盘，如下面的动图所示，那么输入法是如何识别出我们手写的字迹是什么字的呢？这是一个对人而言非常简单（前提是你写的字体不过于潦草），但是对于程序而言，可能就没有那么简单了，这次我就从一个更简单的角度来试一下，如何去识别手写的数字。 <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="李航《统计学习方法》第四章">[1]</span></a></sup> <a id="more"></a> <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/handinput.gif" alt="手写输入"></p><h2 id="朴素贝叶斯">朴素贝叶斯</h2><p>我们在前面的文章中提到了<a href="https://suool.net/2018/06/21/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF/">贝叶斯定理</a>，公式表示如下 <span class="math display">\[P(y|x)= \frac{P(x,y)}{p(x)} = \frac {P(x|y)P(y)} {P(x)} = \frac {P(x|y)P(y)} {\sum_{y \in Y} P(x|y)P(y)}\]</span> 并从中得出了这样的结论: &gt; 贝叶斯定理可以精确的说明在已知新证据 <span class="math inline">\(x\)</span> 的情况下，我们应该改变多少关于 <span class="math inline">\(y\)</span> 的信念，这个等式中，<span class="math inline">\(P(y)\)</span> 是新证据 <span class="math inline">\(x\)</span> 出现之前我对于 <span class="math inline">\(y\)</span> 的先验信念。 <span class="math inline">\(P(x|y)\)</span> 是在 <span class="math inline">\(y\)</span> 确定的前提下，得到证据 <span class="math inline">\(x\)</span> 的可能性。 <span class="math inline">\(P(y|x)\)</span> 是在考虑新证据后我对于 <span class="math inline">\(y\)</span> 的后验信念。</p><p>而朴素贝叶斯（Naive Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于学习到模型，对于新的输入 <span class="math inline">\(x\)</span> ，利用贝叶斯定理求出后验概率最大的输出 <span class="math inline">\(y\)</span> ，朴素贝叶斯法实现简单，学习和预测的效率也非常高，是一种很常用的分类方法。</p><h3 id="朴素贝叶斯法的学习与分类">朴素贝叶斯法的学习与分类</h3><p>朴素贝叶斯法如何用于分类工作呢？这里用更加数学的语言，可以表述如下： 首先设输入空间 $ R^n $ 为 n 维向量的集合，输出空间为类标记集合 <span class="math inline">\(\mathcal{Y} = \{c_1, c_2, \cdots , c_k\}\)</span>，其中输入为特征向量 <span class="math inline">\(x \in \mathcal{X}\)</span>，输出为类标记（class label） <span class="math inline">\(y \in \mathcal{Y}\)</span> .则 $ P(X, Y) $ 是 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的联合概率分布.</p><p>其次训练数据集表示为: <span class="math display">\[T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N) \}\]</span> 由 $P(X, Y) $ 独立同分布产生.</p><p>最后由朴素贝叶斯法通过上述训练数据集学习到联合概率分布 $P(X, Y) $ ，由条件概率公式可以知道只需要学习到如下的先验概率分布: <span class="math display">\[P(Y = c_k), k=1,2,\cdots, K\]</span> 以及条件概率分布： <span class="math display">\[P(X = x \mid Y = c_k) = P(X^{(1)} = x^{(1)}, \cdots, X^{(n)} = x^{(n)} \mid Y = c_k)\]</span> 二者相乘即可得到联合概率分布。</p><p>但是，事实上，从我们学到的条件概率分布的知识可以知道，$P(X = x Y = c_k) $ 是一个具有指数级数量的参数，一旦训练集稍微具有规模，那么这个概率分布都是无法估计的。而机器学习中，训练集的规模化显然是不可避免的。</p><p>那么现在我们明显已经知道通过学习上述两个概率分布，可以得到我们想要的联合概率分布，从而得到训练模型，然而其中条件概率分布计算的不可行性却让我们止步不前。</p><p>在这样一个尴尬的时候，朴素贝叶斯法站出来为我们解围了，它提出了一个非常强的假设，就是假设条件概率分布是特征条件独立的，因此朴素贝叶斯，也就朴素在这里:他的假设太强，强到改变了理论上计算的规则。</p><p>具体的，条件独立性假设是： <span class="math display">\[\begin{align}P(X = x \mid Y = c_k) &amp; = P(X^{(1)} = x^{(1)}, \cdots, X^{(n)} = x^{(n)} \mid Y = c_k) \\&amp; = \prod^{n}_{j=1}P(X^{(j)} = x^{(j)} \mid Y=c_k) \tag{1}\end{align}\]</span> 这个假设等于是说用于分类的特征在类别确定的条件下都是条件独立的，这一假设使得条件概率的计算变得异常简单，但是显然也牺牲了一定的准确率，因为一般情况下特征并不都是独立的而是有关联的。</p><p>通过上述学习到模型，就可以计算后验概率分布 <span class="math inline">\(P(Y = c_k \mid X = x)\)</span>，将后验概率最大的类别作为要预测的样本 <span class="math inline">\(x\)</span> 的输出。后验概率的计算依据贝叶斯定理进行： <span class="math display">\[P(Y = c_k \mid X = x) = \frac{P(X=x\mid Y=c_k) P(Y=c_k)}{\sum_k P(X=x\mid Y=c_k) P(Y=c_k)} \tag{2}\]</span></p><p>将 2 式代入 1 式，得到： <span class="math display">\[P(Y = c_k \mid X = x) = \frac{P(Y=c_k) \prod_{j}P(X^{(j)} = x^{(j)} \mid Y=c_k)}{\sum_k P(Y=c_k) \prod_{j}P(X^{(j)} = x^{(j)} \mid Y=c_k)}\]</span></p><p>所以，贝叶斯分类器可以表示为： <span class="math display">\[y=f(x)= \arg \max_{c_k} \frac{P(Y=c_k) \prod_{j}P(X^{(j)} = x^{(j)} \mid Y=c_k)}{\sum_k P(Y=c_k) \prod_{j}P(X^{(j)} = x^{(j)} \mid Y=c_k)}\]</span></p><p>上式因为分母对所有 <span class="math inline">\(c_k\)</span> 都是相同的，所以： <span class="math display">\[y = \arg \max_{c_k} P(Y=c_k) \prod_{j}P(X^{(j)} = x^{(j)} \mid Y=c_k)\]</span></p><p>这就是朴素贝叶斯分类器的最简形式，从这个式子可以看出来，朴素贝叶斯分类器本质上是根据先验概率及条件概率求最大化的后验概率，从而推断类别。</p><h3 id="朴素贝叶斯法的参数估计">朴素贝叶斯法的参数估计</h3><h4 id="极大似然估计"><strong>极大似然估计</strong></h4><p>在上面所说的朴素贝叶斯法中，训练学习意味着估计先验概率 <span class="math inline">\(P(Y=c_k)\)</span> 和条件概率 <span class="math inline">\(P(X^{(j)} = x^{(j)} \mid Y=c_k)\)</span>，这可以应用极大似然估计来估计相应的概率，其中先验概率的极大似然估计是：</p><p><span class="math display">\[P(Y=c_k) = \frac{\sum^{N}_{i=1} I(y_i=c_k)}{N} ，k = 1,2,\cdots,K\]</span> 设第 j 个特征 <span class="math inline">\(x^{j}\)</span> 可能的取值的集合为 <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/0EE6F321-E6D7-495F-BEDD-0CA48526F6EB.png" alt="0EE6F321-E6D7-495F-BEDD-0CA48526F6EB">，则条件概率的极大似然估计为：</p><p><span class="math display">\[P(X^{(j)} = a_{jl} \mid Y=c_k) = \frac{\sum^{N}_{i=1} I (x_i^{(j)} = a_{jl}, y_i = c_k)}{\sum^{N}_{i=1}I (y_i=c_k)} \tag{3}\]</span></p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/D5690E7F-DC11-4F6C-8842-77972CDFEBE8.png" alt="D5690E7F-DC11-4F6C-8842-77972CDFEBE8"><figcaption>D5690E7F-DC11-4F6C-8842-77972CDFEBE8</figcaption></figure><p>####<strong>贝叶斯估计</strong></p><p>因为用极大似然估计可能会出现所要估计的概率值为 0 的情况，这就会导致后续的计算出现错误，使得分类出现偏差，解决这个问题的方法就是使用贝叶斯估计，即是平滑处理估计结果，条件概率的贝叶斯估计是： <span class="math display">\[P(X^{(j)} = a_{jl} \mid Y=c_k) = \frac{\sum^{N}_{i=1} I (x_i^{(j)} = a_{jl}, y_i = c_k) + \lambda}{\sum^{N}_{i=1}I (y_i=c_k) + S_j\lambda} \tag{3}\]</span></p><p>其中 <span class="math inline">\(\lambda \geq 0\)</span>，等价于给随机变量的各个取值的频数加上一个正数 <span class="math inline">\(\lambda &gt; 0\)</span> 常取 <span class="math inline">\(\lambda = 1\)</span>，称为<strong>拉普拉斯平滑</strong>。</p><h2 id="实战朴素贝叶斯">实战朴素贝叶斯</h2><p>有了上面的基础，那么处理手写数字识别上面就变得很简单了，整个处理的步骤分为： - 图片预处理 - 图片数据化 - 模型训练 - 模型测试</p><h3 id="图片预处理">图片预处理</h3><p>这里我们取现成的 mnist 数据集 <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="[THE MNIST DATABASE](http://yann.lecun.com/exdb/mnist/)">[2]</span></a></sup> ，首先数据集下载下来的是特殊格式的压缩包，其中训练集的图片包和标签包格式如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/traintype.png" alt="压缩格式"></p><p>可以看出在 train-images.idx3-ubyte 中，第一个数为 32 位的整数（魔数，图片类型的数），第二个数为32位的整数（图片的个数），第三和第四个也是 32 位的整数（分别代表图片的行数和列数），接下来的都是一个字节的无符号数（即像素，值域为0~255），因此，我们只需要依次获取魔数和图片的个数，然后获取图片的长和宽，最后逐个按照图片大小的像素读取就可以得到一张张的图片内容了。标签数据集及测试数据集的的数据读取都是一样的原理。</p><p>读取训练图片集并将图片存储成图片，读取标签集的代码实现代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_image</span><span class="params">(filename)</span>:</span></span><br><span class="line">    f = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    buf = f.read()</span><br><span class="line">    f.close()</span><br><span class="line">    // 开始读取 魔数、图片数目、图片行数、列数</span><br><span class="line">    magic, images, rows, columns = struct.unpack_from(<span class="string">'&gt;IIII'</span>, buf, index)</span><br><span class="line">    index += struct.calcsize(<span class="string">'&gt;IIII'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(images):</span><br><span class="line">        <span class="comment"># 逐个读取图片，每个图片字节数为 行数X列数</span></span><br><span class="line">        image = Image.new(<span class="string">'L'</span>, (columns, rows))</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> range(rows):</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> range(columns):</span><br><span class="line">                <span class="comment"># 读取并填充图片的像素值，每个像素值为一个字节</span></span><br><span class="line">                image.putpixel((y, x), int(struct.unpack_from(<span class="string">'&gt;B'</span>, buf, index)[<span class="number">0</span>]))</span><br><span class="line">                index += struct.calcsize(<span class="string">'&gt;B'</span>)</span><br><span class="line">        print(<span class="string">'save '</span> + str(i) + <span class="string">'image'</span>)</span><br><span class="line">        image.save(<span class="string">'train/'</span> + str(i) + <span class="string">'.png'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_label</span><span class="params">(filename, saveFilename)</span>:</span></span><br><span class="line">    f = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    buf = f.read()</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="comment"># 开始读取 魔数及标签数目</span></span><br><span class="line">    magic, labels = struct.unpack_from(<span class="string">'&gt;II'</span>, buf, index)</span><br><span class="line">    index += struct.calcsize(<span class="string">'&gt;II'</span>)</span><br><span class="line">    labelArr = [<span class="number">0</span>] * labels</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(labels):</span><br><span class="line">        <span class="comment"># 一个标签一个字节</span></span><br><span class="line">        labelArr[x] = int(struct.unpack_from(<span class="string">'&gt;B'</span>, buf, index)[<span class="number">0</span>])</span><br><span class="line">        index += struct.calcsize(<span class="string">'&gt;B'</span>)</span><br><span class="line">    save = open(saveFilename, <span class="string">'w'</span>)</span><br><span class="line">    save.write(<span class="string">','</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> labelArr]))</span><br><span class="line">    save.write(<span class="string">'\n'</span>)</span><br><span class="line">    save.close()</span><br><span class="line">    print(<span class="string">'save labels success'</span>)</span><br><span class="line">    <span class="keyword">return</span> labelArr</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    read_image(<span class="string">'train-images.idx3-ubyte'</span>)</span><br><span class="line">    read_label(<span class="string">'train-labels.idx1-ubyte'</span>, <span class="string">'train/label.txt'</span>)</span><br></pre></td></tr></table></figure></p><p>读取到图片之后，存储的结果如下：</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/dataprocess-1.png" alt="数据解压"><figcaption>数据解压</figcaption></figure><p>###图片数据化 为了将图片变成更易为计算机接受的形式，这里需要将图片二值化，即是只包含0和1的图片表现形式，即是下面这样的矩阵，其方法是超过一定像素值的点标记为 1，否则为 0： <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/FE2F3FF5-7847-4F83-9C62-400196BB55FF.png" alt="FE2F3FF5-7847-4F83-9C62-400196BB55FF"></p><p>可以隐约看出一个 5 的形状。</p><p>这里为了方便继续处理图片特征，将这个 28 *28 的矩阵进行 reshape 操作，将一幅图展开为行向量。因此整个训练集（）60000张图片）就变成了一个大小为 60000×784 的矩阵，之后尽量进行矩阵操作。</p><p>同时为了方便标记，将每行向量表示的数字写在最后一列，因此整个矩阵的大小为 60000×785。</p><p>并为了后续数据操作的方便，将这个矩阵存在本地的CSV文件中，代码及结果如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">28</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_set</span><span class="params">()</span>:</span></span><br><span class="line">    f = open(<span class="string">'data.csv'</span>, <span class="string">'wb'</span>)</span><br><span class="line">    category = MR.read_label(<span class="string">'train-labels.idx1-ubyte'</span>, <span class="string">'train/label.txt'</span>)</span><br><span class="line"></span><br><span class="line">    file_names = os.listdir(<span class="string">r"./train/"</span>, )</span><br><span class="line">    train_picture = np.zeros([len(file_names)<span class="number">-1</span>, N ** <span class="number">2</span> + <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 遍历文件，转为向量存储</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> range(len(file_names)<span class="number">-1</span>):</span><br><span class="line">        img_num = io.imread(<span class="string">'./train/%d.png'</span> % (file))</span><br><span class="line">        rows, cols = img_num.shape</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(rows):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(cols):</span><br><span class="line">                <span class="keyword">if</span> img_num[i, j] &lt; <span class="number">100</span>:</span><br><span class="line">                    img_num[i, j] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    img_num[i, j] = <span class="number">1</span></span><br><span class="line">        train_picture[file, <span class="number">0</span>:N ** <span class="number">2</span>] = img_num.reshape(N ** <span class="number">2</span>)</span><br><span class="line">        train_picture[file, N ** <span class="number">2</span>] = category[file]</span><br><span class="line">        print(<span class="string">"完成处理第%d张图片"</span> % (file+<span class="number">1</span>))</span><br><span class="line">    np.savetxt(f,train_picture,fmt=<span class="string">'%d'</span>,delimiter=<span class="string">','</span>, newline=<span class="string">'\n'</span>, header=<span class="string">''</span>, footer=<span class="string">''</span>)</span><br><span class="line">    f.close()</span><br><span class="line">    time_e = time.time()</span><br><span class="line">    print(<span class="string">'process data train cost '</span>, time_e - time_0, <span class="string">' seconds'</span>, <span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">return</span> train_picture</span><br></pre></td></tr></table></figure></p><p>结果为，红色框内即是该行向量的标签： <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/EAFE6D9C-EC29-4806-80D2-055FCE2CFFB5.png" alt="EAFE6D9C-EC29-4806-80D2-055FCE2CFFB5"></p><p>处理测试数据也是一样的原理。</p><h3 id="模型训练">模型训练</h3><p>从第一部分我们可以了解到，要求后验概率的本质是求在类别为 j 的条件下，样本 x 的第 i 个特征出现的条件概率的，将所有特征的概率与该类别的先验概率作连乘即得到后验概率。</p><p>因此，重点是计算类别的先验概率和在类别为 j 的条件下，样本 x 的第 i 个特征出现的条件概率，这也就是我们要训练的模型，代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Train</span><span class="params">()</span>:</span></span><br><span class="line">    conditional_probability = np.zeros((class_num, feature_len, <span class="number">2</span>))   <span class="comment"># 条件概率</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算先验概率及条件概率</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labels)):</span><br><span class="line">        img = data_map[i, :]</span><br><span class="line">        label = labels[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(feature_len):</span><br><span class="line">            conditional_probability[label][j][img[j]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将概率归到[1.1001]</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(class_num):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(feature_len):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 经过二值化后图像只有0，1两种取值</span></span><br><span class="line">            pix_0 = conditional_probability[i][j][<span class="number">0</span>]</span><br><span class="line">            pix_1 = conditional_probability[i][j][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算0，1像素点对应的条件概率</span></span><br><span class="line">            probalility_0 = (float(pix_0)/float(pix_0+pix_1))*<span class="number">1000</span> + <span class="number">1</span></span><br><span class="line">            probalility_1 = (float(pix_1)/float(pix_0+pix_1))*<span class="number">1000</span> + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            conditional_probability[i][j][<span class="number">0</span>] = probalility_0</span><br><span class="line">            conditional_probability[i][j][<span class="number">1</span>] = probalility_1</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> conditional_probability</span><br></pre></td></tr></table></figure></p><p>其中部分说明如下： - 由于 Python 浮点数精度的原因，784个浮点数联乘后结果变为 Inf，而 Python 中 int 可以无限相乘的，因此可以利用python int 数据类型的特性对先验概率与条件概率进行一些改造。</p><ul><li>先验概率： 由于先验概率分母都是 N，因此不用除于 N，直接用分子即可。</li><li>条件概率： 条件概率公式如上说明济代码所示，我们得到概率后再乘以1000000 （最小的可能性为1/784，同时需要尽量保存概率精度，这里保存到白万分之一，因此乘以1000000），将概率映射到[0,1000000]中，但是为防止出现概率值为0的情况，人为的加上1，使概率映射到[1,1000001]中。<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="李航《统计学习方on实现朴素贝叶斯分类器（MNIST数据集）](https://blog.csdn.net/wds2006sdo/article/details/51967839)">[3]</span></a></sup></li></ul><h3 id="模型预测">模型预测</h3><p>模型预测的方法就是根据上面训练出来的朴素贝叶斯模型，对任一个新的样本 x ，分别计算它是类别 j 的条件下的后验概率，取最大后验概率的类别即可。</p><p>代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算概率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_probability</span><span class="params">(img, label)</span>:</span></span><br><span class="line">    probability = int(prior_probability[label])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(img)):</span><br><span class="line">        probability *= int(conditional_probability[label][i][img[i]])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> probability</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Predict</span><span class="params">(testset, test_labels)</span>:</span></span><br><span class="line">    predict = []</span><br><span class="line">    accuracy = []</span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    rows, cols = testset.shape</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(rows):</span><br><span class="line">        <span class="comment"># 图像二值化</span></span><br><span class="line">        img = testset[row, :]</span><br><span class="line"></span><br><span class="line">        max_label = <span class="number">0</span></span><br><span class="line">        max_probability = calculate_probability(img, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">            probability = calculate_probability(img, j)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> max_probability &lt; probability:</span><br><span class="line">                max_label = j</span><br><span class="line">                max_probability = probability</span><br><span class="line">        predict.append(max_label)</span><br><span class="line">        <span class="keyword">if</span> max_label == test_labels[row]:</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (row+<span class="number">1</span>) % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            accuracy.append(float(right)/(row+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> float(right)/len(test_labels), np.array(predict), accuracy</span><br></pre></td></tr></table></figure></p><p>整个代码运行的结果如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/15310514214661.jpg" alt="-w455"> 准确率曲线如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/07/08/8EAA5A09-8E40-45F3-85EC-D20F83C5C844.png" alt="8EAA5A09-8E40-45F3-85EC-D20F83C5"></p><h3 id="其他说明">其他说明</h3><ul><li>本次实验将图像展开、对单个像素独立判断，损失了图像的空间信息，而这种空间信息正是我们人眼识别图像的关键。所以最终的准确率受此影响。</li><li>测试数据的准确率为84.47%，还是非常不错的（共有10个分类，理论上随机猜测的准确率只有10%）。能使准确率达到84.15%的原因主要是：(1) MNIST数据集被预先处理过。通过上面的示例图片可以看出，MNIST中的图片较为纯净，没有噪声干扰，非常清晰。所以实验中可以直接进行二值化。(2) MNIST数据集中，图片中的数字总是在中心位置，大小合适，比较饱满。这一点保留了部分的空间信息。</li><li>本例中的二值化过程是直接将超过一定像素阀值值置1，如果采用更合理的阈值(比如取当前图片最大像素阀值为 100 )进行二值化操作，进行训练、测试过程，最终的准确率可能会提升。 <sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="[用朴素贝叶斯法对MNIST数据集分类](http://geyao1995.com/%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%AF%B9MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB/)">[4]</span></a></sup></li></ul><p>以上就是这次朴素贝叶斯法的理论及实战啦。</p><p>下次是 K 近邻算法在手写识别上的应用。</p><h2 id="reference">Reference</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">李航《统计学习方法》第四章<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">THE MNIST DATABASE</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">李航《统计学习方on实现朴素贝叶斯分类器（MNIST数据集）](https://blog.csdn.net/wds2006sdo/article/details/51967839)<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://geyao1995.com/%E7%94%A8%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%E5%AF%B9MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB/" target="_blank" rel="noopener">用朴素贝叶斯法对MNIST数据集分类</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;我们都见过或者用过的一个东西就是输入法的手写键盘，如下面的动图所示，那么输入法是如何识别出我们手写的字迹是什么字的呢？这是一个对人而言非常简单（前提是你写的字体不过于潦草），但是对于程序而言，可能就没有那么简单了，这次我就从一个更简单的角度来试一下，如何去识别手写的数字。 &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top-right hint--error hint--large&quot; data-aria-label=&quot;李航《统计学习方法》第四章&quot;&gt;[1]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>最小二乘法矩阵表示及非线性响应</title>
    <link href="http://suool.net/2018/06/26/Least-Squares-with-Matrix-Nolinear/"/>
    <id>http://suool.net/2018/06/26/Least-Squares-with-Matrix-Nolinear/</id>
    <published>2018-06-26T05:43:19.000Z</published>
    <updated>2018-10-29T12:53:08.397Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>早上上班的路上读《社会心理学》里面有一段话，觉得不错，摘在这里。</p><blockquote><p><strong>我们人类总是有一种不可抑制的冲动，想要解释行为，对其归因，以使其变得次序井然．具有可预见性，使一切尽在掌握之中。</strong>你我对于类似的情境却可能表现出截然不同的反应，这是因为我们的想法不同。我们对朋友的责难做何反应，取决于我们对其所做的解释，取决于我们是把它归咎于朋友的敌意行为，还是归结于他糟糕的心情。 从某种角度来说，我们都是天生的科学家。我们解释着他人的行为，通常足够快也足够准确，以适应我们日常生活的需要。当他人的行为具有一致性而且与众不同时，我们会把其行为归因于他们的人格。例如。如果你发现一个人说话总是对人冷嘲热讽．你可能就会推断此人秉性不良，然后便设法尽量避免与他的接触。</p></blockquote><p>当然这是指更大范围内的人类心理，在对数据和模型的痴迷上，人类的欲望显然也是强烈的，人类总是想尽一切的办法打破现有的桎梏，在创造了更多的不确定性之后，期望通过对数据的把控和预测以看到更确切的未来。 <a id="more"></a> 这次我们来从矩阵和向量的角度来解释最小二乘法及非线性的模拟建模。</p><h2 id="ls-的矩阵推导">LS 的矩阵推导</h2><p>继续使用上一次的方程式并将其改写成矩阵的形式如下：</p><p><span class="math display">\[f(x_n;w_0,w_1) = w_0 + w_1x = w^Tx \tag{2.1} \]</span></p><p>代入最小二乘损失函数，得到结果为：</p><p><span class="math display">\[\begin{align}\mathcal{L} &amp;= \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_n(t_n,f(x_n;w_0,w_1 )) \\&amp;= \frac{1}{N} \sum_{n=1}^{N} (t_n-w^tx_n)^2 \tag{2.2} \\&amp; = \frac{1}{N} \sum_{n=1}^{N} (t-Xw)^T(t-Xw) \tag{2.3}\end{align}\]</span></p><p>式 2.2 到式 2.3 的证明忽略。</p><p>其中 <span class="math inline">\(X\)</span> 、<span class="math inline">\(W\)</span> 和 <span class="math inline">\(T\)</span> 为：</p><p><span class="math display">\[ X = \begin{bmatrix} x_1^T  \\ x_2^T  \\ x_3^T   \\ \vdots \\  x_n^T  \\ \end{bmatrix} =  \begin{bmatrix} 1 &amp; x_1  \\ 1 &amp;  x_2 \\ 1 &amp; x_3  \\ \vdots &amp; \vdots\\ 1 &amp; x_n  \\ \end{bmatrix} \\ w = \begin{bmatrix} w_0^T  \\ w_1^T \end{bmatrix} \quad \\ t = \begin{bmatrix} t_1^T  \\ t_2^T  \\ t_3^T   \\ \vdots \\  t_n^T  \\ \end{bmatrix}\]</span></p><p>对上述 2.3 展开，得到下面的表达式：</p><p><span class="math display">\[\begin{align}\mathcal{L} &amp;= \frac{1}{N} w^TX^TXw - \frac {2} {N}w^TX^Tt + \frac {1}{N} t^Tt \\&amp; = \frac{1}{N} (w^TX^TXw - 2 w^TX^Tt + t^Tt)\end{align}\]</span></p><p>为了得到最小值，需要得到 <span class="math inline">\(\mathcal{L}\)</span>的拐点（极小值）一致的向量<span class="math inline">\(w\)</span>的值，这里一样是求 <span class="math inline">\(\mathcal{L}\)</span> 关于 <span class="math inline">\(w\)</span> 的偏导数，并令其为 0，可以代入上述的矩阵进行求解，也可以使用一些恒等式来直接化简:</p><p><span class="math display">\[% inner array of minimum values 内层&quot;最小值&quot;数组\begin{array}{c|c}\hlinef(w) &amp; \frac {\partial f} {\partial w} \\\hlinew^Tx &amp; x \\x^t=Tw &amp; x \\w^Tw &amp; 2w \\w^TCw &amp; 2Cw \\\hline\end{array}\]</span></p><p>得到的表达式如下：</p><p><span class="math display">\[\frac {\partial \mathcal{L}} {\partial w} = \frac {2} {N} X^TXw - \frac {2} {N}X^Tt = 0 \]</span></p><p><span class="math display">\[X^Xw = X^Tt \tag{2.4}\]</span></p><p>从而得到使损失最小的<span class="math inline">\(w\)</span>值，<span class="math inline">\(\hat w\)</span> 的矩阵公式为：</p><p><span class="math display">\[\hat w = (X^TX)^{-1}X^Tt \tag{2.5}\]</span></p><p>根据此公式解出的值与上次用标量形式解出的是一样的。</p><h2 id="线性模型的非线性响应">线性模型的非线性响应</h2><p>前面我们所假设的是拟合函数是一次函数，但是可以明显的看出拟合效果并不好，所以需要往更高次的多项式延伸，假如是二次的话，那么可以表示为：</p><p><span class="math display">\[f(x;w) = w^Tx = w_0+w_1x+w_2x^2\]</span></p><p>更一般的，扩展 <span class="math inline">\(x\)</span> 的幂次到任意任意阶的多项式函数，对于一个 K 阶多项式，可以扩展数据矩阵为：</p><p><span class="math display">\[t = \begin{bmatrix} x_1^0 &amp; x_1^1 &amp; x_1^2 &amp; \cdots &amp; x_1^k \\ x_2^0 &amp; x_2^1 &amp; x_2^2 &amp; \cdots &amp; x_2^k\\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\x_n^0 &amp; x_n^1 &amp; x_n^2 &amp; \cdots &amp; x_n^k \\\end{bmatrix}\]</span></p><p>函数表达为更一般的形式：</p><p><span class="math display">\[f(x;w)= \sum^K_{k=0} w_kx^k\]</span></p><p>其解一样适用于式 <span class="math inline">\(2.5\)</span></p><p>可以求得其八阶线性拟合结果如下：</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/07/03/FE170A95-A609-4B5D-AE3C-4090B7B63FA2.png" alt="FE170A95-A609-4B5D-AE3C-4090B7B63FA2"><figcaption>FE170A95-A609-4B5D-AE3C-4090B7B63FA2</figcaption></figure><p>代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_cord = []</span><br><span class="line">y_cord = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawScatterDiagram</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    fr=open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr=line.split(<span class="string">','</span>)</span><br><span class="line">        x_cord.append(float(lineArr[<span class="number">0</span>]))</span><br><span class="line">        y_cord.append(float(lineArr[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># plt.scatter(x_cord,y_cord,s=30,c='red',marker='o', alpha=0.7,label='比赛成绩 ')</span></span><br><span class="line">    <span class="comment"># plt.xlabel("year")</span></span><br><span class="line">    <span class="comment"># plt.ylabel("time")</span></span><br><span class="line">    <span class="comment"># plt.title("result of game")</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">noLinearMoreTimesCalculate</span><span class="params">()</span>:</span></span><br><span class="line">    x_mat = np.ones((len(x_cord),<span class="number">1</span>))</span><br><span class="line">    y_mat = np.mat(y_cord).T</span><br><span class="line">    test = np.array(x_cord)</span><br><span class="line">    test = (test - <span class="number">1896</span>)/<span class="number">4.0</span>  // 数据预处理，缩小化</span><br><span class="line">    <span class="comment"># x_mat = np.mat(np.c_[x_zeros,test])</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">9</span>):</span><br><span class="line">        x_temp = test**index</span><br><span class="line">        x_mat = np.mat(np.c_[x_mat, x_temp])</span><br><span class="line">    print(x_mat)</span><br><span class="line">    w_mat = ((x_mat.T * x_mat).I*x_mat.T*y_mat)[::<span class="number">-1</span>]</span><br><span class="line">    w_mat = w_mat.T</span><br><span class="line">    c = np.squeeze([i <span class="keyword">for</span> i <span class="keyword">in</span> w_mat])</span><br><span class="line">    print(c)</span><br><span class="line">    func = np.poly1d(c)</span><br><span class="line">    x_mLo = np.linspace(<span class="number">0</span>, <span class="number">29</span>, <span class="number">112</span>)</span><br><span class="line"></span><br><span class="line">    y_mLo = func(x_mLo)</span><br><span class="line"></span><br><span class="line">    plt.scatter(test,y_cord,s=<span class="number">30</span>,c=<span class="string">'red'</span>,marker=<span class="string">'o'</span>, alpha=<span class="number">0.7</span>,label=<span class="string">'比赛成绩 '</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"year"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"time"</span>)</span><br><span class="line">    plt.title(<span class="string">"result of game"</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot(x_mLo,y_mLo, c=<span class="string">"yellow"</span>,label=<span class="string">'线性拟合-8阶'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    drawScatterDiagram(<span class="string">"olympic100m.txt"</span>)</span><br><span class="line">    <span class="comment"># linearCalculate()</span></span><br><span class="line">    <span class="comment"># noLinearCalculate()</span></span><br><span class="line">    noLinearMoreTimesCalculate()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="过拟合与验证">过拟合与验证</h2><p>从上次的一阶线性拟合到现在的八阶拟合，从直观上看，八阶的拟合效果比一阶的明显好很多，但是我们做拟合的根本目的是为了做预测，不是纯粹为了拟合而拟合，因为理论上说，一般对于 n 个点，可以拟合 n-1 次多项式以便完全通过这些点。但是这样子的拟合结果对于预测而言可能是极其差的。</p><p>那么如何衡量呢? 和拟合是一样的思想，看模型在泛化问题时候的表现，即是在预测验证数据上的表现，如果其损失很小，那么可以认为这个模型的预测能力不错。</p><h3 id="验证数据">验证数据</h3><p>克服过拟合问题的一般方法是使用第二个数据集，即是验证集。用其来验证模型的预测性能，验证集可以是单独提供的，也可以从原始训练集中拿出来的。</p><p>从验证集计算的损失对于验证集数据的选择敏感，如果验证集非常小，那就更加困难了，而交叉验证是一个有效使用现有数据集的方法。</p><p>一般是 K 折交叉验证，当 K = N 的时候，及 K 恰好是等于数据集中的可观测数据的数量，每个观测数据依次拿出用作测试其他 N-1 个对象训练得到的模型，其又叫 留一交叉验证 （Leave-One-Out Cross Validation, LOOCV）,对于 LOOCV 的均方验证为：</p><p><span class="math display">\[\mathcal{L}^{CV} = \frac {1}{N} \sum^N_{n=1}(t_n-\hat{w}^T_{-n}x_n)^2\]</span></p><p>其中 <span class="math inline">\(\hat{w}_{-n}\)</span> 是除去第 n 个训练样例的参数估计值。</p><h3 id="应用实例及代码实现">应用实例及代码实现</h3><p>首先定义一个带噪声的函数，并用其生成 100 个随机数据点作为训练数据，生成 1000 个随机数据点作为测试数据，生成数据点的 <span class="math inline">\(x\)</span> 范围是 [-5,5], 表达式如下，其中噪声<span class="math inline">\(\mathcal {N}\)</span> 符合正态分布 <span class="math display">\[y = 5 \times x^3 - x^2 + x + 100 \times \mathcal N \\\mathcal N(x \mid \mu,\sigma^2) = \frac {1}{\sigma \sqrt {2 \pi}} \exp \{- \frac {1}{2\sigma^2}(x-\mu)^2\}\]</span></p><p>使用如上的矩阵式，对数据进行不同阶数的拟合，得到的结果如下:</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/07/03/51A2EEB0-A96E-44EC-BFC9-10C5C196DC89.png" alt="51A2EEB0-A96E-44EC-BFC9-10C5C196D"><figcaption>51A2EEB0-A96E-44EC-BFC9-10C5C196D</figcaption></figure><p>然后，分别对比训练损失、测试损失及使用 LOOCV 方法拟合及其损失随着阶数增高而变化的曲线如下图所示:</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/07/03/274FF482-8C88-4A58-B39D-3231ADBBFAF4.png" alt="274FF482-8C88-4A58-B39D-3231ADBBFAF4"><figcaption>274FF482-8C88-4A58-B39D-3231ADBBFAF4</figcaption></figure><p>可以从图中看出，在 3 阶的时候，测试损失和 LOOCV 损失达到最小，且 3 阶拟合之后训练损失不明显，但是测试损失明显增高，即产生了过拟合的现象。</p><p>实现代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.random.randint(<span class="number">-5000</span>,<span class="number">5000</span>, size=[<span class="number">100</span>,<span class="number">1</span>]) / <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line">t = <span class="number">5</span>*(x**<span class="number">3</span>)  - x**<span class="number">2</span> + x + <span class="number">100</span>*np.random.randn(<span class="number">100</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">testx = np.linspace(<span class="number">-5</span>,<span class="number">5</span>,<span class="number">1001</span>)</span><br><span class="line"></span><br><span class="line">testt = <span class="number">5</span>*(testx**<span class="number">3</span>) - testx**<span class="number">2</span> + testx + <span class="number">100</span>*np.random.randn(<span class="number">1001</span>, <span class="number">1</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(x, t, s=<span class="number">35</span>, c=<span class="string">"red"</span>, marker=<span class="string">'o'</span>, alpha=<span class="number">0.9</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">plt.title(<span class="string">"points of train"</span>)</span><br><span class="line"></span><br><span class="line">x_mat = np.ones((len(x),<span class="number">1</span>))</span><br><span class="line">y_mat = np.mat(t)</span><br><span class="line"></span><br><span class="line">color = [<span class="string">""</span>,<span class="string">""</span>,<span class="string">""</span>]</span><br><span class="line"></span><br><span class="line">degrees = np.arange(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">trainLoss = []</span><br><span class="line">predictLoss = []</span><br><span class="line">LOOCVLoss = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainAndGetLoss</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> degree <span class="keyword">in</span> degrees:</span><br><span class="line">        <span class="keyword">global</span> x_mat</span><br><span class="line">        <span class="keyword">global</span> y_mat</span><br><span class="line">        tempX = x_mat</span><br><span class="line">        tempY = y_mat</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, degree):</span><br><span class="line">            x_temp = x ** index</span><br><span class="line">            tempX = np.mat(np.c_[tempX, x_temp])</span><br><span class="line">        <span class="comment"># print(tempX)</span></span><br><span class="line">        print(<span class="string">"计算"</span>)</span><br><span class="line">        print(type(tempX.T*tempX))</span><br><span class="line">        w_mat = ((tempX.T * tempX).I * tempX.T * tempY)[::<span class="number">-1</span>]</span><br><span class="line">        w_mat = w_mat.T</span><br><span class="line">        c = np.squeeze([i <span class="keyword">for</span> i <span class="keyword">in</span> w_mat])</span><br><span class="line">        print(c)</span><br><span class="line">        func = np.poly1d(c)</span><br><span class="line">        <span class="keyword">if</span> degree ==<span class="number">2</span> <span class="keyword">or</span> degree == <span class="number">3</span> <span class="keyword">or</span> degree == <span class="number">5</span> :</span><br><span class="line">            x_mLo = np.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">100</span>)</span><br><span class="line">            y_mLo = func(x_mLo)</span><br><span class="line">            plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            plt.plot(x_mLo, y_mLo, linewidth=<span class="number">2</span>, c=np.random.rand(<span class="number">3</span>,), label=<span class="string">'线性拟合-%d阶'</span> % (degree - <span class="number">1</span>))</span><br><span class="line">            plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">        <span class="comment">#  训练数据损失</span></span><br><span class="line">        y_train = func(x)</span><br><span class="line">        print(np.mean((y_train-t)**<span class="number">2</span>), type(y_train), len(y_train))</span><br><span class="line">        trainLoss.append(np.mean((y_train-t)**<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 预测数据损失</span></span><br><span class="line">        y_predict = func(testx)</span><br><span class="line">        print(np.mean((y_predict-testt)**<span class="number">2</span>), type(y_predict), len(y_predict))</span><br><span class="line">        predictLoss.append(np.mean((y_predict-testt)**<span class="number">2</span>))</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    new_ticks = np.linspace(<span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">    plt.xticks(new_ticks)</span><br><span class="line">    plt.plot(new_ticks, trainLoss, <span class="string">'o-'</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">    plt.title(<span class="string">"训练数据损失"</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">    plt.xticks(new_ticks)</span><br><span class="line">    plt.plot(new_ticks, predictLoss, <span class="string">'o-'</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">    plt.title(<span class="string">"预测数据损失"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainLOOCVAndGetLoss</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> degree <span class="keyword">in</span> degrees:</span><br><span class="line">        trainLoocv = []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">            <span class="comment"># 准备训练数据</span></span><br><span class="line">            index_x = (index)</span><br><span class="line">            new_x = np.delete(x, index_x)</span><br><span class="line">            new_y = np.delete(t, index_x)</span><br><span class="line">            tempX = np.ones([<span class="number">99</span>, <span class="number">1</span>])</span><br><span class="line">            tempY = np.mat(new_y).T</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, degree):</span><br><span class="line">                x_temp = new_x ** index</span><br><span class="line">                tempX = np.mat(np.c_[tempX, x_temp])</span><br><span class="line">            w_mat = ((tempX.T * tempX).I * tempX.T * tempY)[::<span class="number">-1</span>]</span><br><span class="line">            w_mat = w_mat.T</span><br><span class="line">            c = np.squeeze([i <span class="keyword">for</span> i <span class="keyword">in</span> w_mat])</span><br><span class="line">            func = np.poly1d(c)</span><br><span class="line">            <span class="keyword">if</span> index_x == <span class="number">0</span>:</span><br><span class="line">                print((func(x[index,<span class="number">0</span>])-t[index,<span class="number">0</span>])**<span class="number">2</span>)</span><br><span class="line">            trainLoocv.append((func(x[index,<span class="number">0</span>])-t[index,<span class="number">0</span>])**<span class="number">2</span>)</span><br><span class="line">        LOOCVLoss.append(np.mean(trainLoocv))</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">        new_ticks = np.linspace(<span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">        plt.xticks(new_ticks)</span><br><span class="line">        plt.plot(new_ticks, predictLoss, <span class="string">'o-'</span>, color=<span class="string">'g'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">        plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">        plt.title(<span class="string">"LOOCV损失"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    trainAndGetLoss()</span><br><span class="line">    trainLOOCVAndGetLoss()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>## Next</p><p>下一次是真正入门啦，将之前说过的贝叶斯定理应用于机器学习，和你分享如何使用朴素的贝叶斯方法来进行简单的分类工作，比如识别手写数字，新闻素材的主题分类。（具体是挖那个主题的坑，还不知道</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;早上上班的路上读《社会心理学》里面有一段话，觉得不错，摘在这里。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;我们人类总是有一种不可抑制的冲动，想要解释行为，对其归因，以使其变得次序井然．具有可预见性，使一切尽在掌握之中。&lt;/strong&gt;你我对于类似的情境却可能表现出截然不同的反应，这是因为我们的想法不同。我们对朋友的责难做何反应，取决于我们对其所做的解释，取决于我们是把它归咎于朋友的敌意行为，还是归结于他糟糕的心情。 从某种角度来说，我们都是天生的科学家。我们解释着他人的行为，通常足够快也足够准确，以适应我们日常生活的需要。当他人的行为具有一致性而且与众不同时，我们会把其行为归因于他们的人格。例如。如果你发现一个人说话总是对人冷嘲热讽．你可能就会推断此人秉性不良，然后便设法尽量避免与他的接触。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当然这是指更大范围内的人类心理，在对数据和模型的痴迷上，人类的欲望显然也是强烈的，人类总是想尽一切的办法打破现有的桎梏，在创造了更多的不确定性之后，期望通过对数据的把控和预测以看到更确切的未来。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>最小二乘法的朴素实现</title>
    <link href="http://suool.net/2018/06/24/Least-Squares/"/>
    <id>http://suool.net/2018/06/24/Least-Squares/</id>
    <published>2018-06-24T12:23:21.000Z</published>
    <updated>2018-10-29T12:53:08.397Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>最小二乘法（Least Squares Method, 简记为LS）的维基解释。</p><blockquote><p>最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。 <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="维基百科[最小二乘法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)">[1]</span></a></sup></p></blockquote><p>如果大学的时候学过高等数学，或许你对这个名词就不会陌生，这是下册第九章第十节的内容。 <a id="more"></a> 其表示法为</p><p><span class="math display">\[\min\sum_in(y_m^{(i)}-y^{(i)})^2 \]</span></p><p>其中 <span class="math inline">\(y_m\)</span> 表示我们拟合函数得到的拟合结果，<span class="math inline">\(y_i\)</span> 表示真实值。</p><p>    “最小二乘法”是最优化问题中建立经验公式的一种实现方法。了解它的原理，对于了解后面“Logistic回归”和“支持向量机的学习”都很有裨益。</p><p>这次就是最小二乘法的朴素实现，即是不借助矩阵、向量等，纯粹借助数学标量推导完成。</p><h2 id="起源背景">起源背景</h2><p>最小二乘法是在十九世纪的产生的，源于天文学和测地学上的应用需要。</p><p>其中高斯使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中，而法国科学家勒让德于1806年独立发现“最小二乘法”，但因不为时人所知而默默无闻。两人曾为谁最早创立最小二乘法原理发生争执。<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="维基百科[最小二乘法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)">[1]</span></a></sup></p><p>1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，即高斯-马尔可夫定理。</p><h2 id="使用及实现">使用及实现</h2><h3 id="问题引入">问题引入</h3><p>“最小二乘法”的核心就是保证所有数据偏差的平方和最小。</p><p>首先有这样一组看起来比较杂乱的数据，这组数据是 1896 到 2008 年间部分年份的男子百米赛跑最佳成绩，其散点图效果如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/06/26/F8FC66C1-84BB-450C-BE96-8477C1C0C45F.png" alt="散点图"></p><p>如何于混沌中找出规律，在杂乱中确定关系，往往是我们最关心的事情，那么对于上面这样一组数据，我们就尝试对它进行建模拟合。</p><p>最简单的拟合是线性建模的拟合，即假设存在最佳的线性关系 $y=f(x;w_0,w_1)=w_0+w_1x $ 可以拟合这组数据。</p><h3 id="什么是最好">什么是最好？</h3><p>这里首先要解决的一个问题就是什么是“最佳”？或者如何衡量“最佳”？这里最佳显然是存在由一组<span class="math inline">\(w_0,w_1\)</span>确定的直线，这条直线尽可能的与所有的数据点接近，那么衡量远近即距离的最佳方法显然使用平方差，则这里可以使用下面这样一个平方差函数表示( <span class="math inline">\(x_n\)</span> 是年份，<span class="math inline">\(t_n\)</span> 是比赛成绩)： <span class="math display">\[\mathcal{L}_n =(t_n-f(x_n;w_0,w_1))^2 \]</span></p><p>这个表达式的值越小，则表示误差越小。</p><p>这里用下面的公式表示 N 年的平均损失，最小二乘损失函数 <span class="math display">\[\mathcal{L}= \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_n(t_n,f(x_n;w_0,w_1 )) \]</span></p><p>这个值显然越小越好，而其最小的关键是寻找到最合适的<span class="math inline">\(w_0, w_1\)</span>，这个数学表达式为</p><p><span class="math display">\[\mathop{\arg\;\min}\limits_{w_0, w_1} \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_n(t_n,f(x_n;w_0,w_1 ))\]</span></p><p>此时将函数关系表示为 <span class="math display">\[f(x_n;w_0,w_1) = w_0 + w_1x\]</span></p><p>代入最小二乘损失函数，得到结果为：</p><p><span class="math display">\[\begin{align}\mathcal{L} &amp;= \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_n(t_n,f(x_n;w_0,w_1 )) \\&amp; = \frac{1}{N} \sum_{n=1}^{N}(w_1^2x_n^2 + 2w_1x_n(w_0-t_n)+w_0^2-2w_0t_n+t^2)\end{align}\]</span></p><p>在上述函数 <span class="math inline">\(\mathcal {L}\)</span> 的最小值点处，其关于 <span class="math inline">\(w_0\)</span> 和 <span class="math inline">\(w_1\)</span> 的偏导数一定是 0。因此，对上函数式求偏导，使其等于 0 并对 <span class="math inline">\(w_0\)</span> 和 <span class="math inline">\(w_1\)</span> 求解，可以得到最小值。</p><h3 id="化简结果">化简结果</h3><p>关于 <span class="math inline">\(w_0\)</span> 和 <span class="math inline">\(w_1\)</span> 的表示式分别为 <span class="math display">\[\left\{ \begin{array}{c}\hat{w_0} = \overline {t} - w_1\overline {x} \\ \hat{w_1} = \frac {\overline {xt} - \overline{x} \overline{t}} {\overline{x^2}-(\overline{x})^2} \end{array}\right. \]</span></p><p>###代码实现 有了上述的表达式，直接使用代码计算得出的结果如下： <img src="https://suool-bolg.b0.upaiyun.com/2018/06/26/5C466B2C-2AF5-4469-A8F5-5325E8C7D2B5.png" alt="线性建模结果"></p><p>代码实现如下：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-<span class="number">8</span> -*</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as <span class="built_in">np</span></span><br><span class="line"></span><br><span class="line">x_cord = []</span><br><span class="line">y_cord = []</span><br><span class="line">def drawScatterDiagram(fileName):</span><br><span class="line">    fr=open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr=line.<span class="built_in">split</span>(',')</span><br><span class="line">        x_cord.<span class="built_in">append</span>(<span class="built_in">float</span>(lineArr[<span class="number">0</span>]))</span><br><span class="line">        y_cord.<span class="built_in">append</span>(<span class="built_in">float</span>(lineArr[<span class="number">1</span>]))</span><br><span class="line">    plt.scatter(x_cord,y_cord,s=<span class="number">30</span>,c='red',marker='o', alpha=<span class="number">0.7</span>)</span><br><span class="line">    plt.<span class="built_in">xlabel</span>(<span class="string">"year"</span>)</span><br><span class="line">    plt.<span class="built_in">ylabel</span>(<span class="string">"time"</span>)</span><br><span class="line">    plt.<span class="built_in">title</span>(<span class="string">"result of game"</span>)</span><br><span class="line"></span><br><span class="line">def linearCalculate():</span><br><span class="line">    x = <span class="built_in">np</span>.<span class="built_in">array</span>(x_cord)</span><br><span class="line">    y = <span class="built_in">np</span>.<span class="built_in">array</span>(y_cord)</span><br><span class="line">    x_mean = <span class="built_in">np</span>.<span class="built_in">mean</span>(x_cord)</span><br><span class="line">    y_mean = <span class="built_in">np</span>.<span class="built_in">mean</span>(y_cord)</span><br><span class="line">    xy_mean = <span class="built_in">np</span>.<span class="built_in">mean</span>(x*y)</span><br><span class="line">    x_square_mean = <span class="built_in">np</span>.<span class="built_in">mean</span>(x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    w1 = (xy_mean-x_mean*y_mean)/(x_square_mean-x_mean**<span class="number">2</span>)</span><br><span class="line">    w0 = y_mean - w1*x_mean</span><br><span class="line">    xasix = <span class="built_in">np</span>.linspace(<span class="number">1896</span>, <span class="number">2008</span>, <span class="number">112</span>)</span><br><span class="line">    yasix = w1 * xasix + w0</span><br><span class="line">    plt.plot(xasix,yasix, <span class="built_in">label</span>='<span class="built_in">linear</span> line')</span><br><span class="line">    plt.<span class="built_in">legend</span>(loc='upper right')</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == '__main__':</span><br><span class="line">    drawScatterDiagram(<span class="string">"olympic100m.txt"</span>)</span><br><span class="line">    linearCalculate()</span><br><span class="line">    plt.<span class="built_in">show</span>()</span><br></pre></td></tr></table></figure><h2 id="reference">Reference</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">维基百科<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">最小二乘法</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;最小二乘法（Least Squares Method, 简记为LS）的维基解释。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。 &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top-right hint--error hint--large&quot; data-aria-label=&quot;维基百科[最小二乘法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)&quot;&gt;[1]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果大学的时候学过高等数学，或许你对这个名词就不会陌生，这是下册第九章第十节的内容。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>信息论、贝叶斯及机器学习</title>
    <link href="http://suool.net/2018/06/21/%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://suool.net/2018/06/21/信息论和贝叶斯/</id>
    <published>2018-06-21T12:23:21.000Z</published>
    <updated>2018-10-29T12:53:08.546Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言">引言</h2><p>1956年，让机器来做聪明的事情的科学被称为“人工智能”。直到1997年，人类才创造出来能下象棋的电脑并打败了世界冠军。通过这样的一个例子及数字计算机的发展历史表明，感知其实是一个很难解决的问题。但是，我们的脑却能够很简单的解决这个问题，这是否意味着，数字计算机不是人脑的一个好隐喻？或者，我们需要为计算机的运行找新的运算方式？</p><a id="more"></a><p>同时信息论的发展使得我们看到物理事件和电脉冲是如何转化为精神事件和讯息的。但是，在最初表达中存在一个根本的问题。一条信息中的信息量，或者更通俗的说，任何刺激中的信息量完全由那个刺激源来决定，这种界定信息的方法看上去很完美，实际上会产生自相矛盾的结果。</p><p>比如在图像的处理中，图片是由像素点组成的，以此形成不同的颜色。比如看这样一张图片，它是一张简单的以白色为背景的黑色正方形的图片，这张图片中的哪些要素含有最多的信息？当我们的眼睛扫过一个颜色不变的区域的时候，因为没有任何的改变，就不会产生任何的惊奇感。而当我们眼睛扫到边缘的时候，颜色突然变化，我们就会感到“惊奇”。因此，根据信息论，图片的边缘所含的信息量是最大的，这和我们的直觉也确实是相符的，假如我们用轮廓来代替这个物体，换句话说，只留下有信息的边缘，我们仍然能够认出这个物体。</p><figure><img src="https://suool-bolg.b0.upaiyun.com/2018/06/21/06E79D3A-1211-4697-A3AF-B712D13C4EAE-1.png" alt="轮廓信息"><figcaption>轮廓信息</figcaption></figure><p>但是，这种表述实际上是自相矛盾的，按照这种界定，当我们用眼睛扫一幅图片的时候，我们预测不到接下来会发生什么，这样子的图片所含的信息量最多，完全由随机的点构成。比如电视机出现故障的时候出现的“雪花”屏幕，如果说这样子的图片含有最丰富的信息，恐怕你是不会同意的。 <img src="https://suool-bolg.b0.upaiyun.com/2018/06/21/BAD85663-52A5-47A4-BCAB-3FF1D50B4FA6-1.png" alt="雪花"></p><p>出现这种矛盾的问题在于，信息论没有考虑到观看者本身，更根本的说，是没有考虑到观看者的先验知识和预期期望，这些不同会明显的影响我们对事物的感知。</p><p>比如刚刚提到的黑色正方形，对于一些观看者来说，这就是一个黑色正方形，但是这个黑色正方形是俄罗斯绘画至上主义者的幻想、非具象艺术的首例，它是 Kazimir Malevich 于1913年展示的。这个例子里，知道“这是一件重要的艺术品”的先验知识，可以改变你对这个黑色正方形的感知，虽然它的信息量并无改变。 <img src="https://suool-bolg.b0.upaiyun.com/2018/06/21/A905B126-2523-491D-9574-1313E36D0557.png" alt="Kazimir Malevich"></p><h2 id="thomas-bayes-牧师">Thomas Bayes 牧师</h2><p>那么如何修正信息论使它能够考虑到观察者的不同经历和期望呢？我们可以说一条讯息携带的信息量可以达到改变接受者对世界信念的程度。要知道讯息承载了多少信息量给接收者，我们就得在讯息到达之前了解接收者的信念，然后才能知道接受者在收到讯息之后的信念改变了多少。但是，能否测量出接受者前先前的信念和信念的变化呢？</p><p>这个问题的答案就是每个学习过概率论和数理统计的人都知道的贝叶斯定理。</p><p>提出贝叶斯定理的 Thomas Bayes 牧师不是一个墨守成规的人，他的一生（1702-1761）没有发表过一篇论文，但是他在1742年却成为了英国皇家学会的会员。直到去世后两年，他的经典论文才被发表，而后的一百多年来，他的经典论文依然为人遗忘而无人问津。直到20世纪20年代，他才声名鹊起。对于当时的英国皇家协会主席和统计学界的人看来，Bayes 是一个的的确确的伟人，而在统计学界之外，他毫无名气，而且那些了解贝叶斯统计的人也常常认为它缺少适当的客观性。</p><p>然而，20世纪末至今，Thomas Bayes 成为了一个超级巨星。如今当你学习概率论或者想要入门人工智能，一个绕不过去的坎就是贝叶斯定理以及由此引发出的各种理论。</p><p>贝叶斯定理火起来之后，也导致了统计学的贝叶斯学派和频率学派的不休的争论。理解这二者的区别，对于理解贝叶斯定理也有一定的帮助，这里就简单的说一下。</p><h3 id="频率学派">频率学派</h3><p>频率学派认为，我们要观察的现象，其分布是确定的，是一直不变的，而我们所需要做的就是不断的做试验来接近它。因此对于频率学派来说，可以通过大量的独立重复实验，观察事件出现的频率来估计它出现的概率。针对模型<span class="math inline">\(P(x;θ)\)</span>来说，频率学派认为模型的参数是存在且固定的，我们要做的就是求出让<span class="math inline">\(P(x;θ)\)</span>值最大的参数<span class="math inline">\(θ\)</span>（我们认为这样的参数更接近于客观存在的那个真实的参数），也就是通过不断的调整参数来使得通过该模型在该参数下我们所观察到的现象出现的概率最大。</p><p>可以看出频率学派是针对似然来进行建模的，他更关心的的是似然<span class="math inline">\(P(x|θ)\)</span>，也就是在怎样的参数<span class="math inline">\(θ\)</span>下能使得我们所观察到的现象出现的概率最大。因此针对这样的关注重点，通常使用极大似然法来求解模型参数。 基于这种思想的方法，其缺点很明显，就是很容易过拟合，因为它的目标是尽最大努力来重现当前观察到的数据，这就是这种方法最大的问题所在。因此如何解决过拟合问题是基于频率思想方法必然要考虑的。通常我们会使用以下两种方法来缓解过拟合问题：（当然不限于此类问题的解决，很多其他的算法都可以用这些方法来解决 Over Fitting 的问题）</p><ul><li>使用 <span class="math inline">\(L1 \;或\; L2\)</span> 正则化，即在目标函数中加入正则项（罚项）；</li><li>使用交叉验证方法</li></ul><h3 id="贝叶斯学派">贝叶斯学派</h3><p>对于贝叶斯学派，它不再相信上帝的存在，即不再相信任何的事件发生的背后都拥有一个固定不变的分布，而更倾向于认为世界上所有的事情都是不确定的，而这种不确定性更多是由于观察者自身所储备的先验知识所带来的。因此对于贝叶斯学派，其通常会基于观察到的事件来假设一个先验分布<span class="math inline">\(P(y)\)</span>，然后利用贝叶斯公式：</p><p><span class="math display">\[P(y|x)= \frac{P(x,y)}{p(x)} = \frac {P(x|y)P(y)} {P(x)} = \frac {P(x|y)P(y)} {\sum_{y \in Y} P(x|y)P(y)}\]</span></p><p>来求得后验分布。而后验分布我们又可以认为是在得到新的知识<span class="math inline">\(x\)</span>后对先验分布的一个修正。因此对于贝叶斯学派，其认为对于事物的观察是一个不断学习不断修正的过程。</p><p>这里站在脑科学的层面对上面的贝叶斯公式做一个解释，假定现象 <span class="math inline">\(y\)</span> 是我们要了解的，观察资料 <span class="math inline">\(x\)</span> 是关于 <span class="math inline">\(y\)</span> 的证据，贝叶斯定理告诉我们，鉴于新证据 <span class="math inline">\(x\)</span> ，我们应该更新多少关于 <span class="math inline">\(y\)</span> 的知识。我们可以先不必担心这个等式的细节。重要的是，这个等式恰好是我们一直在寻找的关于信念的数学等式。在这里，表达信念的数学术语是概率。概率提供了我们对某事的信任尺度。当我们对于某件事是完全确定的时候（比如太阳从东方升起），概率就是 1，可以表示为 <span class="math inline">\(p(日出东方)=1\)</span>。如果确定某件事不会发生，那么概率就是 0。但是，我们大部分的时候信念是不坚定的，处于 0 和 1 之间，比如 <span class="math inline">\(p(今天上班可能要迟到)=0.5\)</span>，我得到了新的证据，那么这个处于中间的信念的概率就会不断地修正调整，比如上班之前，看了天气预报，等下要下大暴雨，那么这个信念可能就会发生显著地变化，当然有时候可能并不会发生什么变化。</p><p>贝叶斯定理可以精确的说明在已知新证据 <span class="math inline">\(x\)</span> 的情况下，我们应该改变多少关于 <span class="math inline">\(y\)</span> 的信念，这个等式中，<span class="math inline">\(P(y)\)</span> 是新证据 <span class="math inline">\(x\)</span> 出现之前我对于 <span class="math inline">\(y\)</span> 的先验信念。 <span class="math inline">\(P(x|y)\)</span> 是在 <span class="math inline">\(y\)</span> 确定的前提下，得到证据 <span class="math inline">\(x\)</span> 的可能性。 <span class="math inline">\(P(y|x)\)</span> 是在考虑新证据后我对于 <span class="math inline">\(y\)</span> 的后验信念。</p><h2 id="结语">结语</h2><p>而事实上，我们可以认为我们对世界的感知是一种与现实相符的幻觉。</p><p>首先我们对于现实世界的感知的一切都来自于脑的反馈，而大脑是如何做出一个判断的呢？比如在你看到这篇文章的每个字，每个标点，每句话的时候，我们的大脑是如何做出判断的呢？</p><p>显然，脑的判断来自于各个感官（眼睛、耳朵等）的感觉，综合各个感官所提供的证据 <span class="math inline">\(p(x|y)\)</span>，我们的大脑会做出一个基于我们已有的先验知识的判断。</p><p>当大脑的判断出现错误或者误差的时候，我们的大脑也会利用这些新的误差（新的证据）来更新我们对于世界的信念，并产生一个更好的信念 <span class="math inline">\(p(y|x)\)</span>，一旦这种更新发生，我们的脑就对世界产生了一个新的信念，并通过感官察觉的活动模式进行新的预测。大脑每重复一次这样的过程，每循环一次，预测的误差就会减少一些，当误差变得足够小的时候，大脑就可以“知道”外在世界的东西到底是何物了，这在某程度上来说，这正是我们学习的过程。（而此时是否是真的知道呢？是否可以如这一节开头所说“我们对世界的感知是一种与现实相符的幻觉”？）。</p><p>这样一个认知的过程，除了在学习新的信念（比如学习新语言或者技能）的时候，我们能深刻的体会到，大部分时候我们是几乎体验不到的，因为，处理平常判断的时候，大脑的运算速度非常之快，快到我们自以为判断客观世界的物体到底是何物是一件轻而易举的事情，但是我们的大脑却永远的陷入这种永无止境的贝叶斯预测和更新循环当中。</p><p>所以，先验知识是非常重要的学习依据，当我们面对一个毫无先验知识的物体的时候，恐怕真的会像《West World》中的 host 那样脱口而出一句话：“It doesn’t look like anything to me”。</p><p>而这种思维在机器学习领域的应用也非常广泛且效果显著。</p><p>下一次，我将将贝叶斯定理应用于机器学习，和你分享如何使用朴素的贝叶斯方法来进行简单的分类工作，比如识别手写数字，新闻素材的主题分类。</p><blockquote><p>本文的大部分内容摘自<a href="https://book.douban.com/subject/10864488/" target="_blank" rel="noopener">《心智的构建——脑如何创造我们的精神世界》</a><sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top-right hint--error hint--large" data-aria-label="Chris Frith . 心智的构建［M]. 华东师范大学出版社，2012-7.">[1]</span></a></sup>一书，这是一本很久前读的书，最近学习机器学习的时候，觉得贝叶斯这一块有种似曾相识的熟悉（先验知识的“作祟”），仔细一想便想到这本书中谈到过，因此，重新翻出，形成本文，对于脑科学和认知科学感兴趣的同学推荐阅读一下，是一本不可多得的好书。</p></blockquote><p>##文献引用</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Chris Frith . 心智的构建［M]. 华东师范大学出版社，2012-7.<a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;1956年，让机器来做聪明的事情的科学被称为“人工智能”。直到1997年，人类才创造出来能下象棋的电脑并打败了世界冠军。通过这样的一个例子及数字计算机的发展历史表明，感知其实是一个很难解决的问题。但是，我们的脑却能够很简单的解决这个问题，这是否意味着，数字计算机不是人脑的一个好隐喻？或者，我们需要为计算机的运行找新的运算方式？&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>矩阵论之线性空间和内积空间</title>
    <link href="http://suool.net/2018/06/18/%E7%9F%A9%E9%98%B5%E8%AE%BA%E4%B9%8B%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4/"/>
    <id>http://suool.net/2018/06/18/矩阵论之线性空间和内积空间/</id>
    <published>2018-06-18T08:54:57.000Z</published>
    <updated>2018-10-29T12:53:08.548Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="预备知识集合-映射-数域">预备知识:集合, 映射, 数域</h2><h3 id="集合">集合</h3><ul><li>定义<ul><li>集合: 由具有某种性质所确定的事物的总体. <a id="more"></a></li></ul></li><li>表示法<ul><li>列举法</li><li>概括法</li></ul></li><li>其他概念<ul><li>子集</li><li>包含关系</li><li>集合相等</li><li>有限集</li><li>无限集</li><li>空集</li></ul></li><li>集合运算<ul><li>并集 <span class="math display">\[  A \cup B = \{x\mid x \in A  \text { 或 } x\in B\} \tag{1}  \]</span></li><li>交集 <span class="math display">\[  A \cap B = \{ x \mid x \in A \text { 且 } x \in B \} \tag{2}  \]</span></li><li>二元关系<ul><li>集合的 Descartes 积</li><li><span class="math inline">\(A \times B\)</span> 的子集 R 是<span class="math inline">\(A \times B\)</span>中的一个二元关系</li><li>等价关系</li><li>等价类</li><li>商集</li><li>分类</li></ul></li></ul></li></ul><h3 id="映射">映射</h3><p>映射是函数概念的推广, 描述了两个集合的元素之间的关系</p><ul><li>基本概念<ul><li>映射</li><li>像</li><li>原像</li><li>定义域</li><li>值域</li><li>符号 <span class="math display">\[ f : A \to B \tag{3}\]</span></li><li>恒等映射/单位映射</li><li>单映射</li><li>满映射</li><li>双映射</li><li>映射相等</li><li>映射的乘积<ul><li>设 <span class="math inline">\(A, B, C\)</span> 是三个非空集合,并设有两个映射 <span class="math inline">\(f_1: A \to B\)</span> , <span class="math inline">\(f_2: B \to C\)</span> , 由$f_1  f_2 $ 确定的 <span class="math inline">\(A\)</span> 到 <span class="math inline">\(C\)</span> 的映射 <span class="math inline">\(f_3: a \to f_2(f_1(a)), a \in A\)</span> 成为映射$f_1  f_2 $ 的乘积, 记为 <span class="math inline">\(f_3 = f_2 \cdot f_1\)</span></li><li>映射的乘积不具有交换律, 具有结合律</li></ul></li><li>逆映射 <span class="math inline">\(f^{-1}\)</span><ul><li>可逆映射的逆映射唯一</li><li>可逆映射的充要条件是一一映射</li></ul></li></ul>### 数域和代数运算<ul><li>定义<ul><li>设 <span class="math inline">\({\displaystyle {\mathcal {P}}}\)</span> 是复数域 ${ } $ 的子集。若 <span class="math inline">\({\displaystyle {\mathcal {P}}}\)</span> 中包含0与1，并且 <span class="math inline">\({\displaystyle {\mathcal {P}}}\)</span> 中任两个数的和、差、乘积以及商（约定除数不为0）都仍在 <span class="math inline">\({\displaystyle {\mathcal {P}}}\)</span> 中，就称 <span class="math inline">\({\displaystyle {\mathcal {P}}}\)</span> 为一个数域</li></ul></li><li>代数运算</li><li>二元运算</li></ul></li></ul><h2 id="矩阵运算">矩阵运算</h2><h2 id="线性方程组">线性方程组</h2><h2 id="向量组的极大线性无关组和秩">向量组的极大线性无关组和秩</h2><h2 id="矩阵的秩及等价标准形">矩阵的秩及等价标准形</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;预备知识集合-映射-数域&quot;&gt;预备知识:集合, 映射, 数域&lt;/h2&gt;
&lt;h3 id=&quot;集合&quot;&gt;集合&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;定义
&lt;ul&gt;
&lt;li&gt;集合: 由具有某种性质所确定的事物的总体.
    
    </summary>
    
      <category term="数学" scheme="http://suool.net/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="线性代数" scheme="http://suool.net/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="矩阵论" scheme="http://suool.net/tags/%E7%9F%A9%E9%98%B5%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的数学基础--线性代数</title>
    <link href="http://suool.net/2018/06/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    <id>http://suool.net/2018/06/18/机器学习中的数学基础-线性代数/</id>
    <published>2018-06-17T16:39:23.000Z</published>
    <updated>2018-10-29T12:53:08.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>换了个新的博客主题,开启新的学习主题.</p></blockquote><h1 id="线性代数">线性代数</h1><h2 id="标量向量矩阵和张量">标量、向量、矩阵和张量</h2><ul><li><p>标量(scalar): 一个标量就是一个单独的数，它不同于线性代数中研 究的其他大部分对象(通常是多个数的数组)。 <a id="more"></a></p></li><li>向量(vector): 一个向量是一列数。这些数是有序排列的。通过次序中的索引，我们可以确定每个单独的数。</li><li>矩阵(matrix): 矩阵是一个二维数组，其中的每一个元素被两个索引(而非一个)所确定。</li><li><p>张量(tensor): 在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。</p></li></ul><p><strong>转置(transpose)</strong>是矩阵的重要操作之一。矩阵的转置是以对角线为轴的镜像， 这条从左上角到右下角的对角线被称为 主对角线(main diagonal)。我们将矩阵 A 的转置表示为 <span class="math inline">\(A^T\)</span>，定义如下</p><p><span class="math display">\[(A^T)_{i,j} = A_{j,i} \tag{1}\]</span></p><p>向量可以看作只有一列的矩阵。对应地，向量的转置可以看作是只有一行的矩阵。标量可以看作是只有一个元素的矩阵。</p><p>只要矩阵的形状一样，我们可以把两个矩阵相加。两个矩阵相加是指对应位置的元素相加。</p><p>标量和矩阵相乘，或是和矩阵相加时，我们只需将其与矩阵的每个元素相乘或相加。</p><h2 id="矩阵和向量相乘">矩阵和向量相乘</h2><p>两个矩阵 <span class="math inline">\(\bf A\)</span> 和 $ _{i,j}$ 的步骤看作是 <span class="math inline">\(A\)</span> 的第 <span class="math inline">\(i\)</span> 行和 <span class="math inline">\(B\)</span> 的第 <span class="math inline">\(j\)</span> 列之间的点积。</p><p>两个向量点积的结果是标量</p><h2 id="单位矩阵和逆矩阵">单位矩阵和逆矩阵</h2><p>单位矩阵 (identity matrix) 矩阵逆 (matrix inversion)</p><p>任意向量和单位矩阵相乘，都不会改变。</p><p>单位矩阵的结构很简单:所有沿主对角线的元素都是 1，而所有其他位置的元素都是 0。如下所示。</p><p><span class="math display">\[\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1 \\  \end{bmatrix} \tag{3}\]</span></p><p>矩阵 <span class="math inline">\(A\)</span> 的矩阵逆(matrix inversion)记作 <span class="math inline">\(A^{−1}\)</span>，其定义的矩阵满足如下条件 <span class="math display">\[A^{−1}A = I_n.\tag{4}\]</span></p><p>一组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即: <span class="math display">\[\sum_{i} {c_iv^i}.\tag{5}\]</span></p><p>一组向量的<strong>生成子空间(span)</strong>是原始向量线性组合后所能抵达的点的集合</p><p>如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为<strong>线性无关 (linearly independent</strong>)。</p><p>一个列向量线性相关的方阵被称为<strong>奇异的(singular)</strong></p><h2 id="范数">范数</h2><p>在机器学习中，我们经常使用被称为<strong>范数 (norm)</strong>的函数衡量向量大小。形式上，<span class="math inline">\(L^p\)</span> 范数定义如下</p><p><span class="math display">\[||x||_p=\left(\sum_{i}{|x_i|^p}\right)^{\frac{1}{p}}\tag{6}\]</span></p><p>其中 $p R , p 1 $.</p><p>范数(包括 <span class="math inline">\(L^p\)</span> 范数)是将向量映射到非负值的函数。直观上来说，向量 <span class="math inline">\(x\)</span> 的范数衡量从原点到点 <span class="math inline">\(x\)</span> 的距离。</p><p>当 <span class="math inline">\(p = 2\)</span> 时，<span class="math inline">\(L^2\)</span> 范数被称为<strong>欧几里得范数(Euclidean norm)</strong>。它表示从原点出发到向量 <span class="math inline">\(x\)</span> 确定的点的欧几里得距离。<span class="math inline">\(L^2\)</span> 范数在机器学习中出现地十分频繁，经常简化表示为 <span class="math inline">\(∥x∥\)</span>，略去了下标 2。平方 <span class="math inline">\(L^2\)</span> 范数也经常用来衡量向量的大小，可以简单地通过点积 <span class="math inline">\(x^⊤x\)</span> 计算。</p><p>平方 <span class="math inline">\(L^2\)</span> 范数在数学和计算上都比 <span class="math inline">\(L^2\)</span> 范数本身更方便。例如，平方 <span class="math inline">\(L^2\)</span> 范数对 <span class="math inline">\(x\)</span> 中每个元素的导数只取决于对应的元素，而 <span class="math inline">\(L^2\)</span> 范数对每个元素的导数却和整个向量相关。但是在很多情况下，平方 <span class="math inline">\(L^2\)</span> 范数也可能不受欢迎，因为它在原点附近增长 得十分缓慢。在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素 是很重要的。在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数:<span class="math inline">\(L^1\)</span> 范数. <span class="math inline">\(L^1\)</span> 范数可以简化如下:</p><p><span class="math display">\[||x||_1 = \sum_i {|x_i|}\tag{7}\]</span></p><p>当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 <span class="math inline">\(L^1\)</span> 范数。每当 <span class="math inline">\(x\)</span> 中某个元素从 0 增加 <span class="math inline">\(\epsilon\)</span>，对应的 L1 范数也会增加 <span class="math inline">\(\epsilon\)</span> 。</p><p>另外一个经常在机器学习中出现的范数是 <span class="math inline">\(L^\infty\)</span> 范数，也被称为<strong>最大范数(max norm)</strong>。这个范数表示向量中具有最大幅值的元素的绝对值:</p><p><span class="math display">\[||x||_\infty = \underset{i}{max}|x_i|\tag{8}\]</span></p><p>有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用 <strong>Frobenius 范数(Frobenius norm)</strong>，</p><p><span class="math display">\[||A||_F = \sqrt {\sum_{i,j}A^2_{i,j}}\tag{9}\]</span></p><p>其类似于向量的 <span class="math inline">\(L^2\)</span> 范数。 两个向量的<strong>点积(dot product)</strong>可以用范数来表示。具体地，</p><p><span class="math display">\[x^Ty=||x||_2||y||_2\cos\theta  \tag{10}\]</span></p><p>其中 <span class="math inline">\(θ\)</span> 表示 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 之间的夹角。</p><p>如果 <span class="math inline">\(x^⊤y = 0\)</span>，那么向量 <span class="math inline">\(x\)</span> 和向量 <span class="math inline">\(y\)</span> 互相<strong>正交(orthogonal)</strong>。如果两个向量都有非零范数，那么这两个向量之间的夹角是 90 度。在 <span class="math inline">\(R^n\)</span> 中，至多有 <span class="math inline">\(n\)</span> 个范数非零向量互相正交。如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们 是<strong>标准正交(orthonormal)</strong>。</p><p><strong>正交矩阵(orthogonal matrix)</strong>是指行向量和列向量是分别标准正交的方阵,<span class="math inline">\(A^{−1} = A^⊤\)</span></p><h2 id="特征分解">特征分解</h2><p><strong>特征分解(eigendecomposition)</strong>是使用最广的矩阵分解之一，即我们将矩阵分解成一组<strong>特征向量</strong>和<strong>特征值</strong>。 方阵 A 的<strong>特征向量(eigenvector)</strong>是指与 A 相乘后相当于对该向量进行缩放的非零向量 v: <span class="math display">\[Av = \lambda v\tag{11}\]</span> 标量 <span class="math inline">\(λ\)</span> 被称为这个特征向量对应的<strong>特征值(eigenvalue)</strong></p><p>A 的<strong>特征分解(eigendecomposition)</strong>可以记作</p><p><span class="math display">\[A=Vdiag(\lambda)V^{-1}\tag{12}\]</span></p><p>所有特征值都是正数的矩阵被称为<strong>正定(positive definite)</strong>;所有特征值都是非负数的矩阵被称为<strong>半正定(positive semidefinite)</strong>。同样地，所有特征值都是负数的 矩阵被称为<strong>负定(negative definite)</strong>;所有特征值都是非正数的矩阵被称为<strong>半负定(negative semidefinite)</strong>。半正定矩阵受到关注是因为它们保证 <span class="math inline">\(∀x, x^⊤Ax ≥ 0\)</span>。此外，正定矩阵还保证 <span class="math inline">\(x^⊤Ax = 0 ⇒ x = 0\)</span>。</p><h2 id="奇异值分解">奇异值分解</h2><p><strong>奇异值分解(singular value decomposition, SVD)</strong>，将矩阵分解为<strong>奇异向量(singular vector)</strong>和<strong>奇异值(singular value)</strong>。通过奇异值分解，我们会得到一些与特征分解相同类型的信息。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。</p><p>分解表达式: <span class="math display">\[A = UDV^⊤. \tag{13}\]</span></p><p>假设 <span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(m×n\)</span> 的矩阵，那么 <span class="math inline">\(U\)</span> 是一个 <span class="math inline">\(m×m\)</span> 的矩阵，<span class="math inline">\(D\)</span> 是一个 <span class="math inline">\(m×n\)</span> 的矩阵，<span class="math inline">\(V\)</span> 是一个 <span class="math inline">\(n × n\)</span> 矩阵。 这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵 <span class="math inline">\(U\)</span> 和 <span class="math inline">\(V\)</span> 都定义为正交矩阵，而矩阵 <span class="math inline">\(D\)</span> 定义为对角矩阵。注意，矩阵 <span class="math inline">\(D\)</span> 不一定是方阵。 对角矩阵 <span class="math inline">\(D\)</span> 对角线上的元素被称为矩阵 <span class="math inline">\(A\)</span> 的<strong>奇异值(singular value)</strong>。矩阵 <span class="math inline">\(U\)</span> 的列向量被称为<strong>左奇异向量(left singular vector)</strong>，矩阵 <span class="math inline">\(V\)</span> 的列向量被称<strong>右奇异向量(right singular vector)</strong>。 事实上，我们可以用与 <span class="math inline">\(A\)</span> 相关的特征分解去解释 <span class="math inline">\(A\)</span> 的奇异值分解。<span class="math inline">\(A\)</span>的<strong>左奇异向量(left singular vector)</strong>是 <span class="math inline">\(AA^⊤\)</span> 的特征向量。<span class="math inline">\(A\)</span> 的<strong>右奇异向量(right singular vector)</strong>是 <span class="math inline">\(A^⊤A\)</span> 的特征向量。A 的非零奇异值是 <span class="math inline">\(A^⊤A\)</span> 特征值的平方根，同时也是 <span class="math inline">\(AA^⊤\)</span> 特征值的平方根。</p><h2 id="迹运算">迹运算</h2><p>迹运算返回的是矩阵对角元素的和: <span class="math display">\[Tr(A)=\sum_i {A_{i,i}}\]</span></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;换了个新的博客主题,开启新的学习主题.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;线性代数&quot;&gt;线性代数&lt;/h1&gt;
&lt;h2 id=&quot;标量向量矩阵和张量&quot;&gt;标量、向量、矩阵和张量&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;标量(scalar): 一个标量就是一个单独的数，它不同于线性代数中研 究的其他大部分对象(通常是多个数的数组)。
    
    </summary>
    
      <category term="机器学习" scheme="http://suool.net/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://suool.net/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性代数" scheme="http://suool.net/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>树莓派折腾指南之将你树莓派变成智能家居中枢</title>
    <link href="http://suool.net/2018/01/13/%E6%A0%91%E8%8E%93%E6%B4%BE%E6%8A%98%E8%85%BE%E6%8C%87%E5%8D%97%E4%B9%8B%E5%B0%86%E4%BD%A0%E6%A0%91%E8%8E%93%E6%B4%BE%E5%8F%98%E6%88%90%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E4%B8%AD%E6%9E%A2/"/>
    <id>http://suool.net/2018/01/13/树莓派折腾指南之将你树莓派变成智能家居中枢/</id>
    <published>2018-01-13T09:45:13.000Z</published>
    <updated>2018-10-29T12:53:08.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="将你树莓派变成智能家居中枢">将你树莓派变成智能家居中枢</h1><p>树莓派由于本身足够的小巧且扩展性极高，所以它智能家居方面的应用具有天然的优势。对于喜欢折腾和 Geek 的人的而言，其扩展性具有无比的吸引力。 <a id="more"></a></p><p>这次我们就来尝试将树莓派变身为智能家居的管理中枢，将其桥接到苹果家的 Home 应用当中，使用 Siri 来控制所有的智能家居。</p><p>由于 Apple 家认证的智能家居暂时承担不起，而刚好又可以使用树莓派加上 <code>HomeBridge</code> 相关的框架来将小米的智能家居产品加入 <code>HomeKit</code> 。</p><p>我使用的是小米智能家居全家桶套装，极客学院送的2017年元旦讲师礼物，在此表示感谢！其包含内容产品主要有：多功能网关，人体传感器，智能插座，无线开关，门窗传感器。效果图如下： <img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158306050366.jpg" alt="-w629"></p><figure><img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158306313468.jpg" alt="-w451"><figcaption>-w451</figcaption></figure><p>用习惯之后，不得不说确实很方便，特别是寒冷的冬天不用起床就能开关灯。</p><p>下面就进入正题，如何将你的树莓派变身智能家居桥接中枢。</p><h2 id="准备工作">准备工作</h2><h3 id="材料准备">材料准备</h3><p>硬件：</p><ul><li>Raspberry Pi</li><li>小米智能家居产品</li></ul><p>软件： * <a href="https://github.com/nfarina/homebridge" target="_blank" rel="noopener">HomeBridge</a> * <a href="https://github.com/YinHangCode/homebridge-mi-aqara" target="_blank" rel="noopener">homebridge-mi-aqara</a></p><h3 id="基本步骤">基本步骤</h3><ol type="1"><li>硬件准备和连接</li><li>软件环境和依赖安装</li><li>安装运行 <code>Homebridge</code></li><li>安装及配置 <code>homebridge-mi-aqara</code></li><li>测试及后台运行</li><li>其他</li></ol><h2 id="步骤一硬件准备和连接">步骤一：硬件准备和连接</h2><p>首先将你的<a href="https://www.jianshu.com/p/b62a8229a74c" target="_blank" rel="noopener">树莓派安装好系统并配置好</a>，最好可以使用 SSR 全局代理可以科学上网，这样可以改善你的树莓派网络环境，一定程度加快安装过程。具体的方法参考<a href="https://www.jianshu.com/p/445256a2367b" target="_blank" rel="noopener">之前的文章</a>。</p><p>使用 SSH 远程连接你的树莓派：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh <span class="literal">pi</span><span class="meta">@raspberrypi</span>.<span class="keyword">local</span></span><br></pre></td></tr></table></figure><p>更新树莓派相关依赖环境</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> update</span><br><span class="line">sudo apt-<span class="builtin-name">get</span> upgrade</span><br></pre></td></tr></table></figure><p>完成之后进入下一步。</p><h2 id="步骤二软件环境和依赖安装">步骤二：软件环境和依赖安装</h2><p>首先安装这两个库使用的都是 <code>Node</code> 环境的 <code>npm</code> 工具，因此要先安装 <code>Node</code>，命令如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -sL http<span class="variable">s:</span>//<span class="keyword">deb</span>.nodesource.<span class="keyword">com</span>/setup_8.<span class="keyword">x</span> | sudo -E bash -</span><br><span class="line">sudo apt-<span class="built_in">get</span> install -<span class="keyword">y</span> nodejs</span><br></pre></td></tr></table></figure><p>其次需要安装 <code>avahi</code> 包</p><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install libavahi-compat-libdnssd-<span class="built_in">dev</span></span><br></pre></td></tr></table></figure><p>如果上述包安装出现错误，可以参考这里的<a href="https://stackoverflow.com/questions/26571326/how-do-i-resolve-the-following-packages-have-unmet-dependencies" target="_blank" rel="noopener">解决方法</a></p><p>尝试使用</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo aptitude <span class="keyword">install</span> libavahi-compat-libdnssd-dev</span><br></pre></td></tr></table></figure><p>命令安装依赖，可能需要将相关软件包降级就可以顺利安装了。</p><h2 id="步骤三安装运行-homebridge">步骤三：安装运行 Homebridge</h2><p>这里具体的安装步骤可以直接去参考其 github 主页的 wiki 内容。简单的来说，步骤如下: 首先可以试一下使用如下命令安装：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> -g homebridge</span><br></pre></td></tr></table></figure><p>如果上述命令安装过程出现了错误，那么可以尝试使用如下的命令：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">sudo npm install -g --unsafe-perm homebridge hap-nodejs node-gyp</span><br><span class="line">cd /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">node_modules</span>/<span class="title">homebridge</span>/</span></span><br><span class="line">sudo npm install --unsafe-perm bignum</span><br><span class="line">cd /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">node_modules</span>/<span class="title">hap</span>-<span class="title">nodejs</span>/<span class="title">node_modules</span>/<span class="title">mdns</span></span></span><br><span class="line">sudo node-gyp BUILDTYPE=Release rebuild</span><br></pre></td></tr></table></figure><p>上面的 <code>/usr/lib/</code> 目录，如果你不是使用的 <code>apt-get</code> 命令安装的 <code>Node</code>，需要换成 <code>/usr/local/lib/</code> 。 一定要严格安装上述命令的步骤来安装，该切换目录就切换目录。</p><p>此时安装应该不会有什么问题了。</p><h2 id="步骤四安装及配置-homebridge-mi-aqara">步骤四：安装及配置 homebridge-mi-aqara</h2><p>其项目主页是 <a href="https://github.com/YinHangCode/homebridge-mi-aqara" target="_blank" rel="noopener">homebridge-mi-aqara</a>，可以在他的项目主页看到其支持的小米硬件，基本是非常全面的。</p><p>安装命令如下：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm <span class="keyword">install</span> -g homebridge-mi-aqara</span><br></pre></td></tr></table></figure><p>安装完成之后，需要对其进行基本的配置，从而能够将你的小米全家桶硬件加入到 <code>HomeBridge</code> 中。</p><p>首先要获取的是 局域网通信协议密码 以及 网关的 <code>MAC</code> 地址。</p><p>下载米家 APP，连接上你的智能网关以及其他小米智能家居硬件，打开米家 APP，选择你的多功能网关，点击 APP 右上角 <code>···</code> 符号，进入 <code>关于</code> 选项: <img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158331055340.jpg"> <img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158331906989.jpg"> 如上图所示，要不断的点击空白处，片刻后界面就会多了<code>局域网通信协议</code>还有<code>网关信息</code>等选项。 这时候，分别点选他们，记录你的网关的<code>局域网通信协议密码</code>以及<code>网关的MAC地址</code>，记得要开启局域网通信协议，记录下密码后，点击确定。网关的 MAC 地址位置如下： <img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158337428781.jpg"></p><p>上面的 <code>MAC</code> 地址去除冒号，全变成小写后记录下来，是一串12位的字符。局域网通信协议密码要保留大小写的记录下来。</p><p>记录下上述两个地址之后，就可以开始在终端中配置 <code>HomeBridge</code> 的配置文件了。执行的命令如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir ~/<span class="selector-class">.homebridge</span> </span><br><span class="line">cd ~/<span class="selector-class">.homebridge</span>      </span><br><span class="line">sudo nano config.json</span><br></pre></td></tr></table></figure><p>执行了上述 nano 命令之后，会创建一个配置的 <code>json</code> 文件，将以下内粘贴到终端编辑环境中</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"bridge"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>:<span class="string">"Homebridge"</span>,</span><br><span class="line">        <span class="attr">"username"</span>:<span class="string">"CC:22:3D:E3:CE:23"</span>,</span><br><span class="line">        <span class="attr">"port"</span>:<span class="number">51826</span>,</span><br><span class="line">        <span class="attr">"pin"</span>:<span class="string">"723-92-124"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"platforms"</span>: [&#123;</span><br><span class="line">        <span class="attr">"platform"</span>: <span class="string">"MiAqaraPlatform"</span>,</span><br><span class="line">        <span class="attr">"gateways"</span>: &#123;</span><br><span class="line">            <span class="attr">"网关 mac 地址"</span>: <span class="string">"局域网通信协议密码"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将上述的两个参数替换成刚刚你记录下来的即可。其他参数说明： <code>name</code> iOS 的 <code>Homekit</code> 在添加配件的时候看到的名字 <code>username</code> 如果只是用 <code>Homebridge</code>，这里可以是任意一个类似 <code>MAC</code> 地址的字符串 <code>port</code> 随意，只要不被占用的端口 <code>pin</code> iOS 的 <code>Homekit</code> 在添加配件时需要的验证码</p><p>使用 <code>Ctrl + o</code> 保存， <code>Ctrl + x</code> 退出。</p><h2 id="步骤五测试及后台运行">步骤五：测试及后台运行</h2><p>在终端输入</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">homebridge -D</span></span><br></pre></td></tr></table></figure><p>这时候，界面也会出现刚刚你自己填写的 PIN 码 <img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158339825667.jpg"></p><p>进入你 iPhone 或者 iPad 的家庭APP，添加配件，扫码几乎不可能加入成功，直接选择输入 PIN 码，即是输入下方的 <img src="http://7xjsv3.com1.z0.glb.clouddn.com/15158342592263.jpg"> 添加完成之后，你就会看到所有网关所附带的配件了，如我刚开始所配图的一般，不过此时按钮的名字可能是一串英文符号，你需要自己确定各个按钮的作用，给他们一个你想好的名字即可。</p><p>但是上面这个只是在测试环境运行，如果断了 SSH，你手机或者 iPad 里面的所有设备都会处于无响应的状态，所以，我们还需要能够在后台运行的 <code>Homebridge</code>。</p><p>借助 <code>screen</code> 工具即可实现这一需求，具体安装命令如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install screen</span><br></pre></td></tr></table></figure><p>安装完成之后，首先开一一个名字叫做 <code>home</code> 的窗口，具体的名字你可以随意选取，命令如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -S <span class="built_in">home</span></span><br></pre></td></tr></table></figure><p>然后所打开的 <code>screen</code> 进程中开启一个 <code>homebridge</code> 进程</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">homebridge -D</span></span><br></pre></td></tr></table></figure><p>在 <code>screen</code> 里开启的 <code>homebridge</code> 不会随着 SSH 关闭而被关闭。使用 <code>Ctrl+A</code> 然后按 <code>d</code> 就可以跳出来了。</p><p>具体关于 <code>screen</code> 命令的用法你可以查看这里的文章: <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-screen/" target="_blank" rel="noopener">linux 技巧：使用 screen 管理你的远程会话</a> 及 <a href="http://www.runoob.com/linux/linux-comm-screen.html" target="_blank" rel="noopener">Linux screen命令</a>。</p><h2 id="其他">其他</h2><p>完成上面的步骤，此时你只能在你的路由器所在的局域网内完成智能家居的控制，如果要在外网实现操作，及 home 的自动化操作，你需要一台 iPad 作为家居中枢，具体的参考 <a href="https://support.apple.com/zh-cn/HT207057" target="_blank" rel="noopener">自动化和远程访问 HomeKit 配件</a></p><p>上面就是一个利用树莓派实现让 Siri 帮你开关灯的过程，下一步可以使用 <code>HomeAssistant</code> 来实现更加智能化和扩展化的家居管理。</p><p>更下一步的尝试可能会比较底层一些了： * 尝试一下扩展温湿度感应器模块 * 尝试使用 <code>GPIO</code> 控制 RGB 彩色 LED 灯 * 扩展摄像头模块，尝试一下人脸识别等基本的人工智能 * 扩展私有云存储及远程下载</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;将你树莓派变成智能家居中枢&quot;&gt;将你树莓派变成智能家居中枢&lt;/h1&gt;
&lt;p&gt;树莓派由于本身足够的小巧且扩展性极高，所以它智能家居方面的应用具有天然的优势。对于喜欢折腾和 Geek 的人的而言，其扩展性具有无比的吸引力。
    
    </summary>
    
      <category term="硬件" scheme="http://suool.net/categories/%E7%A1%AC%E4%BB%B6/"/>
    
    
      <category term="树莓派" scheme="http://suool.net/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
  </entry>
  
</feed>
